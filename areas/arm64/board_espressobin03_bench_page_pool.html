<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Benchmarks for branch mvneta_03_page_pool_recycle</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/hideshow.css"/>
<script type="text/javascript" src="/styles/bigblow/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/bigblow.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/hideshow.js"></script>
<script type="text/javascript" src="/styles/lib/js/jquery.stickytableheaders.min.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">Benchmarks for branch mvneta_03_page_pool_recycle</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Patch-desc-notes">Patch desc notes</a>
<ul>
<li><a href="#Patch-desc--net--core--add-recycle-capabilities-on-skbs-via-page_pool-API">Patch desc: net: core: add recycle capabilities on skbs via page_pool API</a></li>
</ul>
</li>
<li><a href="#Experiments-while-testing">Experiments while testing</a>
<ul>
<li><a href="#Remove-wrong-prefetch-in-mvneta_rx_swbm">Remove wrong prefetch in mvneta_rx_swbm</a></li>
<li><a href="#Instruction-cache-misses">Instruction cache misses</a></li>
<li><a href="#kfree_skb-instead-of-napi_gro_receive">kfree_skb instead of napi_gro_receive</a></li>
<li><a href="#Use-netif_receive_skb_list">Use netif_receive_skb_list</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
Benchmark investigations and experiments on git branch
mvneta_03_page_pool_recycle located here:
</p>
<ul class="org-ul">
<li><a href="https://github.com/apalos/bpf-next/commits/mvneta_03_page_pool_recycle">https://github.com/apalos/bpf-next/commits/mvneta_03_page_pool_recycle</a></li>
</ul>

<p>
This doc describe some experiments done on top of mentioned branch,
which change driver mvneta to use page_pool and 1-page-per RX-frame.
Plus adding a recycle hook in netstack-core in <code>skb_free_head()</code> which
gets called by <code>__kfree_skb()</code>.
</p>

<p>
This netstack/SKB callback hook is one of the mentioned solutions in:
</p>
<ul class="org-ul">
<li><a href="../mem/page_pool01_evolving_API.html">../mem/page_pool01_evolving_API.html</a></li>
</ul>

<div id="outline-container-Patch-desc-notes" class="outline-2">
<h2 id="Patch-desc-notes"><a href="#Patch-desc-notes">Patch desc notes</a></h2>
<div class="outline-text-2" id="text-Patch-desc-notes">
<p>
For branch mvneta_03_page_pool_recycle
</p>
</div>

<div id="outline-container-Patch-desc--net--core--add-recycle-capabilities-on-skbs-via-page_pool-API" class="outline-3">
<h3 id="Patch-desc--net--core--add-recycle-capabilities-on-skbs-via-page_pool-API"><a href="#Patch-desc--net--core--add-recycle-capabilities-on-skbs-via-page_pool-API">Patch desc: net: core: add recycle capabilities on skbs via page_pool API</a></h3>
<div class="outline-text-3" id="text-Patch-desc--net--core--add-recycle-capabilities-on-skbs-via-page_pool-API">
<p>
net: core: add recycle capabilities on skbs via page_pool API
</p>

<p>
This patch is changing struct sk_buff, and is thus per-definition
controversial upstream.
</p>

<p>
TODO: Argue why the placement of xdp_mem_info is correct, and
argue how handling of SKB cloning is still correct.
</p>

<p>
IDEA: We could change/extend xdp_mem_info with 8-bit flags, that could
contain/replace skb-&gt;head_frag and skb-&gt;pfmemalloc.  This likely
require another patch to be reviewable.
</p>
</div>
</div>
</div>

<div id="outline-container-Experiments-while-testing" class="outline-2">
<h2 id="Experiments-while-testing"><a href="#Experiments-while-testing">Experiments while testing</a></h2>
<div class="outline-text-2" id="text-Experiments-while-testing">
</div>
<div id="outline-container-Remove-wrong-prefetch-in-mvneta_rx_swbm" class="outline-3">
<h3 id="Remove-wrong-prefetch-in-mvneta_rx_swbm"><a href="#Remove-wrong-prefetch-in-mvneta_rx_swbm">Remove wrong prefetch in mvneta_rx_swbm</a></h3>
<div class="outline-text-3" id="text-Remove-wrong-prefetch-in-mvneta_rx_swbm">
<p>
Before (with wrong prefetch):
</p>
<ul class="org-ul">
<li>2195983      cache-misses #    0.854 % of all cache refs</li>
</ul>

<pre class="example">
Performance counter stats for 'CPU(s) 0' (3 runs):

 569786261  instructions    #    0.57  insn per cycle           ( +-  0.00% )
1003904562  cycles                                              ( +-  0.00% )
 250976363  bus-cycles                                          ( +-  0.00% )
   2195983  cache-misses    #    0.854 % of all cache refs      ( +-  0.02% )
 257083692  cache-references                                    ( +-  0.00% )
  59939555  branches                                            ( +-  0.00% )
   4179109  branch-misses   #    6.97% of all branches          ( +-  0.04% )
</pre>

<p>
After removing prefetch:
</p>
<ul class="org-ul">
<li>1354464      cache-misses #    0.520 % of all cache refs</li>
<li>841519 (2195983-1354464) less cache-misses</li>
</ul>

<pre class="example">
Performance counter stats for 'CPU(s) 0' (3 runs):

 576645517  instructions    #    0.57  insn per cycle           ( +-  0.13% )
1003882263  cycles                                              ( +-  0.00% )
 250970714  bus-cycles                                          ( +-  0.00% )
   1354464  cache-misses    #    0.520 % of all cache refs      ( +-  0.13% )
 260314221  cache-references                                    ( +-  0.13% )
  60712134  branches                                            ( +-  0.13% )
   4310594  branch-misses   #    7.10% of all branches          ( +-  0.78% )

 1.0040368 +- 0.0000266 seconds time elapsed  ( +-  0.00% )
</pre>

<p>
Adding prefetch after dma_sync, seem to be bad:
</p>
<ul class="org-ul">
<li>2122745      cache-misses              #    0.816 % of all cache refs</li>
</ul>

<pre class="example">
Performance counter stats for 'CPU(s) 0' (3 runs):

        575902276      instructions              #    0.57  insn per cycle           ( +-  0.01% )
       1003959366      cycles                                                        ( +-  0.00% )
        250981924      bus-cycles                                                    ( +-  0.01% )
          2122745      cache-misses              #    0.816 % of all cache refs      ( +-  0.07% )
        260062100      cache-references                                              ( +-  0.01% )
         60362006      branches                                                      ( +-  0.01% )
          4241670      branch-misses             #    7.03% of all branches          ( +-  0.01% )

        1.0041323 +- 0.0000282 seconds time elapsed  ( +-  0.00% )
</pre>
</div>
</div>

<div id="outline-container-Instruction-cache-misses" class="outline-3">
<h3 id="Instruction-cache-misses"><a href="#Instruction-cache-misses">Instruction cache misses</a></h3>
<div class="outline-text-3" id="text-Instruction-cache-misses">
<p>
It is very alarming that the L1-icache-load-misses is so high!
</p>

<pre class="example">
perf stat -C0 -r 3 -e L1-icache-load -e L1-icache-load-misses \
                   -e L1-dcache-loads -e L1-dcache-load-misses \
		   -e L1-dcache-stores -e L1-dcache-store-misses \
		   sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

         425783707      L1-icache-load                                                ( +-  0.15% )
          10713567      L1-icache-load-misses     #    2.52% of all L1-icache hits    ( +-  1.03% )
         258114938      L1-dcache-loads                                               ( +-  0.16% )
           1374091      L1-dcache-load-misses     #    0.53% of all L1-dcache hits    ( +-  0.27% )
         258115686      L1-dcache-stores                                              ( +-  0.16% )
           1374112      L1-dcache-store-misses                                        ( +-  0.27% )

         1.0040808 +- 0.0000513 seconds time elapsed  ( +-  0.01% )

Show adapter(s) (wan) statistics (ONLY that changed!)
Ethtool(wan     ) stat:      1227002 (      1,227,002) &lt;= hist_64bytes /sec
Ethtool(wan     ) stat:      1227002 (      1,227,002) &lt;= in_accepted /sec
Ethtool(wan     ) stat:      1227002 (      1,227,002) &lt;= in_da_unknown /sec
Ethtool(wan     ) stat:     80529040 (     80,529,040) &lt;= in_good_octets /sec
Ethtool(wan     ) stat:      1227002 (      1,227,002) &lt;= in_unicast /sec
Ethtool(wan     ) stat:     11454740 (     11,454,740) &lt;= rx_bytes /sec
Ethtool(wan     ) stat:       249016 (        249,016) &lt;= rx_packets /sec
</pre>
</div>
</div>

<div id="outline-container-kfree_skb-instead-of-napi_gro_receive" class="outline-3">
<h3 id="kfree_skb-instead-of-napi_gro_receive"><a href="#kfree_skb-instead-of-napi_gro_receive">kfree_skb instead of napi_gro_receive</a></h3>
<div class="outline-text-3" id="text-kfree_skb-instead-of-napi_gro_receive">
<p>
A quick test to estimate HW early drop speeds, is to simply replace
driver call to napi_gro_receive() with kfree_skb().
</p>

<p>
This early drop test show 519Kpps.
</p>

<pre class="example">
$ sar -n DEV 2 100
Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s    %ifutil
Average:         eth0 519360.00      0.50  34488.75      0.02      28.25
Average:        bond0      0.00      0.00      0.00      0.00       0.00
Average:           lo      0.50      0.50      0.04      0.04       0.00
Average:          wan      0.00      0.00      0.00      0.00       0.00
Average:         lan0      0.00      0.50      0.00      0.02       0.00
Average:         lan1      0.00      0.00      0.00      0.00       0.00
</pre>

<p>
This also reduce I-cache usage significantly.
</p>

<pre class="example">
Performance counter stats for 'CPU(s) 0' (3 runs):

        271648034      L1-icache-load                                                ( +-  0.00% )
           352879      L1-icache-load-misses     #    0.13% of all L1-icache hits    ( +-  0.36% )
        187930437      L1-dcache-loads                                               ( +-  0.01% )
          1327673      L1-dcache-load-misses     #    0.71% of all L1-dcache hits    ( +-  0.08% )
        187930908      L1-dcache-stores                                              ( +-  0.01% )
          1327679      L1-dcache-store-misses                                        ( +-  0.08% )

        1.0040270 +- 0.0000299 seconds time elapsed  ( +-  0.00% )
</pre>
</div>
</div>


<div id="outline-container-Use-netif_receive_skb_list" class="outline-3">
<h3 id="Use-netif_receive_skb_list"><a href="#Use-netif_receive_skb_list">Use netif_receive_skb_list</a></h3>
<div class="outline-text-3" id="text-Use-netif_receive_skb_list">
<p>
TODO: Test if using netif_receive_skb_list() helps performance.
As that is also an I-cache optimization.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2020-06-19 Fri 12:18</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
