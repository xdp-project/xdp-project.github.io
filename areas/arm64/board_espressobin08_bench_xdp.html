<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Benchmarks for Espressobin driver mvneta</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/hideshow.css"/>
<script type="text/javascript" src="/styles/bigblow/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/bigblow.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/hideshow.js"></script>
<script type="text/javascript" src="/styles/lib/js/jquery.stickytableheaders.min.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">Benchmarks for Espressobin driver mvneta</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Code-being-tested">Code being tested</a></li>
<li><a href="#Review-notes--Issues-with-patchset-code">Review notes: Issues with patchset/code</a>
<ul>
<li><a href="#Wrong-stats">Wrong stats</a>
<ul>
<li><a href="#Stats-issue-1--XDP_DROP-are-invisible">Stats-issue-1: XDP_DROP are invisible</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#Extra--Config-settings">Extra: Config settings</a></li>
<li><a href="#Initial-benchmark--Identify-DMA-sync-cost">Initial benchmark: Identify DMA sync cost</a>
<ul>
<li><a href="#iptables-drop">iptables drop</a></li>
<li><a href="#XDP_DROP-via-xdp1">XDP_DROP via xdp1</a></li>
<li><a href="#XDP_REDIRECT">XDP_REDIRECT</a></li>
</ul>
</li>
<li><a href="#Experiment--NO-DMA-sync-to-device">Experiment: NO DMA sync to device</a>
<ul>
<li><a href="#No-DMA-sync--iptables-raw-drop">No-DMA-sync: iptables raw-drop</a></li>
</ul>
</li>
<li><a href="#Experiment--1536-bytes-DMA-sync-to-device">Experiment: 1536 bytes DMA sync to device</a>
<ul>
<li><a href="#XDP_DROP--with-1536-bytes-DMA-sync">XDP_DROP: with 1536 bytes DMA sync</a></li>
<li><a href="#XDP_REDIRECT--with-1536-bytes-DMA-sync">XDP_REDIRECT: with 1536 bytes DMA sync</a></li>
</ul>
</li>
<li><a href="#Experiment--faster-XDP_DROP">Experiment: faster XDP_DROP</a>
<ul>
<li><a href="#XDP_DROP--with-page_pool_recycle_direct">XDP_DROP: with page_pool_recycle_direct</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
This document focus on benchmarking the patchset from Lorenzo Bianconi
</p>
<ul class="org-ul">
<li><a href="https://patchwork.ozlabs.org/project/netdev/list/?series=134430&amp;state=%2a">Patchset V1</a></li>
<li>Message-Id: &lt;cover.1570307172.git.lorenzo@kernel.org&gt;</li>
<li>Thread on <a href="https://lore.kernel.org/netdev/cover.1570307172.git.lorenzo@kernel.org/">lore.kernel.org</a></li>
</ul>

<p>
The patchset adds XDP support to the Marvell driver mvneta.
</p>

<p>
The benchmark will be performed on a <a href="https://espressobin.net/">EspressoBin</a> board.
</p>

<div id="outline-container-Code-being-tested" class="outline-2">
<h2 id="Code-being-tested"><a href="#Code-being-tested">Code being tested</a></h2>
<div class="outline-text-2" id="text-Code-being-tested">
<p>
Due to some adjustment to patchset V1, that involved DMA-sync to-device,
which is <b>VERY</b> expensive on this board, I've decided to create a separate
branch on a git tree (under my control), to keep this reproducible.
</p>

<p>
In flux Lorenzo GitHub tree with mvneta developement is here:
</p>
<ul class="org-ul">
<li><a href="https://github.com/LorenzoBianconi/net-next/tree/mvneta-devel">https://github.com/LorenzoBianconi/net-next/tree/mvneta-devel</a></li>
</ul>

<p>
More stable tree with patches I've tested:
</p>
<ul class="org-ul">
<li>Kernel.org git tree:</li>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/hawk/net-next-xdp.git/">https://git.kernel.org/pub/scm/linux/kernel/git/hawk/net-next-xdp.git/</a></li>
<li>Branch: xdp_mvneta01-bench</li>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/hawk/net-next-xdp.git/log/?h=xdp_mvneta01-bench">link to branch</a></li>
</ul>
</div>
</div>

<div id="outline-container-Review-notes--Issues-with-patchset-code" class="outline-2">
<h2 id="Review-notes--Issues-with-patchset-code"><a href="#Review-notes--Issues-with-patchset-code">Review notes: Issues with patchset/code</a></h2>
<div class="outline-text-2" id="text-Review-notes--Issues-with-patchset-code">
</div>
<div id="outline-container-Wrong-stats" class="outline-3">
<h3 id="Wrong-stats"><a href="#Wrong-stats">Wrong stats</a></h3>
<div class="outline-text-3" id="text-Wrong-stats">
<p>
The stats accounting is wrong. Other drivers also gets this wrong, and are
inconsistent. Upstream have not fully agree on the semantics.  Thus,
claiming it is "wrong" might be too harsh.
</p>
</div>

<div id="outline-container-Stats-issue-1--XDP_DROP-are-invisible" class="outline-4">
<h4 id="Stats-issue-1--XDP_DROP-are-invisible"><a href="#Stats-issue-1--XDP_DROP-are-invisible">Stats-issue-1: XDP_DROP are invisible</a></h4>
<div class="outline-text-4" id="text-Stats-issue-1--XDP_DROP-are-invisible">
<p>
Usually drivers (e.g. ixgbe + i40e) will still keep track of XDP_DROP frames
and count them as RX-frames in net_device stats, as XDP is a software drop,
and other counters deeper in network stack will not increment, which should
give enough info. The drivers that don't account as regular RX, will usually
have an ethtool stats.
</p>

<p>
During XDP_DROP (via <code>xdp1</code>), this drivers ethtool stats look like:
</p>
<pre class="example">
Show adapter(s) (eth0) statistics (ONLY that changed!)
Ethtool(eth0    ) stat:       959198 (        959,198) &lt;= frames_65_to_127_octets /sec
Ethtool(eth0    ) stat:       959199 (        959,199) &lt;= good_frames_received /sec
Ethtool(eth0    ) stat:     69062313 (     69,062,313) &lt;= good_octets_received /sec
Ethtool(eth0    ) stat:       959243 (        959,243) &lt;= p00_hist_65_127bytes /sec
Ethtool(eth0    ) stat:     69065516 (     69,065,516) &lt;= p00_out_octets /sec
Ethtool(eth0    ) stat:       959243 (        959,243) &lt;= p00_out_queue_0 /sec
Ethtool(eth0    ) stat:       959243 (        959,243) &lt;= p00_out_unicast /sec
Ethtool(eth0    ) stat:       537866 (        537,866) &lt;= rx_discard /sec
</pre>

<p>
The packet generator doesn't send faster than 959Kpps.
It's possible to deduce the XDP_DROP rate via subtraction:
</p>
<ul class="org-ul">
<li>959199-537866 = 421333 pps</li>
</ul>
</div>
</div>
</div>
</div>


<div id="outline-container-Extra--Config-settings" class="outline-2">
<h2 id="Extra--Config-settings"><a href="#Extra--Config-settings">Extra: Config settings</a></h2>
<div class="outline-text-2" id="text-Extra--Config-settings">
<p>
Enable kernel config setting: <code>CONFIG_ARM64_PSEUDO_NMI</code>
</p>

<p>
I've not setup the kernel parameter "irqchip.gicv3_pseudo_nmi" to 1, but I
want to have the option to enable this, as this allow for better perf
reports.
</p>

<p>
See why in Kernel-Recipes 2019 talk:
</p>
<ul class="org-ul">
<li><a href="https://kernel-recipes.org/en/2019/talks/no-nmi-no-problem-implementing-arm64-pseudo-nmi/">Implementing Arm64 Pseudo-NMI</a></li>
</ul>
</div>
</div>

<div id="outline-container-Initial-benchmark--Identify-DMA-sync-cost" class="outline-2">
<h2 id="Initial-benchmark--Identify-DMA-sync-cost"><a href="#Initial-benchmark--Identify-DMA-sync-cost">Initial benchmark: Identify DMA sync cost</a></h2>
<div class="outline-text-2" id="text-Initial-benchmark--Identify-DMA-sync-cost">
<p>
The initial benchmarking clearly show, that the primary cost for this
hardware/driver is due to DMA-sync for device (kernel function
<code>dma_direct_sync_single_for_device</code> and resulting cache invalidation).
</p>

<p>
The code line that cause this overhead:
</p>
<div class="org-src-container">
<pre class="src src-diff">$ git diff
diff --git a/drivers/net/ethernet/marvell/mvneta.c b/drivers/net/ethernet/marvell/mvneta.c
index 3e036642257c..ed4a0843cb05 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/drivers/net/ethernet/marvell/mvneta.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/drivers/net/ethernet/marvell/mvneta.c</span>
<span style="font-weight: bold;">@@ -1857,6 +1857,7 @@</span><span style="font-weight: bold;"> static int mvneta_rx_refill(struct mvneta_port *pp,</span>

        phys_addr = page_pool_get_dma_addr(page) + pp-&gt;rx_offset_correction;
        dma_dir = page_pool_get_dma_dir(rxq-&gt;page_pool);
+       // Below: DMA-sync costly: notice big area 'MVNETA_MAX_RX_BUF_SIZE'
        dma_sync_single_for_device(pp-&gt;dev-&gt;dev.parent, phys_addr,
                                   MVNETA_MAX_RX_BUF_SIZE, dma_dir);
        mvneta_rx_desc_fill(rx_desc, phys_addr, page, rxq);
</pre>
</div>
</div>

<div id="outline-container-iptables-drop" class="outline-3">
<h3 id="iptables-drop"><a href="#iptables-drop">iptables drop</a></h3>
<div class="outline-text-3" id="text-iptables-drop">
<pre class="example">
espressobin:~# iptables -t raw -I PREROUTING -p udp --dport 9 -j DROP
root@espressobin:~# nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    151169             0.0
IpExtInOctets                   6953544            0.0
IpExtInNoECTPkts                151165             0.0
</pre>

<p>
Perf report for iptables-raw drop:
</p>
<pre class="example">
#
# Samples: 35K of event 'cycles:ppp'
# Event count (approx.): 8632918197
#
# Overhead  CPU  Command          Symbol
# ........  ...  ...............  .....................................
#
    18.89%  000  ksoftirqd/0      [k] mvneta_poll
    12.29%  000  ksoftirqd/0      [k] __pi___inval_dcache_area
            |
            ---__pi___inval_dcache_area
                   |--9.69%--dma_direct_sync_single_for_device
                   |          mvneta_rx_refill.isra.74
                   |          mvneta_poll
                   |          net_rx_action
                    --2.61%--dma_direct_sync_single_for_cpu
                              mvneta_poll

     5.80%  000  ksoftirqd/0      [k] __netif_receive_skb_core
     4.36%  000  ksoftirqd/0      [k] ipt_do_table
     4.01%  000  ksoftirqd/0      [k] eth_type_trans
     3.71%  000  ksoftirqd/0      [k] get_page_from_freelist
     2.83%  000  ksoftirqd/0      [k] dev_gro_receive
     2.44%  000  ksoftirqd/0      [k] ip_rcv_core.isra.17
     1.71%  000  ksoftirqd/0      [k] free_unref_page
     1.61%  000  ksoftirqd/0      [k] kmem_cache_alloc
     1.58%  000  ksoftirqd/0      [k] skb_release_data
     1.57%  000  ksoftirqd/0      [k] kmem_cache_free
     1.53%  000  ksoftirqd/0      [k] __netif_receive_skb_one_core
     1.51%  000  ksoftirqd/0      [k] edsa_rcv
     1.37%  000  ksoftirqd/0      [k] netif_receive_skb_internal
     1.28%  000  ksoftirqd/0      [k] __page_pool_put_page
     1.21%  000  ksoftirqd/0      [k] udp_mt
     1.18%  000  ksoftirqd/0      [k] __alloc_pages_nodemask
     1.15%  000  ksoftirqd/0      [k] dsa_switch_rcv
     1.12%  000  ksoftirqd/0      [k] __rcu_read_unlock
     1.09%  000  ksoftirqd/0      [k] ktime_get_with_offset
     1.06%  000  ksoftirqd/0      [k] __rcu_read_lock
     1.01%  000  ksoftirqd/0      [k] free_unref_page_prepare.part.77
     1.00%  000  ksoftirqd/0      [k] bpf_skb_load_helper_16
     0.99%  000  ksoftirqd/0      [k] build_skb
     0.96%  000  ksoftirqd/0      [k] dma_direct_map_page
     0.87%  000  ksoftirqd/0      [k] slabinfo_write
     0.87%  000  ksoftirqd/0      [k] mvneta_rx_refill.isra.74
     0.86%  000  ksoftirqd/0      [k] ip_rcv
     0.85%  000  ksoftirqd/0      [k] page_frag_free
     0.83%  000  ksoftirqd/0      [k] __build_skb
     0.76%  000  ksoftirqd/0      [k] __local_bh_enable_ip
     0.71%  000  ksoftirqd/0      [k] memmove
     0.69%  000  ksoftirqd/0      [k] page_pool_alloc_pages
     0.68%  000  ksoftirqd/0      [k] __page_pool_clean_page
     0.62%  000  ksoftirqd/0      [k] netif_receive_skb
     0.61%  000  ksoftirqd/0      [k] __page_pool_alloc_pages_slow
     0.59%  000  ksoftirqd/0      [k] __netif_receive_skb
</pre>
</div>
</div>

<div id="outline-container-XDP_DROP-via-xdp1" class="outline-3">
<h3 id="XDP_DROP-via-xdp1"><a href="#XDP_DROP-via-xdp1">XDP_DROP via xdp1</a></h3>
<div class="outline-text-3" id="text-XDP_DROP-via-xdp1">
<pre class="example">
root@espressobin:~/samples/bpf# ./xdp1 3
proto 0:      27797 pkt/s
proto 0:     421419 pkt/s
proto 0:     421444 pkt/s
proto 0:     421393 pkt/s
proto 0:     421440 pkt/s
proto 0:     421184 pkt/s
</pre>

<p>
Perf report during xdp1 dropping ALL packets:
</p>
<pre class="example">
perf report --sort cpu,comm,symbol --no-children --stdio -g none
# Samples: 16K of event 'cycles:ppp'
# Event count (approx.): 3976182320
#
# Overhead  CPU  Command          Symbol
# ........  ...  ...............  .............................................
#
    32.96%  000  ksoftirqd/0      [k] mvneta_poll
    26.88%  000  ksoftirqd/0      [k] __pi___clean_dcache_area_poc
            |
            ---__pi___clean_dcache_area_poc
               dma_direct_sync_single_for_device
               mvneta_rx_refill.isra.74
               mvneta_poll
               net_rx_action

     7.29%  000  ksoftirqd/0      [k] 0xffff8000000b04d4  (&lt;-- BPF-prog)
     7.09%  000  ksoftirqd/0      [k] __pi___inval_dcache_area
            |
            ---__pi___inval_dcache_area
               dma_direct_sync_single_for_cpu
               mvneta_poll
               net_rx_action

     5.82%  000  ksoftirqd/0      [k] __xdp_return
     2.49%  000  ksoftirqd/0      [k] mvneta_rx_refill.isra.74
     1.43%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_cpu
     1.40%  000  ksoftirqd/0      [k] __page_pool_put_page
     1.29%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_device
     1.25%  000  ksoftirqd/0      [k] page_pool_alloc_pages
     1.23%  000  ksoftirqd/0      [k] __softirqentry_text_start
     1.08%  000  ksoftirqd/0      [k] percpu_array_map_lookup_elem
     0.95%  000  ksoftirqd/0      [k] arch_sync_dma_for_device
     0.93%  000  ksoftirqd/0      [k] arch_sync_dma_for_cpu
     0.74%  000  ksoftirqd/0      [k] __rcu_read_unlock
     0.61%  000  ksoftirqd/0      [k] xdp_mem_id_cmp
     0.60%  000  ksoftirqd/0      [k] __rcu_read_lock
     0.57%  000  ksoftirqd/0      [k] xdp_return_buff
     0.40%  000  ksoftirqd/0      [k] xdp_mem_id_hashfn
</pre>
</div>
</div>

<div id="outline-container-XDP_REDIRECT" class="outline-3">
<h3 id="XDP_REDIRECT"><a href="#XDP_REDIRECT">XDP_REDIRECT</a></h3>
<div class="outline-text-3" id="text-XDP_REDIRECT">
<p>
Doing redirect out same interface:
</p>
<pre class="example">
root@espressobin:~/samples/bpf# ./xdp_redirect_map 3 3
input: 3 output: 3
Kernel error message: XDP program already attached
WARN: link set xdp fd failed on 3
map[0] (vports) = 4, map[1] (map) = 5, map[2] (count) = 0
ifindex 3:     212760 pkt/s
ifindex 3:     263269 pkt/s
ifindex 3:     263348 pkt/s
ifindex 3:     263334 pkt/s
</pre>

<p>
Check if packets are getting transmitted, although only via stats:
</p>
<pre class="example">
sar -n DEV 2 100
Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s     %ifutil
Average:           lo      0.00      0.00      0.00      0.00        0.00
Average:        bond0      0.00      0.00      0.00      0.00        0.00
Average:         eth0 263049.67 263050.67  17468.14  17468.21       14.31
Average:          wan      0.00      0.00      0.00      0.00        0.00
Average:         lan0      0.00      0.00      0.00      0.00        0.00
Average:         lan1      0.00      0.00      0.00      0.00        0.00
</pre>

<p>
Perf report:
</p>
<pre class="example">
# Samples: 16K of event 'cycles:ppp'
# Event count (approx.): 3971147150
#
# Overhead  CPU  Command          Symbol
# ........  ...  ...............  .............................................
#
    19.37%  000  ksoftirqd/0      [k] mvneta_poll
    17.23%  000  ksoftirqd/0      [k] mvneta_txq_bufs_free.isra.78
    13.92%  000  ksoftirqd/0      [k] __pi___clean_dcache_area_poc
     6.44%  000  ksoftirqd/0      [k] __xdp_return
     4.09%  000  ksoftirqd/0      [k] __pi___inval_dcache_area
     4.05%  000  ksoftirqd/0      [k] 0xffff8000000b0c70
     3.11%  000  ksoftirqd/0      [k] xdp_return_frame
     2.58%  000  ksoftirqd/0      [k] __page_pool_put_page
     2.57%  000  ksoftirqd/0      [k] dev_map_enqueue
     2.51%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_cpu
     1.72%  000  ksoftirqd/0      [k] __lock_text_start
     1.68%  000  ksoftirqd/0      [k] mvneta_xdp_submit_frame.part.80
     1.47%  000  ksoftirqd/0      [k] mvneta_rx_refill.isra.74
     1.41%  000  ksoftirqd/0      [k] dma_direct_unmap_page
     1.38%  000  ksoftirqd/0      [k] arch_sync_dma_for_cpu
     1.17%  000  ksoftirqd/0      [k] xdp_do_redirect
     1.10%  000  ksoftirqd/0      [k] bpf_xdp_redirect_map
     1.00%  000  ksoftirqd/0      [k] __rcu_read_unlock
     0.95%  000  ksoftirqd/0      [k] _raw_write_lock_irqsave
     0.88%  000  ksoftirqd/0      [k] page_pool_alloc_pages
     0.88%  000  ksoftirqd/0      [k] dma_direct_map_page
     0.87%  000  ksoftirqd/0      [k] _raw_spin_lock
     0.81%  000  ksoftirqd/0      [k] __rcu_read_lock
     0.75%  000  ksoftirqd/0      [k] arch_sync_dma_for_device
     0.72%  000  ksoftirqd/0      [k] percpu_array_map_lookup_elem
     0.64%  000  ksoftirqd/0      [k] __dev_map_lookup_elem
     0.64%  000  ksoftirqd/0      [k] xdp_mem_id_cmp
     0.57%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_device
     0.56%  000  ksoftirqd/0      [k] net_rx_action
     0.53%  000  ksoftirqd/0      [k] __softirqentry_text_start
     0.51%  000  ksoftirqd/0      [k] xdp_mem_id_hashfn
     0.32%  000  ksoftirqd/0      [k] mvneta_xdp_xmit
     0.23%  000  ksoftirqd/0      [k] bq_xmit_all
</pre>
</div>
</div>
</div>

<div id="outline-container-Experiment--NO-DMA-sync-to-device" class="outline-2">
<h2 id="Experiment--NO-DMA-sync-to-device"><a href="#Experiment--NO-DMA-sync-to-device">Experiment: NO DMA sync to device</a></h2>
<div class="outline-text-2" id="text-Experiment--NO-DMA-sync-to-device">
<p>
Testing the overhead of the DMA-sync for device <code>dma_sync_single_for_device</code>
via simply commenting the code out.  Like this:
</p>

<div class="org-src-container">
<pre class="src src-diff">diff --git a/drivers/net/ethernet/marvell/mvneta.c b/drivers/net/ethernet/marvell/mvneta.c
index ed4a0843cb05..75b206036511 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/drivers/net/ethernet/marvell/mvneta.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/drivers/net/ethernet/marvell/mvneta.c</span>
<span style="font-weight: bold;">@@ -1858,8 +1858,8 @@</span><span style="font-weight: bold;"> static int mvneta_rx_refill(struct mvneta_port *pp,</span>
        phys_addr = page_pool_get_dma_addr(page) + pp-&gt;rx_offset_correction;
        dma_dir = page_pool_get_dma_dir(rxq-&gt;page_pool);
        // Below: DMA-sync costly: notice too big area 'MVNETA_MAX_RX_BUF_SIZE'
-       dma_sync_single_for_device(pp-&gt;dev-&gt;dev.parent, phys_addr,
-                                  MVNETA_MAX_RX_BUF_SIZE, dma_dir);
+//     dma_sync_single_for_device(pp-&gt;dev-&gt;dev.parent, phys_addr,
+//                                MVNETA_MAX_RX_BUF_SIZE, dma_dir);
        mvneta_rx_desc_fill(rx_desc, phys_addr, page, rxq);

        return 0;
</pre>
</div>


<pre class="example">
root@espressobin:~/samples/bpf# ./xdp1 3
proto 0:     608377 pkt/s
proto 0:     607360 pkt/s
proto 0:     607625 pkt/s
proto 0:     607455 pkt/s
proto 0:     607368 pkt/s
proto 0:     607460 pkt/s
</pre>

<p>
Performance difference is HUGH:
</p>
<ul class="org-ul">
<li>Before: 421419 pps &#x2013; (1/421419*10^9) = 2373 ns</li>
<li>Now   : 607460 pps &#x2013; (1/607460*10^9) = 1646 ns</li>
<li>Diff  : 186041 pps &#x2013;                    727 ns</li>
</ul>

<p>
The size of MVNETA_MAX_RX_BUF_SIZE = 3520 bytes
</p>
<ul class="org-ul">
<li>3520 / 64 = 55 cachelines</li>
<li>Cost per cacheline(?): 727 ns / 55 = 13.22 ns (that seems too low?)</li>
</ul>

<p>
Perf result for No-DMA-sync: XDP_DROP
</p>
<pre class="example">
# Overhead  CPU  Command          Symbol
# ........  ...  ...............  ...................................
#
    48.82%  000  ksoftirqd/0      [k] mvneta_poll
    10.45%  000  ksoftirqd/0      [k] __pi___inval_dcache_area
     9.38%  000  ksoftirqd/0      [k] 0xffff8000000ae574
     9.02%  000  ksoftirqd/0      [k] __xdp_return
     2.21%  000  ksoftirqd/0      [k] page_pool_alloc_pages
     2.13%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_cpu
     2.08%  000  ksoftirqd/0      [k] __page_pool_put_page
     1.82%  000  ksoftirqd/0      [k] percpu_array_map_lookup_elem
     1.35%  000  ksoftirqd/0      [k] arch_sync_dma_for_cpu
     1.27%  000  ksoftirqd/0      [k] __softirqentry_text_start
     1.22%  000  ksoftirqd/0      [k] xdp_mem_id_cmp
     1.05%  000  ksoftirqd/0      [k] __rcu_read_unlock
     0.94%  000  ksoftirqd/0      [k] __rcu_read_lock
     0.87%  000  ksoftirqd/0      [k] xdp_return_buff
     0.70%  000  ksoftirqd/0      [k] xdp_mem_id_hashfn
     0.54%  000  ksoftirqd/0      [k] rcu_qs
     0.26%  000  ksoftirqd/0      [k] rcu_softirq_qs
</pre>
</div>

<div id="outline-container-No-DMA-sync--iptables-raw-drop" class="outline-3">
<h3 id="No-DMA-sync--iptables-raw-drop"><a href="#No-DMA-sync--iptables-raw-drop">No-DMA-sync: iptables raw-drop</a></h3>
<div class="outline-text-3" id="text-No-DMA-sync--iptables-raw-drop">
<p>
Drop via iptables:
</p>
<ul class="org-ul">
<li><code>iptables -t raw -I PREROUTING -p udp --dport 9 -j DROP</code></li>
</ul>

<pre class="example">
root@espressobin:~/samples/bpf# nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    175990             0.0
IpExtInOctets                   8095770            0.0
IpExtInNoECTPkts                175994             0.0
</pre>
</div>
</div>
</div>

<div id="outline-container-Experiment--1536-bytes-DMA-sync-to-device" class="outline-2">
<h2 id="Experiment--1536-bytes-DMA-sync-to-device"><a href="#Experiment--1536-bytes-DMA-sync-to-device">Experiment: 1536 bytes DMA sync to device</a></h2>
<div class="outline-text-2" id="text-Experiment--1536-bytes-DMA-sync-to-device">
<p>
This experiment change the size of the DMA sync to device to be 1536 bytes,
the usual Ethernet MTU aligned to 64 bytes.
</p>

<p>
The git diff:
</p>
<div class="org-src-container">
<pre class="src src-diff">$ git diff
diff --git a/drivers/net/ethernet/marvell/mvneta.c b/drivers/net/ethernet/marvell/mvneta.c
index ed4a0843cb05..7c5bfdf429c4 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/drivers/net/ethernet/marvell/mvneta.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/drivers/net/ethernet/marvell/mvneta.c</span>
<span style="font-weight: bold;">@@ -1859,7 +1859,9 @@</span><span style="font-weight: bold;"> static int mvneta_rx_refill(struct mvneta_port *pp,</span>
        dma_dir = page_pool_get_dma_dir(rxq-&gt;page_pool);
        // Below: DMA-sync costly: notice too big area 'MVNETA_MAX_RX_BUF_SIZE'
        dma_sync_single_for_device(pp-&gt;dev-&gt;dev.parent, phys_addr,
-                                  MVNETA_MAX_RX_BUF_SIZE, dma_dir);
+                                  //MVNETA_MAX_RX_BUF_SIZE,
+                                  1536 /* 24x64 */,
+                                  dma_dir);
        mvneta_rx_desc_fill(rx_desc, phys_addr, page, rxq);

        return 0;
</pre>
</div>
</div>

<div id="outline-container-XDP_DROP--with-1536-bytes-DMA-sync" class="outline-3">
<h3 id="XDP_DROP--with-1536-bytes-DMA-sync"><a href="#XDP_DROP--with-1536-bytes-DMA-sync">XDP_DROP: with 1536 bytes DMA sync</a></h3>
<div class="outline-text-3" id="text-XDP_DROP--with-1536-bytes-DMA-sync">
<pre class="example">
root@espressobin:~/samples/bpf# ./xdp1 3 
proto 0:     479068 pkt/s
proto 0:     479874 pkt/s
proto 0:     479837 pkt/s
proto 0:     479968 pkt/s
proto 0:     479936 pkt/s
proto 0:     479936 pkt/s
</pre>

<p>
Performance difference to full DMA-sync (3520 bytes) is:
</p>
<ul class="org-ul">
<li>Before: 421419 pps &#x2013; (1/421419*10^9) = 2373 ns</li>
<li>Now   : 479936 pps &#x2013; (1/479936*10^9) = 2084 ns</li>
<li>Diff  :  58517 pps &#x2013; 2373-2084       =  289 ns</li>
</ul>

<p>
New DMA-sync size is 1536. As mentioned above the previous size were
MVNETA_MAX_RX_BUF_SIZE = 3520 bytes. This is a reduction of (3520-1536) =
1984 bytes.
</p>

<p>
The size reduction 1984 bytes:
</p>
<ul class="org-ul">
<li>1984 / 64 = 31 cachelines</li>
<li>Reduced cost per cacheline: 289 ns / 31 = 9.32 ns</li>
<li>Hmm&#x2026; this does not correlate with above 13.22 ns</li>
</ul>

<p>
Going from above zero-DMA sync to 1536 bytes
</p>
<ul class="org-ul">
<li>Zero-sync: 607460 pps &#x2013; (1/607460*10^9) = 1646 ns</li>
<li>1536-sync: 479936 pps &#x2013; (1/479936*10^9) = 2084 ns</li>
<li>Diff     : 127524 pps &#x2013;                 =  438 ns</li>
<li>438 / 24 cachelines = 18.25 ns cost per cacheline(?)</li>
<li>Hmm&#x2026; cannot conclude or correlate on this result</li>
</ul>

<pre class="example">
# Samples: 16K of event 'cycles:ppp'
# Event count (approx.): 3992737701
#
# Overhead  CPU  Command          Symbol
# ........  ...  ...............  ................................................
#
    37.71%  000  ksoftirqd/0      [k] mvneta_poll
    16.69%  000  ksoftirqd/0      [k] __pi___clean_dcache_area_poc
     7.84%  000  ksoftirqd/0      [k] __pi___inval_dcache_area
     7.71%  000  ksoftirqd/0      [k] 0xffff8000000b0d2c  (BPF-prog)
     6.92%  000  ksoftirqd/0      [k] __xdp_return
     2.86%  000  ksoftirqd/0      [k] mvneta_rx_refill.isra.74
     1.73%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_device
     1.68%  000  ksoftirqd/0      [k] __page_pool_put_page
     1.56%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_cpu
     1.47%  000  ksoftirqd/0      [k] page_pool_alloc_pages
     1.39%  000  ksoftirqd/0      [k] percpu_array_map_lookup_elem
     1.13%  000  ksoftirqd/0      [k] __softirqentry_text_start
     1.08%  000  ksoftirqd/0      [k] arch_sync_dma_for_device
     0.97%  000  ksoftirqd/0      [k] arch_sync_dma_for_cpu
     0.84%  000  ksoftirqd/0      [k] xdp_mem_id_cmp
     0.76%  000  ksoftirqd/0      [k] __rcu_read_unlock
     0.73%  000  ksoftirqd/0      [k] __rcu_read_lock
     0.67%  000  ksoftirqd/0      [k] xdp_return_buff
     0.52%  000  ksoftirqd/0      [k] xdp_mem_id_hashfn
</pre>
</div>
</div>

<div id="outline-container-XDP_REDIRECT--with-1536-bytes-DMA-sync" class="outline-3">
<h3 id="XDP_REDIRECT--with-1536-bytes-DMA-sync"><a href="#XDP_REDIRECT--with-1536-bytes-DMA-sync">XDP_REDIRECT: with 1536 bytes DMA sync</a></h3>
<div class="outline-text-3" id="text-XDP_REDIRECT--with-1536-bytes-DMA-sync">
<pre class="example">
# ./xdp_redirect_map 3 3 
input: 3 output: 3
Kernel error message: XDP program already attached
WARN: link set xdp fd failed on 3
map[0] (vports) = 4, map[1] (map) = 5, map[2] (count) = 0
ifindex 3:     201397 pkt/s
ifindex 3:     284291 pkt/s
ifindex 3:     284412 pkt/s
ifindex 3:     284384 pkt/s
ifindex 3:     284418 pkt/s
</pre>

<p>
Performance difference is:
</p>
<ul class="org-ul">
<li>Before: 263334 pps &#x2013; (1/263334*10^9) = 3798 ns</li>
<li>Now   : 284418 pps &#x2013; (1/284418*10^9) = 3516 ns</li>
<li>Diff  :  21084 pps &#x2013; (3798-3516)     =  282 ns</li>
</ul>

<p>
This correlate well with above XDP_DROP reduction diff which were 289 ns.
</p>

<p>
Perf report XDP_REDIRECT with 1536 bytes DMA sync
</p>
<pre class="example">
# Samples: 16K of event 'cycles:ppp'
# Event count (approx.): 3968126744
#
# Overhead  CPU  Command          Symbol
# ........  ...  ...............  .............................................
#
    22.90%  000  ksoftirqd/0      [k] mvneta_poll
    17.07%  000  ksoftirqd/0      [k] __pi___clean_dcache_area_poc
            |
            ---__pi___clean_dcache_area_poc
               |
               |--10.19%--dma_direct_sync_single_for_device
               |          mvneta_rx_refill.isra.74
               |          mvneta_poll
               |          net_rx_action
               |          __softirqentry_text_start
               |          run_ksoftirqd
               |          smpboot_thread_fn
               |          kthread
               |          ret_from_fork
               |
                --6.89%--dma_direct_map_page
                          mvneta_xdp_submit_frame.part.80
                          mvneta_xdp_xmit
                          bq_xmit_all

    10.70%  000  ksoftirqd/0      [k] mvneta_txq_bufs_free.isra.78
     4.87%  000  ksoftirqd/0      [k] 0xffff8000000b467c
     4.82%  000  ksoftirqd/0      [k] __pi___inval_dcache_area
     3.85%  000  ksoftirqd/0      [k] __xdp_return
     2.89%  000  ksoftirqd/0      [k] mvneta_xdp_submit_frame.part.80
     2.62%  000  ksoftirqd/0      [k] dev_map_enqueue
     2.20%  000  ksoftirqd/0      [k] xdp_return_frame
     2.02%  000  ksoftirqd/0      [k] mvneta_rx_refill.isra.74
     1.92%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_cpu
     1.77%  000  ksoftirqd/0      [k] page_pool_alloc_pages
     1.76%  000  ksoftirqd/0      [k] dma_direct_map_page
     1.52%  000  ksoftirqd/0      [k] xdp_do_redirect
     1.45%  000  ksoftirqd/0      [k] __page_pool_put_page
     1.40%  000  ksoftirqd/0      [k] arch_sync_dma_for_device
     1.35%  000  ksoftirqd/0      [k] bpf_xdp_redirect_map
     1.27%  000  ksoftirqd/0      [k] __lock_text_start
     1.20%  000  ksoftirqd/0      [k] arch_sync_dma_for_cpu
     0.87%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_device
     0.80%  000  ksoftirqd/0      [k] dma_direct_unmap_page
     0.74%  000  ksoftirqd/0      [k] mvneta_xdp_xmit
     0.66%  000  ksoftirqd/0      [k] percpu_array_map_lookup_elem
     0.65%  000  ksoftirqd/0      [k] net_rx_action
     0.64%  000  ksoftirqd/0      [k] __softirqentry_text_start
     0.60%  000  ksoftirqd/0      [k] _raw_write_lock_irqsave
     0.59%  000  ksoftirqd/0      [k] _raw_spin_lock
     0.57%  000  ksoftirqd/0      [k] __rcu_read_lock
     0.56%  000  ksoftirqd/0      [k] __dev_map_lookup_elem
     0.55%  000  ksoftirqd/0      [k] bq_xmit_all
     0.52%  000  ksoftirqd/0      [k] __rcu_read_unlock
     0.50%  000  ksoftirqd/0      [k] xdp_mem_id_cmp
     0.32%  000  ksoftirqd/0      [k] xdp_mem_id_hashfn
</pre>
</div>
</div>
</div>

<div id="outline-container-Experiment--faster-XDP_DROP" class="outline-2">
<h2 id="Experiment--faster-XDP_DROP"><a href="#Experiment--faster-XDP_DROP">Experiment: faster XDP_DROP</a></h2>
<div class="outline-text-2" id="text-Experiment--faster-XDP_DROP">
<p>
Current patchset uses <code>xdp_return_buff</code> in XDP_DROP code-path, instead of
<code>page_pool_recycle_direct(rxq-&gt;page_pool, page)</code>.
</p>

<p>
The problem with <code>xdp_return_buff()</code> is that it does a "full" lookup from
the mem.id (<code>xdp_buff-&gt;xdp_rxq_info-&gt;mem.id</code>) to find the "allocator" pointer
in this case the <code>page_pool</code> pointer. Here in the driver we already have
access to the stable allocator page_pool pointer via struct mvneta_rx_queue
<code>rxq-&gt;page_pool</code>.
</p>

<p>
Experiment code change git diff:
</p>
<div class="org-src-container">
<pre class="src src-diff">$ git diff
diff --git a/drivers/net/ethernet/marvell/mvneta.c b/drivers/net/ethernet/marvell/mvneta.c
index ed4a0843cb05..4fb7b6eb22d9 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/drivers/net/ethernet/marvell/mvneta.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/drivers/net/ethernet/marvell/mvneta.c</span>
<span style="font-weight: bold;">@@ -1858,8 +1858,8 @@</span><span style="font-weight: bold;"> static int mvneta_rx_refill(struct mvneta_port *pp,</span>
        phys_addr = page_pool_get_dma_addr(page) + pp-&gt;rx_offset_correction;
        dma_dir = page_pool_get_dma_dir(rxq-&gt;page_pool);
        // Below: DMA-sync costly: notice too big area 'MVNETA_MAX_RX_BUF_SIZE'
-       dma_sync_single_for_device(pp-&gt;dev-&gt;dev.parent, phys_addr,
-                                  MVNETA_MAX_RX_BUF_SIZE, dma_dir);
+//     dma_sync_single_for_device(pp-&gt;dev-&gt;dev.parent, phys_addr,
+//                                MVNETA_MAX_RX_BUF_SIZE, dma_dir)
        mvneta_rx_desc_fill(rx_desc, phys_addr, page, rxq);

        return 0;
<span style="font-weight: bold;">@@ -2081,7 +2081,7 @@</span><span style="font-weight: bold;"> mvneta_xdp_xmit(struct net_device *dev, int num_frame,</span>

 static int
 mvneta_run_xdp(struct mvneta_port *pp, struct bpf_prog *prog,
-              struct xdp_buff *xdp)
+              struct xdp_buff *xdp, struct mvneta_rx_queue *rxq)
 {
        u32 ret, act = bpf_prog_run_xdp(prog, xdp);

<span style="font-weight: bold;">@@ -2112,10 +2112,14 @@</span><span style="font-weight: bold;"> mvneta_run_xdp(struct mvneta_port *pp, struct bpf_prog *prog,</span>
        case XDP_ABORTED:
                trace_xdp_exception(pp-&gt;dev, prog, act);
                /* fall through */
-       case XDP_DROP:
+       case XDP_DROP: {
+               struct page *page = virt_to_head_page(xdp-&gt;data);
+
+               page_pool_recycle_direct(rxq-&gt;page_pool, page);
                ret = MVNETA_XDP_CONSUMED;
-               xdp_return_buff(xdp);
+               // xdp_return_buff(xdp);
                break;
+               }
        }

        return ret;
<span style="font-weight: bold;">@@ -2160,7 +2164,7 @@</span><span style="font-weight: bold;"> mvneta_swbm_rx_frame(struct mvneta_port *pp,</span>
        if (xdp_prog) {
                u32 ret;

-               ret = mvneta_run_xdp(pp, xdp_prog, &amp;xdp);
+               ret = mvneta_run_xdp(pp, xdp_prog, &amp;xdp, rxq);
                if (ret == MVNETA_XDP_REDIR ||
                    ret == MVNETA_XDP_TX)
                        mvneta_update_stats(pp, 1, xdp.data_end - xdp.data,
</pre>
</div>
</div>

<div id="outline-container-XDP_DROP--with-page_pool_recycle_direct" class="outline-3">
<h3 id="XDP_DROP--with-page_pool_recycle_direct"><a href="#XDP_DROP--with-page_pool_recycle_direct">XDP_DROP: with page_pool_recycle_direct</a></h3>
<div class="outline-text-3" id="text-XDP_DROP--with-page_pool_recycle_direct">
<p>
Result XDP_DROP via samples/bpf/xdp1
</p>
<pre class="example">
root@espressobin:~/samples/bpf# ./xdp1 3 
proto 0:     698619 pkt/s
proto 0:     695690 pkt/s
proto 0:     695632 pkt/s
proto 0:     695461 pkt/s
proto 0:     695579 pkt/s
</pre>

<p>
Given this ALSO disabled the DMA-sync, it should be compared to the
zero-sync performance.
</p>
<ul class="org-ul">
<li>Zero-sync: 607460 pps &#x2013; (1/607460*10^9) = 1646 ns</li>
<li>Now      : 695461 pps &#x2013; (1/695461)*10^9 = 1438 ns</li>
<li>Diff     :  88001 pps &#x2013;                 =  208 ns</li>
</ul>


<p>
The perf report symbols have decreased a lot:
</p>
<pre class="example">
# Samples: 16K of event 'cycles:ppp'
# Event count (approx.): 3931584629
#
# Overhead  CPU  Command         Symbol
# ........  ...  ..............  ......................................
#
    57.31%  000  ksoftirqd/0     [k] mvneta_poll
    12.21%  000  ksoftirqd/0     [k] 0xffff8000000b0088
    11.67%  000  ksoftirqd/0     [k] __pi___inval_dcache_area
     2.80%  000  ksoftirqd/0     [k] dma_direct_sync_single_for_cpu
     2.55%  000  ksoftirqd/0     [k] __page_pool_put_page
     2.11%  000  ksoftirqd/0     [k] page_pool_alloc_pages
     1.78%  000  ksoftirqd/0     [k] percpu_array_map_lookup_elem
     1.72%  000  ksoftirqd/0     [k] __softirqentry_text_start
     1.49%  000  ksoftirqd/0     [k] arch_sync_dma_for_cpu
     0.32%  000  ksoftirqd/0     [k] rcu_softirq_qs
     0.31%  000  ksoftirqd/0     [k] net_rx_action
     0.30%  000  ksoftirqd/0     [k] 0xffff8000000b01ec
     0.27%  000  ksoftirqd/0     [k] rcu_qs
</pre>

<p>
Perf stat, show low 0.27 insn per cycle, which is disappointing:
</p>
<pre class="example">
perf stat -C0 -r 3 sleep 1
root@espressobin:~/samples/bpf# perf stat -C0 -r 3 sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

    1004.44 msec cpu-clock          #    1.000 CPUs utilized    ( +-  0.00% )
          5      context-switches   #    4.648 M/sec            ( +- 57.14% )
          0      cpu-migrations     #    0.000 K/sec
          0      page-faults        #    0.000 K/sec
 1004416732      cycles             # 1000415.072 GHz           ( +-  0.00% )
  267246942      instructions       #    0.27  insn per cycle   ( +-  0.01% )
   20612533      branches           # 20530411.355 M/sec        ( +-  0.00% )
     227425      branch-misses      #    1.10% of all branches  ( +-  0.27% )

 1.00456320 +- 0.00000735 seconds time elapsed  ( +-  0.00% )
</pre>

<p>
The 0.27 instructions per cycle is disappointing, as we can see, the CPU
runs at 1000MHz, but is in-principle operating at 267 MHz.  I was hoping to
see more efficiency with this very simple workload.
</p>

<p>
There are 227,425 branch-misses, but with a packet rate of 695,461 pps, then
there is only a branch-misses for every 3rd packet (695461/227425 = 3.05).
The number of observed (20,612,533) branches is a little high for this
constrained workload, 384 per packet (267246942/695461).
</p>

<p>
The workload is extremely small and should not activate many functions, as
can be seen in perf report above.  We hope/expect that the instruction cache
can contain the workload.
</p>

<pre class="example">
# perf stat -C0 -r 3 -e L1-icache-load -e L1-icache-load-misses -e L1-dcache-loads -e L1-dcache-load-misses sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

  156084164   L1-icache-load                                       ( +-  0.00% )
     241615   L1-icache-load-misses  # 0.15% of all L1-icache hits ( +-  0.07% )
   95235780   L1-dcache-loads                                      ( +-  0.00% )
    3062412   L1-dcache-load-misses  # 3.22% of all L1-dcache hits ( +-  0.03% )

  1.0044705 +- 0.0000496 seconds time elapsed  ( +-  0.00% )
</pre>

<p>
The 241615 L1-icache-load-misses is less-than the 695461 packets per sec.
The total 156084164 L1-icache-load per packet is (156084164/695461) 224
loads, of 64 bytes each (64*224) gives 14336 bytes.  Thus, it does look like
instruction cache does fit.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2020-03-03 Tue 13:45</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
