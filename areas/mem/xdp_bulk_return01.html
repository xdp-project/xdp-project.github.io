<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Testing XDP redirect bulk return API</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/hideshow.css"/>
<script type="text/javascript" src="/styles/bigblow/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/bigblow.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/hideshow.js"></script>
<script type="text/javascript" src="/styles/lib/js/jquery.stickytableheaders.min.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">Testing XDP redirect bulk return API</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Baseline-testing">Baseline testing</a>
<ul>
<li><a href="#XDP_TX-test">XDP_TX test</a></li>
<li><a href="#XDP-redirect-bounce--baseline-">XDP-redirect bounce (baseline)</a></li>
</ul>
</li>
<li><a href="#Testing-patchset">Testing patchset</a>
<ul>
<li><a href="#Testing-patchset--XDP_TX-test">XDP_TX test</a></li>
<li><a href="#XDP-redirect-bounce">XDP-redirect bounce</a>
<ul>
<li><a href="#Details--perf-stat-measurement">Details: perf stat measurement</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#Optimize-code">Optimize code</a>
<ul>
<li><a href="#Fix-bulk-in-mlx5-driver-code">Fix bulk in mlx5 driver code</a></li>
<li><a href="#Optimize-code--attempt-02">Optimize code: attempt 02</a></li>
<li><a href="#Optimize-code--attempt-02---results">Optimize code: attempt 02 - results</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
Upstream patchset proposal
 <a href="https://lore.kernel.org/netdev/cover.1604686496.git.lorenzo@kernel.org/">https://lore.kernel.org/netdev/cover.1604686496.git.lorenzo@kernel.org/</a>
</p>

<p>
Testing this git tree + branch:
</p>
<ul class="org-ul">
<li><a href="https://github.com/LorenzoBianconi/net-next/tree/xdp_bulk_tx_return_jesper">https://github.com/LorenzoBianconi/net-next/tree/xdp_bulk_tx_return_jesper</a></li>
</ul>

<p>
From cover letter:
</p>
<blockquote>
<p>
XDP bulk APIs introduce a defer/flush mechanism to return
pages belonging to the same xdp_mem_allocator object
(identified via the mem.id field) in bulk to optimize
I-cache and D-cache since xdp_return_frame is usually run
inside the driver NAPI tx completion loop.
</p>

<p>
Convert mvneta, mvpp2 and mlx5 drivers to xdp_return_frame_bulk APIs.
</p>
</blockquote>


<div id="outline-container-Baseline-testing" class="outline-2">
<h2 id="Baseline-testing"><a href="#Baseline-testing">Baseline testing</a></h2>
<div class="outline-text-2" id="text-Baseline-testing">
<div class="org-src-container">
<pre class="src src-sh">[broadwell ~]$ uname -r -v
5.10.0-rc2-lorenzo-baseline-for-bulk+ <span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">4 SMP PREEMPT Mon Nov 9 11:46:02 CET 2020</span>
</pre>
</div>

<p>
Testing at kernel commit bff6f1db91e330d7fba56f815cdbc412c75fe163
</p>

<p>
Generator running:
</p>
<div class="org-src-container">
<pre class="src src-sh">[firesoul pktgen]$ ./pktgen_sample03_burst_single_flow.sh -vi mlx5p1 -d 198.18.1.1 -m ec:0d:9a:db:11:c4 -t 12 
</pre>
</div>

<p>
Generator sending speed:
</p>
<ul class="org-ul">
<li>Ethtool(mlx5p1  ) stat: 44992839 ( 44,992,839) &lt;= tx_packets /sec</li>
<li>Ethtool(mlx5p1  ) stat: 44993648 ( 44,993,648) &lt;= tx_packets_phy /sec</li>
</ul>
</div>

<div id="outline-container-XDP_TX-test" class="outline-3">
<h3 id="XDP_TX-test"><a href="#XDP_TX-test">XDP_TX test</a></h3>
<div class="outline-text-3" id="text-XDP_TX-test">
<p>
This XDP_TX should not really be affected by this optimisation, but it shows
the max/upper performance bound achievable with a single RX-to-TX queue. In
principle this can be used to deduce the overhead of XDP_REDIRECT net-core
code infrastructure.
</p>

<p>
Max XDP_TX performance:
</p>
<pre class="example" id="orgdaa3352">
sudo ./xdp_rxq_info --dev mlx5p1 --act XDP_TX
[...]
Running XDP on dev:mlx5p1 (ifindex:7) action:XDP_TX options:swapmac
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       16,046,587  0          
XDP-RX CPU      total   16,046,587 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    1:1   16,046,598  0          
rx_queue_index    1:sum 16,046,598 
</pre>

<p>
Verifying the actual transmitted packets, as above XDP counter can only
count the RX-events. Something is wrong, as we see only 9,184,940
tx_packets_phy/sec (phy level transmits).
</p>

<pre class="example" id="org41ac0a3">
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:     253253 (        253,253) &lt;= ch1_poll /sec
Ethtool(mlx5p1  ) stat:     253253 (        253,253) &lt;= ch_poll /sec
Ethtool(mlx5p1  ) stat:   16208223 (     16,208,223) &lt;= rx1_cache_reuse /sec
Ethtool(mlx5p1  ) stat:    7023234 (      7,023,234) &lt;= rx1_xdp_drop /sec
Ethtool(mlx5p1  ) stat:     143515 (        143,515) &lt;= rx1_xdp_tx_cqes /sec
Ethtool(mlx5p1  ) stat:    7023234 (      7,023,234) &lt;= rx1_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:    9184953 (      9,184,953) &lt;= rx1_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:     861089 (        861,089) &lt;= rx1_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:     975901 (        975,901) &lt;= rx1_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:    9184953 (      9,184,953) &lt;= rx1_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:   44354640 (     44,354,640) &lt;= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat: 2838694013 (  2,838,694,013) &lt;= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   16208181 (     16,208,181) &lt;= rx_cache_reuse /sec
Ethtool(mlx5p1  ) stat:   28146411 (     28,146,411) &lt;= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:   44354594 (     44,354,594) &lt;= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat: 2838703690 (  2,838,703,690) &lt;= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:   44354740 (     44,354,740) &lt;= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat: 2661276172 (  2,661,276,172) &lt;= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:   44354604 (     44,354,604) &lt;= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:    7023223 (      7,023,223) &lt;= rx_xdp_drop /sec
Ethtool(mlx5p1  ) stat:     143515 (        143,515) &lt;= rx_xdp_tx_cqe /sec
Ethtool(mlx5p1  ) stat:    7023222 (      7,023,222) &lt;= rx_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:    9184949 (      9,184,949) &lt;= rx_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:     861089 (        861,089) &lt;= rx_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:     975901 (        975,901) &lt;= rx_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:    9184949 (      9,184,949) &lt;= rx_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:  587836166 (    587,836,166) &lt;= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:    9184940 (      9,184,940) &lt;= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:  587836799 (    587,836,799) &lt;= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:    9184951 (      9,184,951) &lt;= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:  551096207 (    551,096,207) &lt;= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:    9184940 (      9,184,940) &lt;= tx_vport_unicast_packets /sec
</pre>
</div>
</div>

<div id="outline-container-XDP-redirect-bounce--baseline-" class="outline-3">
<h3 id="XDP-redirect-bounce--baseline-"><a href="#XDP-redirect-bounce--baseline-">XDP-redirect bounce (baseline)</a></h3>
<div class="outline-text-3" id="text-XDP-redirect-bounce--baseline-">
<p>
Use XDP-redirect to act like XDP_TX and redirect packets back-out same
interface. This is done as-a-test to keep same net_device in cache and same
driver code (I-cache) active.
</p>

<p>
Testing xdp_redirect_map:
</p>
<pre class="example" id="org3a4d44a">
jbrouer@broadwell kernel-bpf-samples]$ sudo ./xdp_redirect_map mlx5p1 mlx5p1
input: 7 output: 7
libbpf: Kernel error message: XDP program already attached
WARN: link set xdp fd failed on 7
ifindex 7:    8900610 pkt/s
ifindex 7:    8996142 pkt/s
ifindex 7:    8985280 pkt/s
ifindex 7:    8980360 pkt/s
ifindex 7:    8988103 pkt/s
</pre>

<p>
Ethtool stats to verify packets are transmitted:
</p>
<pre class="example" id="org17ceeb5">
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:       140436 (        140,436) &lt;= ch1_poll /sec
Ethtool(mlx5p1  ) stat:       140436 (        140,436) &lt;= ch_poll /sec
Ethtool(mlx5p1  ) stat:      8987891 (      8,987,891) &lt;= rx1_cache_empty /sec
Ethtool(mlx5p1  ) stat:      8987880 (      8,987,880) &lt;= rx1_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:     44748662 (     44,748,662) &lt;= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   2863921010 (  2,863,921,010) &lt;= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      8987894 (      8,987,894) &lt;= rx_cache_empty /sec
Ethtool(mlx5p1  ) stat:     35760982 (     35,760,982) &lt;= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:     44748762 (     44,748,762) &lt;= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat:   2863907667 (  2,863,907,667) &lt;= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     44748558 (     44,748,558) &lt;= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:   2684927295 (  2,684,927,295) &lt;= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     44748791 (     44,748,791) &lt;= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:      8987876 (      8,987,876) &lt;= rx_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:       140435 (        140,435) &lt;= tx1_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       280871 (        280,871) &lt;= tx1_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       608555 (        608,555) &lt;= tx1_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      8987882 (      8,987,882) &lt;= tx1_xdp_xmit /sec
Ethtool(mlx5p1  ) stat:    575223027 (    575,223,027) &lt;= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      8987852 (      8,987,852) &lt;= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:    575219919 (    575,219,919) &lt;= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:      8987809 (      8,987,809) &lt;= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:    539271092 (    539,271,092) &lt;= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:      8987852 (      8,987,852) &lt;= tx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:       140436 (        140,436) &lt;= tx_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       280871 (        280,871) &lt;= tx_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       608555 (        608,555) &lt;= tx_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      8987878 (      8,987,878) &lt;= tx_xdp_xmit /sec
</pre>

<p>
Baseline: choosing TX-pps at phy level
</p>
<ul class="org-ul">
<li>Baseline: 8987852 ( 8,987,852) &lt;= tx_packets_phy /sec</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-Testing-patchset" class="outline-2">
<h2 id="Testing-patchset"><a href="#Testing-patchset">Testing patchset</a></h2>
<div class="outline-text-2" id="text-Testing-patchset">
<p>
Upstream patchset proposal
 <a href="https://lore.kernel.org/netdev/cover.1604686496.git.lorenzo@kernel.org/">https://lore.kernel.org/netdev/cover.1604686496.git.lorenzo@kernel.org/</a>
</p>

<p>
Testing this git tree + branch:
</p>
<ul class="org-ul">
<li><a href="https://github.com/LorenzoBianconi/net-next/tree/xdp_bulk_tx_return_jesper">https://github.com/LorenzoBianconi/net-next/tree/xdp_bulk_tx_return_jesper</a></li>
</ul>
</div>

<div id="outline-container-Testing-patchset--XDP_TX-test" class="outline-3">
<h3 id="Testing-patchset--XDP_TX-test"><a href="#Testing-patchset--XDP_TX-test">XDP_TX test</a></h3>
<div class="outline-text-3" id="text-Testing-patchset--XDP_TX-test">
<pre class="example" id="orge5cf6ac">
sudo ./xdp_rxq_info --dev mlx5p1 --act XDP_TX
[...]
Running XDP on dev:mlx5p1 (ifindex:7) action:XDP_TX options:swapmac
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       15,282,307  0          
XDP-RX CPU      total   15,282,307 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    5:5   15,282,316  0          
rx_queue_index    5:sum 15,282,316 
</pre>

<p>
Something is still wrong on actual transmitted packet size, so even-though
this slower than baseline, I'm not sure that these XDP_TX results or counter
from the XDP side can be trusted.
</p>

<p>
Ethtool stats to verify packets are transmitted:
</p>
<pre class="example" id="org7d5fb06">
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:       233354 (        233,354) &lt;= ch5_poll /sec
Ethtool(mlx5p1  ) stat:       233355 (        233,355) &lt;= ch_poll /sec
Ethtool(mlx5p1  ) stat:          424 (            424) &lt;= rx5_cache_empty /sec
Ethtool(mlx5p1  ) stat:          424 (            424) &lt;= rx5_cache_full /sec
Ethtool(mlx5p1  ) stat:     14934253 (     14,934,253) &lt;= rx5_cache_reuse /sec
Ethtool(mlx5p1  ) stat:      5581066 (      5,581,066) &lt;= rx5_xdp_drop /sec
Ethtool(mlx5p1  ) stat:       146150 (        146,150) &lt;= rx5_xdp_tx_cqes /sec
Ethtool(mlx5p1  ) stat:      5581066 (      5,581,066) &lt;= rx5_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:      9349435 (      9,349,435) &lt;= rx5_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:       876639 (        876,639) &lt;= rx5_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:       993656 (        993,656) &lt;= rx5_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:      9353579 (      9,353,579) &lt;= rx5_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:     44387430 (     44,387,430) &lt;= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   2840787378 (  2,840,787,378) &lt;= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:          424 (            424) &lt;= rx_cache_empty /sec
Ethtool(mlx5p1  ) stat:          424 (            424) &lt;= rx_cache_full /sec
Ethtool(mlx5p1  ) stat:     14934296 (     14,934,296) &lt;= rx_cache_reuse /sec
Ethtool(mlx5p1  ) stat:     29452362 (     29,452,362) &lt;= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:     44387303 (     44,387,303) &lt;= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat:   2840830503 (  2,840,830,503) &lt;= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     44387977 (     44,387,977) &lt;= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:   2663234382 (  2,663,234,382) &lt;= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     44387241 (     44,387,241) &lt;= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:      5581087 (      5,581,087) &lt;= rx_xdp_drop /sec
Ethtool(mlx5p1  ) stat:       146150 (        146,150) &lt;= rx_xdp_tx_cqe /sec
Ethtool(mlx5p1  ) stat:      5581087 (      5,581,087) &lt;= rx_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:      9349461 (      9,349,461) &lt;= rx_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:       876641 (        876,641) &lt;= rx_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:       993657 (        993,657) &lt;= rx_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:      9353605 (      9,353,605) &lt;= rx_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:    598635096 (    598,635,096) &lt;= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      9353671 (      9,353,671) &lt;= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:    598644630 (    598,644,630) &lt;= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:      9353822 (      9,353,822) &lt;= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:    561220066 (    561,220,066) &lt;= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:      9353664 (      9,353,664) &lt;= tx_vport_unicast_packets /sec
</pre>
</div>
</div>

<div id="outline-container-XDP-redirect-bounce" class="outline-3">
<h3 id="XDP-redirect-bounce"><a href="#XDP-redirect-bounce">XDP-redirect bounce</a></h3>
<div class="outline-text-3" id="text-XDP-redirect-bounce">
<p>
Testing xdp_redirect_map back-out same interface.
</p>

<pre class="example" id="orgac51a1b">
brouer@broadwell kernel-bpf-samples]$ sudo ./xdp_redirect_map mlx5p1 mlx5p1
input: 7 output: 7
libbpf: Kernel error message: XDP program already attached
WARN: link set xdp fd failed on 7
ifindex 7:    9475880 pkt/s
ifindex 7:    9548121 pkt/s
ifindex 7:    9548336 pkt/s
ifindex 7:    9545295 pkt/s
</pre>

<p>
Ethtool stats to verify packets are transmitted:
</p>
<pre class="example" id="org27d27ce">
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:       149101 (        149,101) &lt;= ch5_poll /sec
Ethtool(mlx5p1  ) stat:       149101 (        149,101) &lt;= ch_poll /sec
Ethtool(mlx5p1  ) stat:      9542483 (      9,542,483) &lt;= rx5_cache_empty /sec
Ethtool(mlx5p1  ) stat:      9542491 (      9,542,491) &lt;= rx5_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:     44715715 (     44,715,715) &lt;= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   2861806193 (  2,861,806,193) &lt;= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      9542483 (      9,542,483) &lt;= rx_cache_empty /sec
Ethtool(mlx5p1  ) stat:     35173205 (     35,173,205) &lt;= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:     44715726 (     44,715,726) &lt;= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat:   2861812704 (  2,861,812,704) &lt;= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     44715823 (     44,715,823) &lt;= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:   2682943266 (  2,682,943,266) &lt;= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     44715721 (     44,715,721) &lt;= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:      9542499 (      9,542,499) &lt;= rx_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:       149102 (        149,102) &lt;= tx5_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       298203 (        298,203) &lt;= tx5_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       646106 (        646,106) &lt;= tx5_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      9542490 (      9,542,490) &lt;= tx5_xdp_xmit /sec
Ethtool(mlx5p1  ) stat:    610718914 (    610,718,914) &lt;= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      9542483 (      9,542,483) &lt;= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:    610720267 (    610,720,267) &lt;= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:      9542504 (      9,542,504) &lt;= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:    572550251 (    572,550,251) &lt;= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:      9542504 (      9,542,504) &lt;= tx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:       149102 (        149,102) &lt;= tx_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       298203 (        298,203) &lt;= tx_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       646106 (        646,106) &lt;= tx_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      9542499 (      9,542,499) &lt;= tx_xdp_xmit /sec
</pre>

<p>
Comparing TX-pps at phy level
</p>
<ul class="org-ul">
<li>Baseline: 8987852 ( 8,987,852) &lt;= tx_packets_phy /sec</li>
<li>Patchset: 9542483 ( 9,542,483) &lt;= tx_packets_phy /sec</li>
<li>Packets:   554631 (   554,631) pps faster</li>
<li>Nanosec:   6.4667 ns (1/8987852-1/9542483)*10^9</li>
<li>Percent :  +6.17% faster ((9542483/8987852)-1)*100</li>
</ul>

<p>
These numbers are lower than I measured before in earlier iterations
(10165419 pkt/s (10,165,419 pps)) of patchset. And non-patched was 8514144
pkt/s (8,514,144 pps).  Something need investing.
</p>
</div>

<div id="outline-container-Details--perf-stat-measurement" class="outline-4">
<h4 id="Details--perf-stat-measurement"><a href="#Details--perf-stat-measurement">Details: perf stat measurement</a></h4>
<div class="outline-text-4" id="text-Details--perf-stat-measurement">
<pre class="example" id="orgd3b7bda">
$ perf stat -C5 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -r 4 sleep 1

 Performance counter stats for 'CPU(s) 5' (4 runs):

     3.803.620.734      cycles                                                        ( +-  0,01% )
     9.950.204.056      instructions              #    2,62  insn per cycle           ( +-  0,01% )
        83.226.329      cache-references                                              ( +-  0,02% )
               544      cache-misses              #    0,001 % of all cache refs      ( +- 46,55% )
     1.826.738.580      branches:k                                                    ( +-  0,01% )
         3.226.436      branch-misses:k           #    0,18% of all branches          ( +-  0,20% )
           423.246      l2_rqsts.all_code_rd                                          ( +-  1,44% )
            58.690      l2_rqsts.code_rd_hit                                          ( +-  2,91% )
           364.548      l2_rqsts.code_rd_miss                                         ( +-  1,30% )
           359.595      L1-icache-load-misses                                         ( +-  0,79% )

          1,000988 +- 0,000149 seconds time elapsed  ( +-  0,01% )
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-Optimize-code" class="outline-2">
<h2 id="Optimize-code"><a href="#Optimize-code">Optimize code</a></h2>
<div class="outline-text-2" id="text-Optimize-code">
</div>
<div id="outline-container-Fix-bulk-in-mlx5-driver-code" class="outline-3">
<h3 id="Fix-bulk-in-mlx5-driver-code"><a href="#Fix-bulk-in-mlx5-driver-code">Fix bulk in mlx5 driver code</a></h3>
<div class="outline-text-3" id="text-Fix-bulk-in-mlx5-driver-code">
<p>
Fix to mlx5
</p>
<div class="org-src-container">
<pre class="src src-diff">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
index 5fdfbf390d5c..3a5c1845243c 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c</span>
<span style="font-weight: bold;">@@ -366,13 +366,12 @@</span><span style="font-weight: bold;"> mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq *sq, struct mlx5e_xmit_data *xdptxd,</span>
 static void mlx5e_free_xdpsq_desc(struct mlx5e_xdpsq *sq,
                                  struct mlx5e_xdp_wqe_info *wi,
                                  u32 *xsk_frames,
-                                 bool recycle)
+                                 bool recycle,
+                                 struct xdp_frame_bulk *bq)
 {
        struct mlx5e_xdp_info_fifo *xdpi_fifo = &amp;sq-&gt;db.xdpi_fifo;
-       struct xdp_frame_bulk bq;
        u16 i;

-       bq.xa = NULL;
        for (i = 0; i &lt; wi-&gt;num_pkts; i++) {
                struct mlx5e_xdp_info xdpi = mlx5e_xdpi_fifo_pop(xdpi_fifo);

<span style="font-weight: bold;">@@ -381,7 +380,7 @@</span><span style="font-weight: bold;"> static void mlx5e_free_xdpsq_desc(struct mlx5e_xdpsq *sq,</span>
                        /* XDP_TX from the XSK RQ and XDP_REDIRECT */
                        dma_unmap_single(sq-&gt;pdev, xdpi.frame.dma_addr,
                                         xdpi.frame.xdpf-&gt;len, DMA_TO_DEVICE);
-                       xdp_return_frame_bulk(xdpi.frame.xdpf, &amp;bq);
+                       xdp_return_frame_bulk(xdpi.frame.xdpf, bq);
                        break;
                case MLX5E_XDP_XMIT_MODE_PAGE:
                        /* XDP_TX from the regular RQ */
<span style="font-weight: bold;">@@ -395,7 +394,7 @@</span><span style="font-weight: bold;"> static void mlx5e_free_xdpsq_desc(struct mlx5e_xdpsq *sq,</span>
                        WARN_ON_ONCE(true);
                }
        }
-       xdp_flush_frame_bulk(&amp;bq);
+       // xdp_flush_frame_bulk(&amp;bq); // Wrong place
 }

 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq)
<span style="font-weight: bold;">@@ -406,6 +405,9 @@</span><span style="font-weight: bold;"> bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq)</span>
        u16 sqcc;
        int i;

+       struct xdp_frame_bulk bq;
+       bq.xa = NULL;
+
        sq = container_of(cq, struct mlx5e_xdpsq, cq);

        if (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &amp;sq-&gt;state)))
<span style="font-weight: bold;">@@ -437,7 +439,7 @@</span><span style="font-weight: bold;"> bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq)</span>

                        sqcc += wi-&gt;num_wqebbs;

-                       mlx5e_free_xdpsq_desc(sq, wi, &amp;xsk_frames, true);
+                       mlx5e_free_xdpsq_desc(sq, wi, &amp;xsk_frames, true, &amp;bq);
                } while (!last_wqe);

                if (unlikely(get_cqe_opcode(cqe) != MLX5_CQE_REQ)) {
<span style="font-weight: bold;">@@ -450,6 +452,8 @@</span><span style="font-weight: bold;"> bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq)</span>
                }
        } while ((++i &lt; MLX5E_TX_CQ_POLL_BUDGET) &amp;&amp; (cqe = mlx5_cqwq_get_cqe(&amp;cq-&gt;wq)));

+       xdp_flush_frame_bulk(&amp;bq);
+
        if (xsk_frames)
                xsk_tx_completed(sq-&gt;xsk_pool, xsk_frames);

<span style="font-weight: bold;">@@ -468,6 +472,11 @@</span><span style="font-weight: bold;"> void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq)</span>
 {
        u32 xsk_frames = 0;

+       struct xdp_frame_bulk bq;
+       bq.xa = NULL;
+
+       rcu_read_lock(); /* need for xdp_return_frame_bulk */
+
        while (sq-&gt;cc != sq-&gt;pc) {
                struct mlx5e_xdp_wqe_info *wi;
                u16 ci;
<span style="font-weight: bold;">@@ -477,9 +486,12 @@</span><span style="font-weight: bold;"> void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq)</span>

                sq-&gt;cc += wi-&gt;num_wqebbs;

-               mlx5e_free_xdpsq_desc(sq, wi, &amp;xsk_frames, false);
+               mlx5e_free_xdpsq_desc(sq, wi, &amp;xsk_frames, false, &amp;bq);
        }

+       xdp_flush_frame_bulk(&amp;bq);
+       rcu_read_unlock();
+
        if (xsk_frames)
                xsk_tx_completed(sq-&gt;xsk_pool, xsk_frames);
 }

</pre>
</div>
</div>
</div>

<div id="outline-container-Optimize-code--attempt-02" class="outline-3">
<h3 id="Optimize-code--attempt-02"><a href="#Optimize-code--attempt-02">Optimize code: attempt 02</a></h3>
<div class="outline-text-3" id="text-Optimize-code--attempt-02">
<p>
Change to page_pool:
</p>

<div class="org-src-container">
<pre class="src src-diff">diff --git a/net/core/page_pool.c b/net/core/page_pool.c
index 31dac2ad4a1f..4cba11ff6f41 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/net/core/page_pool.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/net/core/page_pool.c</span>
<span style="font-weight: bold;">@@ -364,7 +364,7 @@</span><span style="font-weight: bold;"> static bool pool_page_reusable(struct page_pool *pool, struct page *page)</span>
  * If the page refcnt != 1, then the page will be returned to memory
  * subsystem.
  */
-static struct page *
+static __always_inline struct page *
 __page_pool_put_page(struct page_pool *pool, struct page *page,
                     unsigned int dma_sync_size, bool allow_direct)
 {
<span style="font-weight: bold;">@@ -435,7 +435,7 @@</span><span style="font-weight: bold;"> void page_pool_put_page_bulk(struct page_pool *pool, void **data,</span>
                        data[bulk_len++] = page;
        }

-       if (!bulk_len)
+       if (unlikely(!bulk_len))
                return;

        /* Bulk producer into ptr_ring page_pool cache */
<span style="font-weight: bold;">@@ -446,6 +446,9 @@</span><span style="font-weight: bold;"> void page_pool_put_page_bulk(struct page_pool *pool, void **data,</span>
        }
        page_pool_ring_unlock(pool);

+       if (likely(!pa_len))
+               return;
+
        /* ptr_ring cache full, free pages outside producer lock since
         * put_page() with refcnt == 1 can be an expensive operation
         */

</pre>
</div>

<p>
Changes to xdp.c:
</p>

<div class="org-src-container">
<pre class="src src-diff">diff --git a/net/core/xdp.c b/net/core/xdp.c
index ff7c801bd40c..a5ab053ae178 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/net/core/xdp.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/net/core/xdp.c</span>
<span style="font-weight: bold;">@@ -402,6 +402,7 @@</span><span style="font-weight: bold;"> void xdp_flush_frame_bulk(struct xdp_frame_bulk *bq)</span>
 }
 EXPORT_SYMBOL_GPL(xdp_flush_frame_bulk);

+/* Must be called with rcu_read_lock held */
 void xdp_return_frame_bulk(struct xdp_frame *xdpf,
                           struct xdp_frame_bulk *bq)
 {
<span style="font-weight: bold;">@@ -413,8 +414,6 @@</span><span style="font-weight: bold;"> void xdp_return_frame_bulk(struct xdp_frame *xdpf,</span>
                return;
        }

-       rcu_read_lock();
-
        xa = bq-&gt;xa;
        if (unlikely(!xa)) {
                xa = rhashtable_lookup(mem_id_ht, &amp;mem-&gt;id, mem_id_rht_params);
<span style="font-weight: bold;">@@ -425,14 +424,12 @@</span><span style="font-weight: bold;"> void xdp_return_frame_bulk(struct xdp_frame *xdpf,</span>
        if (bq-&gt;count == XDP_BULK_QUEUE_SIZE)
                xdp_flush_frame_bulk(bq);

-       if (mem-&gt;id != xa-&gt;mem.id) {
+       if (unlikely(mem-&gt;id != xa-&gt;mem.id)) {
                xdp_flush_frame_bulk(bq);
                bq-&gt;xa = rhashtable_lookup(mem_id_ht, &amp;mem-&gt;id, mem_id_rht_params);
        }

        bq-&gt;q[bq-&gt;count++] = xdpf-&gt;data;
-
-       rcu_read_unlock();
 }
 EXPORT_SYMBOL_GPL(xdp_return_frame_bulk);
</pre>
</div>
</div>
</div>

<div id="outline-container-Optimize-code--attempt-02---results" class="outline-3">
<h3 id="Optimize-code--attempt-02---results"><a href="#Optimize-code--attempt-02---results">Optimize code: attempt 02 - results</a></h3>
<div class="outline-text-3" id="text-Optimize-code--attempt-02---results">
<pre class="example" id="org744b41d">
sudo ./xdp_redirect_map mlx5p1 mlx5p1
[...]
ifindex 7:   10258717 pkt/s
ifindex 7:   10263252 pkt/s
ifindex 7:   10255251 pkt/s
ifindex 7:   10256871 pkt/s
ifindex 7:   10263568 pkt/s
</pre>

<pre class="example" id="org8f3664e">
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:       160388 (        160,388) &lt;= ch1_poll /sec
Ethtool(mlx5p1  ) stat:       160388 (        160,388) &lt;= ch_poll /sec
Ethtool(mlx5p1  ) stat:     10264823 (     10,264,823) &lt;= rx1_cache_empty /sec
Ethtool(mlx5p1  ) stat:     10264802 (     10,264,802) &lt;= rx1_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:     45316225 (     45,316,225) &lt;= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   2900232288 (  2,900,232,288) &lt;= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:     10264799 (     10,264,799) &lt;= rx_cache_empty /sec
Ethtool(mlx5p1  ) stat:     35051252 (     35,051,252) &lt;= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:     45316130 (     45,316,130) &lt;= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat:   2900223009 (  2,900,223,009) &lt;= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     45315989 (     45,315,989) &lt;= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:   2718966819 (  2,718,966,819) &lt;= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     45316114 (     45,316,114) &lt;= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:     10264815 (     10,264,815) &lt;= rx_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:       160387 (        160,387) &lt;= tx1_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       320775 (        320,775) &lt;= tx1_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       695015 (        695,015) &lt;= tx1_xdp_nops /sec
Ethtool(mlx5p1  ) stat:     10264802 (     10,264,802) &lt;= tx1_xdp_xmit /sec
Ethtool(mlx5p1  ) stat:    656948779 (    656,948,779) &lt;= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:     10264844 (     10,264,844) &lt;= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:    656948652 (    656,948,652) &lt;= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     10264823 (     10,264,823) &lt;= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:    615889361 (    615,889,361) &lt;= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     10264823 (     10,264,823) &lt;= tx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:       160388 (        160,388) &lt;= tx_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       320775 (        320,775) &lt;= tx_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       695011 (        695,011) &lt;= tx_xdp_nops /sec
Ethtool(mlx5p1  ) stat:     10264812 (     10,264,812) &lt;= tx_xdp_xmit /sec
</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2021-09-17 Fri 14:04</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
