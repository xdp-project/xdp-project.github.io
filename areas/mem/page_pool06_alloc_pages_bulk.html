<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Testing page-allocator API proposal for alloc_pages_bulk</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/hideshow.css"/>
<script type="text/javascript" src="/styles/bigblow/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/bigblow.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/hideshow.js"></script>
<script type="text/javascript" src="/styles/lib/js/jquery.stickytableheaders.min.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">Testing page-allocator API proposal for alloc_pages_bulk</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Use-case--page_pool">Use-case: page_pool</a>
<ul>
<li><a href="#Test-setup-for-veth">Test setup for veth</a></li>
</ul>
</li>
<li><a href="#Baseline-test">Baseline test</a>
<ul>
<li><a href="#baseline01--veth-redirect">baseline01: veth redirect</a></li>
</ul>
</li>
<li><a href="#Using-alloc_pages_bulk">Using alloc_pages_bulk</a>
<ul>
<li><a href="#test01--veth-redirect--page_pool-bulk-16-">test01: veth redirect (page_pool bulk 16)</a></li>
<li><a href="#test02--veth-redirect--page_pool-bulk-64-">test02: veth redirect (page_pool bulk 64)</a></li>
</ul>
</li>
<li><a href="#Test-with-RFC-patchset">Test with RFC patchset</a>
<ul>
<li><a href="#initial-test">initial test</a></li>
</ul>
</li>
<li><a href="#patch-notes">patch notes</a>
<ul>
<li><a href="#Follow-up-to-Mel-s-patchset">Follow up to Mel's patchset</a></li>
<li><a href="#bench-test-veth">bench test veth</a></li>
<li><a href="#desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path">desc: net: page_pool: use alloc_pages_bulk in refill code path</a></li>
</ul>
</li>
<li><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v4r2">Test on Mel git-tree: mm-bulk-rebase-v4r2</a>
<ul>
<li><a href="#stg-mail">stg mail</a></li>
<li><a href="#Updated-patch">Updated patch</a></li>
<li><a href="#Pop-patch-using-__alloc_pages_bulk">Pop patch using __alloc_pages_bulk</a></li>
</ul>
</li>
<li><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v5r9">Test on Mel git-tree: mm-bulk-rebase-v5r9</a>
<ul>
<li><a href="#micro-benchmark--page_bench04_bulk">micro-benchmark: page_bench04_bulk</a></li>
</ul>
</li>
<li><a href="#Micro-optimisations">Micro optimisations</a>
<ul>
<li><a href="#Observations">Observations</a></li>
<li><a href="#Why-happening">Why happening</a></li>
<li><a href="#Explaining-patch-with-fix">Explaining patch with fix</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
Testing experimental patch from Mel Gorman that add alloc_pages_bulk() API.
</p>

<p>
This patch: <a href="https://lore.kernel.org/linux-mm/20210210130705.GC3629@suse.de/">https://lore.kernel.org/linux-mm/20210210130705.GC3629@suse.de/</a>
With this fix: <a href="https://lore.kernel.org/linux-mm/20210211091235.GC3697@techsingularity.net/">https://lore.kernel.org/linux-mm/20210211091235.GC3697@techsingularity.net/</a>
</p>

<p>
Notice new patchset from Mel:
</p>
<ul class="org-ul">
<li>Message-Id: &lt;20210224102603.19524-1-mgorman@techsingularity.net&gt;</li>
<li><a href="https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/">https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/</a></li>
</ul>

<div id="outline-container-Use-case--page_pool" class="outline-2">
<h2 id="Use-case--page_pool"><a href="#Use-case--page_pool">Use-case: page_pool</a></h2>
<div class="outline-text-2" id="text-Use-case--page_pool">
<p>
The <code>alloc_pages_bulk()</code> use-case for page_pool is in
<code>__page_pool_alloc_pages_slow()</code>, for then the pool goes empty.
</p>

<p>
The mlx5 driver can easily empty the page_pool "cache", when redirected
xdp_frame's are converted into SKBs.
</p>

<p>
This happens both for veth and cpumap, as SKBs (created from xdp_frame) is
missing callback to page_pool.
</p>
</div>

<div id="outline-container-Test-setup-for-veth" class="outline-3">
<h3 id="Test-setup-for-veth"><a href="#Test-setup-for-veth">Test setup for veth</a></h3>
<div class="outline-text-3" id="text-Test-setup-for-veth">
<p>
It requires some setup and two XDP programs to trigger the case, where
page_pool "cache" goes empty. And use of driver mlx5.
</p>

<p>
First: Create veth pair and enabled link up:
</p>

<pre class="example">
ip link add veth1 type veth peer name veth2
ip link set veth1 up
ip link set veth2 up
</pre>

<p>
Disable GRO/GSO/TSO on the veth devices
</p>
<pre class="example" id="orgbc6dc65">
ethtool -K veth1 gso off gro off tso off
ethtool -K veth2 gso off gro off tso off
</pre>

<p>
When XDP-redirecting into a veth, we must remember to attach an XDP prog to
the peer device.
</p>

<p>
Redirect frame from mlx5p1 into veth1 (peer veth2)
</p>
<ul class="org-ul">
<li>sudo ./xdp_redirect_map mlx5p1 veth1</li>
</ul>

<p>
Create SKBs from xdp_frame via XDP_PASS on veth2:
</p>
<ul class="org-ul">
<li>sudo ./xdp_rxq_info &#x2013;dev veth2 &#x2013;act XDP_PASS</li>
</ul>

<p>
As the MAC-addr doesn't match the SKB packets are dropped very early, but it
suites our micro-benchmark test case.
</p>
</div>
</div>
</div>

<div id="outline-container-Baseline-test" class="outline-2">
<h2 id="Baseline-test"><a href="#Baseline-test">Baseline test</a></h2>
<div class="outline-text-2" id="text-Baseline-test">
</div>
<div id="outline-container-baseline01--veth-redirect" class="outline-3">
<h3 id="baseline01--veth-redirect"><a href="#baseline01--veth-redirect">baseline01: veth redirect</a></h3>
<div class="outline-text-3" id="text-baseline01--veth-redirect">
<p>
Kernel: Linux broadwell 5.11.0-net-next
</p>

<pre class="example" id="orge42553b">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       3,677,958   0          
XDP-RX CPU      total   3,677,958  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   3,677,966   0          
rx_queue_index    0:sum 3,677,966  

</pre>

<pre class="example" id="org51cff54">
Samples: 81K of event 'cycles', Event count (approx.): 73929158590
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,87%  [k] __netif_receive_skb_core                         -      -
+    3,50%  [k] kmem_cache_free                                  -      -
+    3,26%  [k] dev_gro_receive                                  -      -
+    3,24%  [k] rmqueue                                          -      -
+    3,15%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,09%  [k] __xdp_release_frame                              -      -
+    2,99%  [k] memset_erms                                      -      -
+    2,89%  [k] get_page_from_freelist                           -      -
+    2,82%  [k] __alloc_pages_nodemask                           -      -
+    2,61%  [k] veth_xdp_rcv_one                                 -      -
+    2,54%  [k] free_unref_page_commit                           -      -
+    2,35%  [k] __list_del_entry_valid                           -      -
+    2,29%  [k] __netif_receive_skb_list_core                    -      -
+    2,13%  [k] dev_map_enqueue                                  -      -
+    2,07%  [k] mlx5e_xdp_handle                                 -      -
+    2,04%  [k] __xdp_build_skb_from_frame                       -      -
+    1,95%  [k] napi_gro_receive                                 -      -
+    1,94%  [k] xdp_do_redirect                                  -      -
+    1,85%  [k] netif_receive_skb_list_internal                  -      -
+    1,85%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    1,84%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    1,82%  [k] bpf_xdp_redirect_map                             -      -
+    1,74%  [k] kmem_cache_alloc_bulk                            -      -
+    1,71%  [k] ip_list_rcv                                      -      -
+    1,69%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,56%  [k] veth_xdp_rcv                                     -      -
+    1,48%  [k] __slab_free                                      -      -
+    1,45%  [k] free_unref_page_prepare.part.0                   -      -
+    1,43%  [k] eth_type_trans                                   -      -
+    1,40%  [k] dma_map_page_attrs                               -      -
+    1,30%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,21%  [k] __list_add_valid                                 -      -
+    1,19%  [k] veth_xdp_xmit                                    -      -
+    1,11%  [k] free_unref_page                                  -      -
+    1,07%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,05%  [k] dma_unmap_page_attrs                             -      -
+    1,03%  [k] page_pool_release_page                           -      -
+    1,00%  [k] gro_normal_one                                   -      -
+    0,99%  [k] build_skb_around                                 -      -
+    0,94%  [k] __page_pool_alloc_pages_slow                     -      -
+    0,92%  [k] ip_rcv_core.isra.0                               -      -
+    0,90%  [k] prep_new_page                                    -      -
+    0,89%  [k] __build_skb_around                               -      -
+    0,81%  [k] free_pcp_prepare                                 -      -
</pre>
</div>
</div>
</div>


<div id="outline-container-Using-alloc_pages_bulk" class="outline-2">
<h2 id="Using-alloc_pages_bulk"><a href="#Using-alloc_pages_bulk">Using alloc_pages_bulk</a></h2>
<div class="outline-text-2" id="text-Using-alloc_pages_bulk">
<p>
Kernel: Linux broadwell 5.11.0-net-next-alloc_pages_bulk+
</p>
<ul class="org-ul">
<li>With this fix <a href="https://patchwork.kernel.org/project/netdevbpf/patch/161402344429.1980160.4798557236979159924.stgit@firesoul/">to mlx5 driver</a></li>
</ul>
</div>

<div id="outline-container-test01--veth-redirect--page_pool-bulk-16-" class="outline-3">
<h3 id="test01--veth-redirect--page_pool-bulk-16-"><a href="#test01--veth-redirect--page_pool-bulk-16-">test01: veth redirect (page_pool bulk 16)</a></h3>
<div class="outline-text-3" id="text-test01--veth-redirect--page_pool-bulk-16-">
<p>
Same veth setup as above: 
</p>

<p>
Results below with page_pool using bulk=16 for alloc_pages_bulk().
</p>

<pre class="example" id="org99cc2da">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       4,066,028   0          
XDP-RX CPU      total   4,066,028  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:5   4,066,023   0          
rx_queue_index    0:sum 4,066,023  
</pre>

<pre class="example" id="orgc595936">
Samples: 51K of event 'cycles', Event count (approx.): 46934149161
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,94%  [k] __netif_receive_skb_core                         -      -
+    4,02%  [k] kmem_cache_free                                  -      -
+    3,78%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,30%  [k] memset_erms                                      -      -
+    3,26%  [k] __xdp_release_frame                              -      -
+    3,23%  [k] dev_gro_receive                                  -      -
+    3,10%  [k] free_unref_page_commit                           -      -
+    3,06%  [k] veth_xdp_rcv_one                                 -      -
+    2,82%  [k] __list_del_entry_valid                           -      -
+    2,42%  [k] bpf_xdp_redirect_map                             -      -
+    2,40%  [k] __netif_receive_skb_list_core                    -      -
+    2,40%  [k] napi_gro_receive                                 -      -
+    2,21%  [k] __xdp_build_skb_from_frame                       -      -
+    2,16%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    2,07%  [k] dev_map_enqueue                                  -      -
+    2,06%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    2,01%  [k] ip_list_rcv                                      -      -
+    1,94%  [k] netif_receive_skb_list_internal                  -      -
+    1,92%  [k] kmem_cache_alloc_bulk                            -      -
+    1,91%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,90%  [k] veth_xdp_rcv                                     -      -
+    1,74%  [k] __list_add_valid                                 -      -
+    1,73%  [k] xdp_do_redirect                                  -      -
+    1,70%  [k] mlx5e_xdp_handle                                 -      -
+    1,63%  [k] free_unref_page_prepare.part.0                   -      -
+    1,61%  [k] dma_map_page_attrs                               -      -
+    1,53%  [k] __alloc_pages_bulk_nodemask                      -      -
+    1,43%  [k] __slab_free                                      -      -
+    1,42%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,36%  [k] eth_type_trans                                   -      -
+    1,32%  [k] __page_pool_alloc_pages_slow                     -      -
+    1,22%  [k] free_unref_page                                  -      -
+    1,18%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,16%  [k] veth_xdp_xmit                                    -      -
+    1,09%  [k] build_skb_around                                 -      -
+    1,03%  [k] gro_normal_one                                   -      -
+    1,02%  [k] page_pool_release_page                           -      -
+    1,00%  [k] ip_rcv_core.isra.0                               -      -
+    1,00%  [k] dma_unmap_page_attrs                             -      -
+    0,99%  [k] __rmqueue_pcplist                                -      -
+    0,95%  [k] free_pcp_prepare                                 -      -
+    0,91%  [k] __build_skb_around                               -      -
     0,84%  [k] kfree_skb                                        -      -
</pre>
</div>
</div>

<div id="outline-container-test02--veth-redirect--page_pool-bulk-64-" class="outline-3">
<h3 id="test02--veth-redirect--page_pool-bulk-64-"><a href="#test02--veth-redirect--page_pool-bulk-64-">test02: veth redirect (page_pool bulk 64)</a></h3>
<div class="outline-text-3" id="text-test02--veth-redirect--page_pool-bulk-64-">
<p>
Same veth setup as above: 
</p>

<p>
Results below with page_pool using bulk=64 for alloc_pages_bulk().
</p>

<pre class="example" id="org8437679">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       4,297,656   0          
XDP-RX CPU      total   4,297,656  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:1   4,297,649   0          
rx_queue_index    0:sum 4,297,649  
</pre>
</div>
</div>
</div>

<div id="outline-container-Test-with-RFC-patchset" class="outline-2">
<h2 id="Test-with-RFC-patchset"><a href="#Test-with-RFC-patchset">Test with RFC patchset</a></h2>
<div class="outline-text-2" id="text-Test-with-RFC-patchset">
<p>
Test with new patchset from Mel:
</p>
<ul class="org-ul">
<li>Message-Id: &lt;20210224102603.19524-1-mgorman@techsingularity.net&gt;</li>
<li><a href="https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/">https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/</a></li>
</ul>
</div>

<div id="outline-container-initial-test" class="outline-3">
<h3 id="initial-test"><a href="#initial-test">initial test</a></h3>
<div class="outline-text-3" id="text-initial-test">
<p>
bulk=64
</p>

<pre class="example" id="orgd402979">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,368,926   0          
XDP-RX CPU      total   4,368,926  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,368,917   0          
rx_queue_index    0:sum 4,368,917  
</pre>
</div>
</div>
</div>

<div id="outline-container-patch-notes" class="outline-2">
<h2 id="patch-notes"><a href="#patch-notes">patch notes</a></h2>
<div class="outline-text-2" id="text-patch-notes">
</div>
<div id="outline-container-Follow-up-to-Mel-s-patchset" class="outline-3">
<h3 id="Follow-up-to-Mel-s-patchset"><a href="#Follow-up-to-Mel-s-patchset">Follow up to Mel's patchset</a></h3>
<div class="outline-text-3" id="text-Follow-up-to-Mel-s-patchset">
<p>
Patchset V1:
</p>
<pre class="example" id="orga6c1b0f">
stg mail --version='RFC net-next' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210224102603.19524-1-mgorman@techsingularity.net' \
 03-reorder-add-page_pool_dma_map..mm-make-zone-free_area-order
</pre>
<p>
Message-ID: &lt;161419296941.2718959.12575257358107256094.stgit@firesoul&gt;
</p>

<p>
V2 with minor changes and dropping micro-optimisation:
</p>
<pre class="example" id="org3e1e1f8">
stg mail --version='RFC V2 net-next' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210224102603.19524-1-mgorman@techsingularity.net' \
 05-03-reorder-add-page_pool_dma_map..06-04-page_pool-use-alloc_pages_bulk
</pre>
<p>
Message-ID: &lt;161460522573.3031322.15721946341157092594.stgit@firesoul&gt;
</p>

<blockquote>
<p>
Use bulk order-0 page allocator API for page_pool
</p>

<p>
This is a followup to Mel Gorman's patchset:
</p>
<ul class="org-ul">
<li>Message-Id: &lt;20210224102603.19524-1-mgorman@techsingularity.net&gt;</li>
<li><a href="https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/">https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/</a></li>
</ul>

<p>
Showing page_pool usage of the API for alloc_pages_bulk().
</p>

<p>
Maybe Mel Gorman will/can carry these patches?
(to keep it together with the alloc_pages_bulk API)
</p>
</blockquote>
</div>
</div>

<div id="outline-container-bench-test-veth" class="outline-3">
<h3 id="bench-test-veth"><a href="#bench-test-veth">bench test veth</a></h3>
<div class="outline-text-3" id="text-bench-test-veth">
<p>
Test again:
</p>
<pre class="example" id="orgd4c19c7">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       4,302,291   0          
XDP-RX CPU      total   4,302,291  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   4,302,285   0          
rx_queue_index    0:sum 4,302,285  
</pre>
</div>
</div>

<div id="outline-container-desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path" class="outline-3">
<h3 id="desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path"><a href="#desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path">desc: net: page_pool: use alloc_pages_bulk in refill code path</a></h3>
<div class="outline-text-3" id="text-desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path">
<blockquote>
<p>
There are cases where the page_pool need to refill with pages from the
page allocator. Some workloads cause the page_pool to release pages
instead of recycling these pages.
</p>

<p>
For these workload it can improve performance to bulk alloc pages from
the page-allocator to refill the alloc cache.
</p>

<p>
For XDP-redirect workload with 100G mlx5 driver (that use page_pool)
redirecting xdp_frame packets into a veth, that does XDP_PASS to create
an SKB from the xdp_frame, which then cannot return the page to the
page_pool. In this case, we saw[1] an improvement of 18.8% from using
the alloc_pages_bulk API (3,677,958 pps -&gt; 4,368,926 pps).
</p>

<p>
[1] <a href="https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org">https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org</a>
</p>
</blockquote>
</div>
</div>
</div>


<div id="outline-container-Test-on-Mel-git-tree--mm-bulk-rebase-v4r2" class="outline-2">
<h2 id="Test-on-Mel-git-tree--mm-bulk-rebase-v4r2"><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v4r2">Test on Mel git-tree: mm-bulk-rebase-v4r2</a></h2>
<div class="outline-text-2" id="text-Test-on-Mel-git-tree--mm-bulk-rebase-v4r2">
<p>
Tests based on Mel Gorman's git tree:
</p>
<ul class="org-ul">
<li>git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git</li>
</ul>

<p>
Branch: mm-bulk-rebase-v4r2
</p>
<ul class="org-ul">
<li>Changed the last patch with page_pool changes</li>
</ul>
</div>

<div id="outline-container-stg-mail" class="outline-3">
<h3 id="stg-mail"><a href="#stg-mail">stg mail</a></h3>
<div class="outline-text-3" id="text-stg-mail">
<p>
Promised to followup in Message-ID: &lt;20210315094038.22d6d79a@carbon&gt;
</p>
<ul class="org-ul">
<li>Below stg <a href="https://lore.kernel.org/netdev/161583677541.3715498.6118778324185171839.stgit@firesoul/">Message-ID</a></li>
</ul>

<pre class="example" id="orgde6fddb">
stg mail --version='mel-git' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com --cc alex \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210315094038.22d6d79a@carbon' \
 net-page_pool-use
</pre>

<blockquote>
<p>
Subj: Followup: Update [PATCH 7/7] in Mel's series
</p>

<p>
This patch is against Mel's git-tree:
 git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git
</p>

<p>
Using branch: mm-bulk-rebase-v4r2 but replacing the last patch related to
the page_pool using __alloc_pages_bulk().
</p>

<p>
<a href="https://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git/log/?h=mm-bulk-rebase-v4r2">https://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git/log/?h=mm-bulk-rebase-v4r2</a>
</p>

<p>
While implementing suggestions by Alexander Duyck, I realised that I could
simplify the code further, and simply take the last page from the
pool-&gt;alloc.cache given this avoids special casing the last page.
</p>

<p>
I re-ran performance tests and the improvement have been reduced to 13% from
18% before, but I don't think the rewrite of the specific patch have
anything to do with this.
</p>

<p>
Notes on tests:
 <a href="https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#test-on-mel-git-tree">https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#test-on-mel-git-tree</a>
</p>
</blockquote>

<p>
Performance summary: +13% faster
</p>
<ul class="org-ul">
<li>(3,810,013 pps -&gt; 4,308,208 pps)</li>
<li>((4308208/3810013)-1)*100 = 13.07%</li>
</ul>

<p>
Previous: 18.8% (3,677,958 pps -&gt; 4,368,926 pps).
</p>
<ul class="org-ul">
<li>Thus, slower than before.</li>
<li>Mostly look like better baseline</li>
</ul>
</div>
</div>

<div id="outline-container-Updated-patch" class="outline-3">
<h3 id="Updated-patch"><a href="#Updated-patch">Updated patch</a></h3>
<div class="outline-text-3" id="text-Updated-patch">
<p>
Alexander Duyck point out there was a cleaner way to implement
changes in function <code>__page_pool_alloc_pages_slow()</code>.
</p>

<pre class="example" id="orga7c6178">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,308,208   0          
XDP-RX CPU      total   4,308,208  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,308,208   0          
rx_queue_index    0:sum 4,308,208  
</pre>
</div>
</div>

<div id="outline-container-Pop-patch-using-__alloc_pages_bulk" class="outline-3">
<h3 id="Pop-patch-using-__alloc_pages_bulk"><a href="#Pop-patch-using-__alloc_pages_bulk">Pop patch using __alloc_pages_bulk</a></h3>
<div class="outline-text-3" id="text-Pop-patch-using-__alloc_pages_bulk">
<pre class="example" id="orgf021926">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       3,810,013   0          
XDP-RX CPU      total   3,810,013  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:1   3,810,013   0          
rx_queue_index    0:sum 3,810,013  
</pre>
</div>
</div>
</div>

<div id="outline-container-Test-on-Mel-git-tree--mm-bulk-rebase-v5r9" class="outline-2">
<h2 id="Test-on-Mel-git-tree--mm-bulk-rebase-v5r9"><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v5r9">Test on Mel git-tree: mm-bulk-rebase-v5r9</a></h2>
<div class="outline-text-2" id="text-Test-on-Mel-git-tree--mm-bulk-rebase-v5r9">
<p>
Tests based on Mel Gorman's git tree:
</p>
<ul class="org-ul">
<li>git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git</li>
<li>Branch: mm-bulk-rebase-v5r9</li>
</ul>
</div>

<div id="outline-container-micro-benchmark--page_bench04_bulk" class="outline-3">
<h3 id="micro-benchmark--page_bench04_bulk"><a href="#micro-benchmark--page_bench04_bulk">micro-benchmark: page_bench04_bulk</a></h3>
<div class="outline-text-3" id="text-micro-benchmark--page_bench04_bulk">
<p>
Notice these "per elem" measurements are alloc+free cost for order-0 pages
</p>

<p>
page_bench04_bulk micro-benchmark on branch: mm-bulk-rebase-v5r9
</p>
<ul class="org-ul">
<li><a href="https://lore.kernel.org/netdev/20210322130446.0a505db0@carbon/">https://lore.kernel.org/netdev/20210322130446.0a505db0@carbon/</a></li>
</ul>

<p>
CPU: Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz
</p>

<pre class="example" id="org42cd05f">
BASELINE
 single_page alloc+put: Per elem: 199 cycles(tsc) 55.472 ns

LIST variant: time_bulk_page_alloc_free_list: step=bulk size

 Per elem: 206 cycles(tsc) 57.478 ns (step:1)
 Per elem: 154 cycles(tsc) 42.861 ns (step:2)
 Per elem: 145 cycles(tsc) 40.536 ns (step:3)
 Per elem: 142 cycles(tsc) 39.477 ns (step:4)
 Per elem: 142 cycles(tsc) 39.610 ns (step:8)
 Per elem: 137 cycles(tsc) 38.155 ns (step:16)
 Per elem: 135 cycles(tsc) 37.739 ns (step:32)
 Per elem: 134 cycles(tsc) 37.282 ns (step:64)
 Per elem: 133 cycles(tsc) 36.993 ns (step:128)

ARRAY variant: time_bulk_page_alloc_free_array: step=bulk size

 Per elem: 202 cycles(tsc) 56.383 ns (step:1)
 Per elem: 144 cycles(tsc) 40.047 ns (step:2)
 Per elem: 134 cycles(tsc) 37.339 ns (step:3)
 Per elem: 128 cycles(tsc) 35.578 ns (step:4)
 Per elem: 120 cycles(tsc) 33.592 ns (step:8)
 Per elem: 116 cycles(tsc) 32.362 ns (step:16)
 Per elem: 113 cycles(tsc) 31.476 ns (step:32)
 Per elem: 110 cycles(tsc) 30.633 ns (step:64)
 Per elem: 110 cycles(tsc) 30.596 ns (step:128)
</pre>
</div>
</div>
</div>

<div id="outline-container-Micro-optimisations" class="outline-2">
<h2 id="Micro-optimisations"><a href="#Micro-optimisations">Micro optimisations</a></h2>
<div class="outline-text-2" id="text-Micro-optimisations">
<p>
<b>UPDATE</b>: Choosing to drop this patch, it is waste too much memory and
it too fragile as it depends on compiler behaviour.
</p>

<p>
Document steps in micro optimizing page-alloactor code:
</p>
<ul class="org-ul">
<li>make zone-&gt;free_area[order] access faster</li>
</ul>
</div>

<div id="outline-container-Observations" class="outline-3">
<h3 id="Observations"><a href="#Observations">Observations</a></h3>
<div class="outline-text-3" id="text-Observations">
<p>
The code del_page_from_free_list() generate a strange imul operation:
</p>
<pre class="example" id="orge50ab45">
imul   $0x58,%rax,%rax
</pre>

<div class="org-src-container">
<pre class="src src-C"><span style="font-weight: bold;">static</span> <span style="font-weight: bold;">inline</span> <span style="font-weight: bold; text-decoration: underline;">void</span> <span style="font-weight: bold;">del_page_from_free_list</span>(<span style="font-weight: bold;">struct</span> <span style="font-weight: bold; text-decoration: underline;">page</span> *<span style="font-weight: bold; font-style: italic;">page</span>, <span style="font-weight: bold;">struct</span> <span style="font-weight: bold; text-decoration: underline;">zone</span> *<span style="font-weight: bold; font-style: italic;">zone</span>,
                                           <span style="font-weight: bold; text-decoration: underline;">unsigned</span> <span style="font-weight: bold; text-decoration: underline;">int</span> <span style="font-weight: bold; font-style: italic;">order</span>)
{
        <span style="font-weight: bold; font-style: italic;">/* </span><span style="font-weight: bold; font-style: italic;">clear reported state and update reported page count </span><span style="font-weight: bold; font-style: italic;">*/</span>
        <span style="font-weight: bold;">if</span> (page_reported(page))
                __ClearPageReported(page);

        list_del(&amp;page-&gt;lru);
        __ClearPageBuddy(page);
        set_page_private(page, 0);
        zone-&gt;free_area[order].nr_free--;
</pre>
</div>

<p>
Tracked this down to:
</p>
<div class="org-src-container">
<pre class="src src-C"><span style="font-weight: bold;">struct</span> <span style="font-weight: bold; text-decoration: underline;">zone</span> {
    [...]
        <span style="font-weight: bold;">struct</span> <span style="font-weight: bold; text-decoration: underline;">free_area</span>        free_area[MAX_ORDER];
</pre>
</div>

<p>
This happens when accessing free_area like this:
</p>
<div class="org-src-container">
<pre class="src src-C">zone-&gt;free_area[order].nr_free--;
</pre>
</div>

<p>
Perf show hot-spot in: rmqueue_bulk.constprop.0 / rmqueue_bulk()
</p>
<pre class="example" id="orgeecf749">
      │         mov    0x8(%rbx),%rax                                                                                            ▒
      │       __list_del():                                                                                                      ▒
      │         mov    %rax,0x8(%rdx)                                                                                            ▒
      │         mov    %rdx,(%rax)                                                                                               ▒
      │       del_page_from_free_list():                                                                                         ▒
44,54 │1  e2:   imul   $0x58,%rbp,%rbp                                                                                           ▒
      │       expand():                                                                                                          ◆
      │         mov    $0x1,%r9d                                                                                                 ▒
      │         mov    %r13d,%ecx                                                                                                ▒
      │       set_page_private():                                                                                                ▒
      │         movq   $0x0,0x20(%rbx)                                                                                           ▒
      │       __ClearPageBuddy():                                                                                                ▒
      │         orl    $0x80,0x28(%rbx)                                                                                          ▒
      │         lea    -0x1(%r13),%r11d                                                                                          ▒
      │       expand():                                                                                                          ▒
      │         shl    %cl,%r9d                                                                                                  ▒
      │       list_del():                                                                                                        ▒
      │         movabs $0xdead000000000100,%rax                                                                                  ▒
      │         mov    %rax,(%rbx)                                                                                               ▒
      │         add    $0x22,%rax                                                                                                ▒
      │       expand():                                                                                                          ▒
      │         movslq %r9d,%r14                                                                                                 ▒
      │       list_del():                                                                                                        ▒
      │         mov    %rax,0x8(%rbx)                                                                                            ▒
      │       del_page_from_free_list():                                                                                         ▒
      │         subq   $0x1,0x110(%r15,%rbp,1)                                                                                   ▒
      │       expand():                                                                                                          ▒
</pre>
</div>
</div>

<div id="outline-container-Why-happening" class="outline-3">
<h3 id="Why-happening"><a href="#Why-happening">Why happening</a></h3>
<div class="outline-text-3" id="text-Why-happening">
<p>
The size of struct free_area is 88 bytes or 0x58 hex.
</p>

<div class="org-src-container">
<pre class="src src-sh">$ pahole -C free_area mm/page_alloc.o
struct free_area {
        struct list_head           free_list[5];         /*     0    80 */
        /* --- cacheline 1 boundary (64 bytes) was 16 bytes ago --- */
        long unsigned int          nr_free;              /*    80     8 */

        /* size: 88, cachelines: 2, members: 2 */
        /* last cacheline: 24 bytes */
};
</pre>
</div>

<p>
The reason for the code is to find the right struct free_area in struct
zone.  The array of 11 comes from define MAX_ORDER.
</p>

<pre class="example" id="orga132a93">
struct zone {
        long unsigned int          _watermark[3];        /*     0    24 */
 [...]
        /* --- cacheline 3 boundary (192 bytes) --- */
        struct zone_padding        _pad1_ __attribute__((__aligned__(64))); /*   192     0 */
        struct free_area           free_area[11];        /*   192   968 */
        /* --- cacheline 18 boundary (1152 bytes) was 8 bytes ago --- */
        long unsigned int          flags;                /*  1160     8 */
        spinlock_t                 lock;                 /*  1168     4 */

        /* XXX 44 bytes hole, try to pack */

        /* --- cacheline 19 boundary (1216 bytes) --- */
        struct zone_padding        _pad2_ __attribute__((__aligned__(64))); /*  1216     0 */

</pre>

<p>
The size 88 bytes or 0x58 hex. The compiler cannot find a shift
operation to use and instead choose to use a imul to find the offset
into the array free_area[].
</p>

<p>
Asm code to lookout for:
(objdump -Sr mm/page_alloc.o-use-imul)
</p>
<div class="org-src-container">
<pre class="src src-asm">   <span style="font-weight: bold;">zone-&gt;free_area</span>[order].nr_free--<span style="font-weight: bold; font-style: italic;">;</span>
<span style="font-weight: bold;">75ee</span>:       <span style="font-weight: bold;">44</span> 89 f0                mov    <span style="font-weight: bold; font-style: italic;">%r14d</span>,<span style="font-weight: bold; font-style: italic;">%eax</span>
<span style="font-weight: bold;">75f1</span>:       <span style="font-weight: bold;">48</span> 6b c0 58             imul   $0x58,<span style="font-weight: bold; font-style: italic;">%rax</span>,<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">75f5</span>:       <span style="font-weight: bold;">48</span> 03 04 24             add    (<span style="font-weight: bold; font-style: italic;">%rsp</span>),<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">75f9</span>:       <span style="font-weight: bold;">49</span> 83 ac 04 10 01 00    subq   $0x1,0x110(<span style="font-weight: bold; font-style: italic;">%r12</span>,<span style="font-weight: bold; font-style: italic;">%rax</span>,1)
</pre>
</div>

<p>
It looks like it happens 45 times in <code>mm/page_alloc.o</code>:
</p>
<div class="org-src-container">
<pre class="src src-C">$ objdump -Sr mm/page_alloc.o | grep imul | grep <span style="font-style: italic;">'0x58,'</span> |wc -l
45
</pre>
</div>

<p>
Code notes for hot-path: The del_page_from_free_list() contains the
zone-&gt;free_area[order].nr_free&#x2013; code, the __rmqueue_smallest was the
hotspot that calls this. This is called by __rmqueue, which is called by
rmqueue_bulk.
</p>
</div>
</div>

<div id="outline-container-Explaining-patch-with-fix" class="outline-3">
<h3 id="Explaining-patch-with-fix"><a href="#Explaining-patch-with-fix">Explaining patch with fix</a></h3>
<div class="outline-text-3" id="text-Explaining-patch-with-fix">
<blockquote>
<p>
mm: make zone-&gt;free_area[order] access faster
</p>

<p>
Avoid multiplication (imul) operations when accessing:
 zone-&gt;free_area[order].nr_free
</p>

<p>
This was really tricky to find. I was puzzled why perf reported that
rmqueue_bulk was using 44% of the time in an imul operation:
</p>

<p>
      │     del_page_from_free_list():
44,54 │ e2:   imul   $0x58,%rax,%rax
</p>

<p>
This operation was generated (by compiler) because the struct free_area
have size 88 bytes or 0x58 hex. The compiler cannot find a shift
operation to use and instead choose to use a imul to find the offset
into the array free_area[].
</p>

<p>
The patch align struct free_area to a cache-line, which cause the
compiler avoid the imul operation. The imul operation is very fast on
modern Intel CPUs. To help fast-path that decrement 'nr_free' move the
member 'nr_free' to be first element, which saves one 'add' operation.
</p>

<p>
Looking up instruction latency this exchange a 3-cycle 'imul' with a
1-cycle 'shl', saving 2-cycles. It does trade some space to do this.
</p>

<p>
Used: gcc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)
</p>
</blockquote>

<p>
Notes about moving members around:
</p>

<p>
Before: Move member 'nr_free':
</p>
<div class="org-src-container">
<pre class="src src-asm"><span style="font-weight: bold;">760e</span>:       <span style="font-weight: bold;">44</span> 89 f0                mov    <span style="font-weight: bold; font-style: italic;">%r14d</span>,<span style="font-weight: bold; font-style: italic;">%eax</span>
<span style="font-weight: bold;">7611</span>:       <span style="font-weight: bold;">48</span> 83 c0 02             add    $0x2,<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">7615</span>:       <span style="font-weight: bold;">48</span> c1 e0 07             shl    $0x7,<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">7619</span>:       <span style="font-weight: bold;">48</span> 03 04 24             add    (<span style="font-weight: bold; font-style: italic;">%rsp</span>),<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">761d</span>:       <span style="font-weight: bold;">49</span> 83 6c 04 10 01       subq   $0x1,0x10(<span style="font-weight: bold; font-style: italic;">%r12</span>,<span style="font-weight: bold; font-style: italic;">%rax</span>,1)
</pre>
</div>

<p>
Move member 'nr_free':
</p>
<div class="org-src-container">
<pre class="src src-asm"><span style="font-weight: bold;">75be</span>:       <span style="font-weight: bold;">44</span> 89 f0                mov    <span style="font-weight: bold; font-style: italic;">%r14d</span>,<span style="font-weight: bold; font-style: italic;">%eax</span>
<span style="font-weight: bold;">75c1</span>:       <span style="font-weight: bold;">48</span> c1 e0 07             shl    $0x7,<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">75c5</span>:       <span style="font-weight: bold;">48</span> 03 04 24             add    (<span style="font-weight: bold; font-style: italic;">%rsp</span>),<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">75c9</span>:       <span style="font-weight: bold;">49</span> 83 ac 04 c0 00 00    subq   $0x1,0xc0(<span style="font-weight: bold; font-style: italic;">%r12</span>,<span style="font-weight: bold; font-style: italic;">%rax</span>,1)
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2021-03-22 Mon 14:18</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
