<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Testing page-allocator API proposal for alloc_pages_bulk</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/hideshow.css"/>
<script type="text/javascript" src="/styles/bigblow/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/bigblow.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/hideshow.js"></script>
<script type="text/javascript" src="/styles/lib/js/jquery.stickytableheaders.min.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">Testing page-allocator API proposal for alloc_pages_bulk</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Use-case--page_pool">Use-case: page_pool</a>
<ul>
<li><a href="#Test-setup-for-veth">Test setup for veth</a></li>
</ul>
</li>
<li><a href="#Baseline-test">Baseline test</a>
<ul>
<li><a href="#baseline01--veth-redirect">baseline01: veth redirect</a></li>
</ul>
</li>
<li><a href="#Using-alloc_pages_bulk">Using alloc_pages_bulk</a>
<ul>
<li><a href="#test01--veth-redirect--page_pool-bulk-16-">test01: veth redirect (page_pool bulk 16)</a></li>
<li><a href="#test02--veth-redirect--page_pool-bulk-64-">test02: veth redirect (page_pool bulk 64)</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
Testing experimental patch from Mel Gorman that add alloc_pages_bulk() API.
</p>

<p>
This patch: <a href="https://lore.kernel.org/linux-mm/20210210130705.GC3629@suse.de/">https://lore.kernel.org/linux-mm/20210210130705.GC3629@suse.de/</a>
With this fix: <a href="https://lore.kernel.org/linux-mm/20210211091235.GC3697@techsingularity.net/">https://lore.kernel.org/linux-mm/20210211091235.GC3697@techsingularity.net/</a>
</p>

<p>
Notice new patchset from Mel:
</p>
<ul class="org-ul">
<li>Message-Id: &lt;20210224102603.19524-1-mgorman@techsingularity.net&gt;</li>
<li><a href="https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/">https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/</a></li>
</ul>

<div id="outline-container-Use-case--page_pool" class="outline-2">
<h2 id="Use-case--page_pool"><a href="#Use-case--page_pool">Use-case: page_pool</a></h2>
<div class="outline-text-2" id="text-Use-case--page_pool">
<p>
The <code>alloc_pages_bulk()</code> use-case for page_pool is in
<code>__page_pool_alloc_pages_slow()</code>, for then the pool goes empty.
</p>

<p>
The mlx5 driver can easily empty the page_pool "cache", when redirected
xdp_frame's are converted into SKBs.
</p>

<p>
This happens both for veth and cpumap, as SKBs (created from xdp_frame) is
missing callback to page_pool.
</p>
</div>

<div id="outline-container-Test-setup-for-veth" class="outline-3">
<h3 id="Test-setup-for-veth"><a href="#Test-setup-for-veth">Test setup for veth</a></h3>
<div class="outline-text-3" id="text-Test-setup-for-veth">
<p>
It requires some setup and two XDP programs to trigger the case, where
page_pool "cache" goes empty. And use of driver mlx5.
</p>

<p>
First: Create veth pair and enabled link up:
</p>

<pre class="example">
ip link add veth1 type veth peer name veth2
ip link set veth1 up
ip link set veth2 up
</pre>

<p>
Disable GRO/GSO/TSO on the veth devices
</p>
<pre class="example" id="org9a50be8">
ethtool -K veth1 gso off gro off tso off
ethtool -K veth2 gso off gro off tso off
</pre>

<p>
When XDP-redirecting into a veth, we must remember to attach an XDP prog to
the peer device.
</p>

<p>
Redirect frame from mlx5p1 into veth1 (peer veth2)
</p>
<ul class="org-ul">
<li>sudo ./xdp_redirect_map mlx5p1 veth1</li>
</ul>

<p>
Create SKBs from xdp_frame via XDP_PASS on veth2:
</p>
<ul class="org-ul">
<li>sudo ./xdp_rxq_info &#x2013;dev veth2 &#x2013;act XDP_PASS</li>
</ul>

<p>
As the MAC-addr doesn't match the SKB packets are dropped very early, but it
suites our micro-benchmark test case.
</p>
</div>
</div>
</div>

<div id="outline-container-Baseline-test" class="outline-2">
<h2 id="Baseline-test"><a href="#Baseline-test">Baseline test</a></h2>
<div class="outline-text-2" id="text-Baseline-test">
</div>
<div id="outline-container-baseline01--veth-redirect" class="outline-3">
<h3 id="baseline01--veth-redirect"><a href="#baseline01--veth-redirect">baseline01: veth redirect</a></h3>
<div class="outline-text-3" id="text-baseline01--veth-redirect">
<p>
Kernel: Linux broadwell 5.11.0-net-next
</p>

<pre class="example" id="org0aef302">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       3,677,958   0          
XDP-RX CPU      total   3,677,958  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   3,677,966   0          
rx_queue_index    0:sum 3,677,966  

</pre>

<pre class="example" id="org9693c96">
Samples: 81K of event 'cycles', Event count (approx.): 73929158590
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,87%  [k] __netif_receive_skb_core                         -      -
+    3,50%  [k] kmem_cache_free                                  -      -
+    3,26%  [k] dev_gro_receive                                  -      -
+    3,24%  [k] rmqueue                                          -      -
+    3,15%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,09%  [k] __xdp_release_frame                              -      -
+    2,99%  [k] memset_erms                                      -      -
+    2,89%  [k] get_page_from_freelist                           -      -
+    2,82%  [k] __alloc_pages_nodemask                           -      -
+    2,61%  [k] veth_xdp_rcv_one                                 -      -
+    2,54%  [k] free_unref_page_commit                           -      -
+    2,35%  [k] __list_del_entry_valid                           -      -
+    2,29%  [k] __netif_receive_skb_list_core                    -      -
+    2,13%  [k] dev_map_enqueue                                  -      -
+    2,07%  [k] mlx5e_xdp_handle                                 -      -
+    2,04%  [k] __xdp_build_skb_from_frame                       -      -
+    1,95%  [k] napi_gro_receive                                 -      -
+    1,94%  [k] xdp_do_redirect                                  -      -
+    1,85%  [k] netif_receive_skb_list_internal                  -      -
+    1,85%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    1,84%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    1,82%  [k] bpf_xdp_redirect_map                             -      -
+    1,74%  [k] kmem_cache_alloc_bulk                            -      -
+    1,71%  [k] ip_list_rcv                                      -      -
+    1,69%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,56%  [k] veth_xdp_rcv                                     -      -
+    1,48%  [k] __slab_free                                      -      -
+    1,45%  [k] free_unref_page_prepare.part.0                   -      -
+    1,43%  [k] eth_type_trans                                   -      -
+    1,40%  [k] dma_map_page_attrs                               -      -
+    1,30%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,21%  [k] __list_add_valid                                 -      -
+    1,19%  [k] veth_xdp_xmit                                    -      -
+    1,11%  [k] free_unref_page                                  -      -
+    1,07%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,05%  [k] dma_unmap_page_attrs                             -      -
+    1,03%  [k] page_pool_release_page                           -      -
+    1,00%  [k] gro_normal_one                                   -      -
+    0,99%  [k] build_skb_around                                 -      -
+    0,94%  [k] __page_pool_alloc_pages_slow                     -      -
+    0,92%  [k] ip_rcv_core.isra.0                               -      -
+    0,90%  [k] prep_new_page                                    -      -
+    0,89%  [k] __build_skb_around                               -      -
+    0,81%  [k] free_pcp_prepare                                 -      -
</pre>
</div>
</div>
</div>


<div id="outline-container-Using-alloc_pages_bulk" class="outline-2">
<h2 id="Using-alloc_pages_bulk"><a href="#Using-alloc_pages_bulk">Using alloc_pages_bulk</a></h2>
<div class="outline-text-2" id="text-Using-alloc_pages_bulk">
<p>
Kernel: Linux broadwell 5.11.0-net-next-alloc_pages_bulk+
</p>
<ul class="org-ul">
<li>With this fix <a href="https://patchwork.kernel.org/project/netdevbpf/patch/161402344429.1980160.4798557236979159924.stgit@firesoul/">to mlx5 driver</a></li>
</ul>
</div>

<div id="outline-container-test01--veth-redirect--page_pool-bulk-16-" class="outline-3">
<h3 id="test01--veth-redirect--page_pool-bulk-16-"><a href="#test01--veth-redirect--page_pool-bulk-16-">test01: veth redirect (page_pool bulk 16)</a></h3>
<div class="outline-text-3" id="text-test01--veth-redirect--page_pool-bulk-16-">
<p>
Same veth setup as above: 
</p>

<p>
Results below with page_pool using bulk=16 for alloc_pages_bulk().
</p>

<pre class="example" id="orgbfb3001">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       4,066,028   0          
XDP-RX CPU      total   4,066,028  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:5   4,066,023   0          
rx_queue_index    0:sum 4,066,023  
</pre>

<pre class="example" id="org13fdd63">
Samples: 51K of event 'cycles', Event count (approx.): 46934149161
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,94%  [k] __netif_receive_skb_core                         -      -
+    4,02%  [k] kmem_cache_free                                  -      -
+    3,78%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,30%  [k] memset_erms                                      -      -
+    3,26%  [k] __xdp_release_frame                              -      -
+    3,23%  [k] dev_gro_receive                                  -      -
+    3,10%  [k] free_unref_page_commit                           -      -
+    3,06%  [k] veth_xdp_rcv_one                                 -      -
+    2,82%  [k] __list_del_entry_valid                           -      -
+    2,42%  [k] bpf_xdp_redirect_map                             -      -
+    2,40%  [k] __netif_receive_skb_list_core                    -      -
+    2,40%  [k] napi_gro_receive                                 -      -
+    2,21%  [k] __xdp_build_skb_from_frame                       -      -
+    2,16%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    2,07%  [k] dev_map_enqueue                                  -      -
+    2,06%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    2,01%  [k] ip_list_rcv                                      -      -
+    1,94%  [k] netif_receive_skb_list_internal                  -      -
+    1,92%  [k] kmem_cache_alloc_bulk                            -      -
+    1,91%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,90%  [k] veth_xdp_rcv                                     -      -
+    1,74%  [k] __list_add_valid                                 -      -
+    1,73%  [k] xdp_do_redirect                                  -      -
+    1,70%  [k] mlx5e_xdp_handle                                 -      -
+    1,63%  [k] free_unref_page_prepare.part.0                   -      -
+    1,61%  [k] dma_map_page_attrs                               -      -
+    1,53%  [k] __alloc_pages_bulk_nodemask                      -      -
+    1,43%  [k] __slab_free                                      -      -
+    1,42%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,36%  [k] eth_type_trans                                   -      -
+    1,32%  [k] __page_pool_alloc_pages_slow                     -      -
+    1,22%  [k] free_unref_page                                  -      -
+    1,18%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,16%  [k] veth_xdp_xmit                                    -      -
+    1,09%  [k] build_skb_around                                 -      -
+    1,03%  [k] gro_normal_one                                   -      -
+    1,02%  [k] page_pool_release_page                           -      -
+    1,00%  [k] ip_rcv_core.isra.0                               -      -
+    1,00%  [k] dma_unmap_page_attrs                             -      -
+    0,99%  [k] __rmqueue_pcplist                                -      -
+    0,95%  [k] free_pcp_prepare                                 -      -
+    0,91%  [k] __build_skb_around                               -      -
     0,84%  [k] kfree_skb                                        -      -
</pre>
</div>
</div>

<div id="outline-container-test02--veth-redirect--page_pool-bulk-64-" class="outline-3">
<h3 id="test02--veth-redirect--page_pool-bulk-64-"><a href="#test02--veth-redirect--page_pool-bulk-64-">test02: veth redirect (page_pool bulk 64)</a></h3>
<div class="outline-text-3" id="text-test02--veth-redirect--page_pool-bulk-64-">
<p>
Same veth setup as above: 
</p>

<p>
Results below with page_pool using bulk=64 for alloc_pages_bulk().
</p>

<pre class="example" id="orgbd1d886">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       4,297,656   0          
XDP-RX CPU      total   4,297,656  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:1   4,297,649   0          
rx_queue_index    0:sum 4,297,649  
</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2021-02-24 Wed 12:06</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
