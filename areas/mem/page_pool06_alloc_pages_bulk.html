<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Testing page-allocator API proposal for alloc_pages_bulk</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/hideshow.css"/>
<script type="text/javascript" src="/styles/bigblow/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/bigblow.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/hideshow.js"></script>
<script type="text/javascript" src="/styles/lib/js/jquery.stickytableheaders.min.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">Testing page-allocator API proposal for alloc_pages_bulk</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Document-index--autogenerated-">Document index (autogenerated)&#xa0;&#xa0;&#xa0;<span class="tag"><span class="toc">toc</span></span></a></li>
<li><a href="#Use-case--page_pool">Use-case: page_pool</a>
<ul>
<li><a href="#Test-setup-for-veth">Test setup for veth</a></li>
</ul>
</li>
<li><a href="#Baseline-test">Baseline test</a>
<ul>
<li><a href="#baseline01--veth-redirect">baseline01: veth redirect</a></li>
</ul>
</li>
<li><a href="#Using-alloc_pages_bulk">Using alloc_pages_bulk</a>
<ul>
<li><a href="#test01--veth-redirect--page_pool-bulk-16-">test01: veth redirect (page_pool bulk 16)</a></li>
<li><a href="#test02--veth-redirect--page_pool-bulk-64-">test02: veth redirect (page_pool bulk 64)</a></li>
</ul>
</li>
<li><a href="#Test-with-RFC-patchset">Test with RFC patchset</a>
<ul>
<li><a href="#initial-test">initial test</a></li>
</ul>
</li>
<li><a href="#patch-notes">patch notes</a>
<ul>
<li><a href="#Follow-up-to-Mel-s-patchset">Follow up to Mel's patchset</a></li>
<li><a href="#bench-test-veth">bench test veth</a></li>
<li><a href="#desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path">desc: net: page_pool: use alloc_pages_bulk in refill code path</a></li>
</ul>
</li>
<li><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v4r2">Test on Mel git-tree: mm-bulk-rebase-v4r2</a>
<ul>
<li><a href="#stg-mail">stg mail</a></li>
<li><a href="#Updated-patch">Updated patch</a></li>
<li><a href="#Pop-patch-using-__alloc_pages_bulk">Pop patch using __alloc_pages_bulk</a></li>
</ul>
</li>
<li><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v5r9">Test on Mel git-tree: mm-bulk-rebase-v5r9</a>
<ul>
<li><a href="#micro-benchmark--page_bench04_bulk">micro-benchmark: page_bench04_bulk</a></li>
<li><a href="#Adjust--inline-__rmqueue_pcplist">Adjust: inline __rmqueue_pcplist</a></li>
<li><a href="#page_pool--use-alloc_pages_bulk_list">page_pool: use alloc_pages_bulk_list</a></li>
<li><a href="#page_pool--use-alloc_pages_bulk_array">page_pool: use alloc_pages_bulk_array</a></li>
<li><a href="#I-cache-layout-for-__alloc_pages_bulk">I-cache layout for __alloc_pages_bulk</a></li>
</ul>
</li>
<li><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v6r5">Test on Mel git-tree: mm-bulk-rebase-v6r5</a>
<ul>
<li><a href="#baseline-kernel">baseline kernel</a>
<ul>
<li><a href="#BASELINE--Investigate-variation-seen">BASELINE: Investigate variation seen</a></li>
</ul>
</li>
<li><a href="#Use-list-variant">Use list variant</a></li>
<li><a href="#Use-array-variant">Use array variant</a>
<ul>
<li><a href="#Investigate-variation-seen">Investigate variation seen</a></li>
</ul>
</li>
<li><a href="#Strange-code-layout">Strange code layout</a></li>
<li><a href="#Send-patches-to-Mel">Send patches to Mel</a></li>
</ul>
</li>
<li><a href="#Test-on-Mel-git-tree--mm-percpu-local_lock-v1r15">Test on Mel git-tree: mm-percpu-local_lock-v1r15</a>
<ul>
<li><a href="#micro-benchmark--page_bench04_bulk--local_lock-v1r15-">micro-benchmark: page_bench04_bulk (local_lock-v1r15)</a></li>
</ul>
</li>
<li><a href="#Micro-optimisations">Micro optimisations</a>
<ul>
<li><a href="#Observations">Observations</a></li>
<li><a href="#Why-happening">Why happening</a></li>
<li><a href="#Explaining-patch-with-fix">Explaining patch with fix</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
Testing experimental patch from Mel Gorman that add alloc_pages_bulk() API.
</p>

<div id="outline-container-Document-index--autogenerated-" class="outline-2">
<h2 id="Document-index--autogenerated-"><a href="#Document-index--autogenerated-">Document index (autogenerated)&#xa0;&#xa0;&#xa0;<span class="tag"><span class="toc">toc</span></span></a></h2>
<div class="outline-text-2" id="text-Document-index--autogenerated-">
<ul class="org-ul">
<li><ul class="org-ul">
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
<li></li>
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
<li></li>
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
<li></li>
<li></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-Use-case--page_pool" class="outline-2">
<h2 id="Use-case--page_pool"><a href="#Use-case--page_pool">Use-case: page_pool</a></h2>
<div class="outline-text-2" id="text-Use-case--page_pool">
<p>
The <code>alloc_pages_bulk()</code> use-case for page_pool is in
<code>__page_pool_alloc_pages_slow()</code>, for then the pool goes empty.
</p>

<p>
The mlx5 driver can easily empty the page_pool "cache", when redirected
xdp_frame's are converted into SKBs.
</p>

<p>
This happens both for veth and cpumap, as SKBs (created from xdp_frame) is
missing callback to page_pool.
</p>
</div>

<div id="outline-container-Test-setup-for-veth" class="outline-3">
<h3 id="Test-setup-for-veth"><a href="#Test-setup-for-veth">Test setup for veth</a></h3>
<div class="outline-text-3" id="text-Test-setup-for-veth">
<p>
It requires some setup and two XDP programs to trigger the case, where
page_pool "cache" goes empty. And use of driver mlx5.
</p>

<p>
First: Create veth pair and enabled link up:
</p>

<pre class="example">
ip link add veth1 type veth peer name veth2
ip link set veth1 up
ip link set veth2 up
</pre>

<p>
Disable GRO/GSO/TSO on the veth devices
</p>
<pre class="example" id="orgf3ab3ba">
ethtool -K veth1 gso off gro off tso off
ethtool -K veth2 gso off gro off tso off
</pre>

<p>
When XDP-redirecting into a veth, we must remember to attach an XDP prog to
the peer device.
</p>

<p>
Redirect frame from mlx5p1 into veth1 (peer veth2)
</p>
<ul class="org-ul">
<li>sudo ./xdp_redirect_map mlx5p1 veth1</li>
</ul>

<p>
Create SKBs from xdp_frame via XDP_PASS on veth2:
</p>
<ul class="org-ul">
<li>sudo ./xdp_rxq_info &#x2013;dev veth2 &#x2013;act XDP_PASS</li>
</ul>

<p>
As the MAC-addr doesn't match the SKB packets are dropped very early, but it
suites our micro-benchmark test case.
</p>
</div>
</div>
</div>

<div id="outline-container-Baseline-test" class="outline-2">
<h2 id="Baseline-test"><a href="#Baseline-test">Baseline test</a></h2>
<div class="outline-text-2" id="text-Baseline-test">
</div>
<div id="outline-container-baseline01--veth-redirect" class="outline-3">
<h3 id="baseline01--veth-redirect"><a href="#baseline01--veth-redirect">baseline01: veth redirect</a></h3>
<div class="outline-text-3" id="text-baseline01--veth-redirect">
<p>
Kernel: Linux broadwell 5.11.0-net-next
</p>

<pre class="example" id="org12b4c9c">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       3,677,958   0          
XDP-RX CPU      total   3,677,958  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   3,677,966   0          
rx_queue_index    0:sum 3,677,966  

</pre>

<pre class="example" id="org12eb58a">
Samples: 81K of event 'cycles', Event count (approx.): 73929158590
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,87%  [k] __netif_receive_skb_core                         -      -
+    3,50%  [k] kmem_cache_free                                  -      -
+    3,26%  [k] dev_gro_receive                                  -      -
+    3,24%  [k] rmqueue                                          -      -
+    3,15%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,09%  [k] __xdp_release_frame                              -      -
+    2,99%  [k] memset_erms                                      -      -
+    2,89%  [k] get_page_from_freelist                           -      -
+    2,82%  [k] __alloc_pages_nodemask                           -      -
+    2,61%  [k] veth_xdp_rcv_one                                 -      -
+    2,54%  [k] free_unref_page_commit                           -      -
+    2,35%  [k] __list_del_entry_valid                           -      -
+    2,29%  [k] __netif_receive_skb_list_core                    -      -
+    2,13%  [k] dev_map_enqueue                                  -      -
+    2,07%  [k] mlx5e_xdp_handle                                 -      -
+    2,04%  [k] __xdp_build_skb_from_frame                       -      -
+    1,95%  [k] napi_gro_receive                                 -      -
+    1,94%  [k] xdp_do_redirect                                  -      -
+    1,85%  [k] netif_receive_skb_list_internal                  -      -
+    1,85%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    1,84%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    1,82%  [k] bpf_xdp_redirect_map                             -      -
+    1,74%  [k] kmem_cache_alloc_bulk                            -      -
+    1,71%  [k] ip_list_rcv                                      -      -
+    1,69%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,56%  [k] veth_xdp_rcv                                     -      -
+    1,48%  [k] __slab_free                                      -      -
+    1,45%  [k] free_unref_page_prepare.part.0                   -      -
+    1,43%  [k] eth_type_trans                                   -      -
+    1,40%  [k] dma_map_page_attrs                               -      -
+    1,30%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,21%  [k] __list_add_valid                                 -      -
+    1,19%  [k] veth_xdp_xmit                                    -      -
+    1,11%  [k] free_unref_page                                  -      -
+    1,07%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,05%  [k] dma_unmap_page_attrs                             -      -
+    1,03%  [k] page_pool_release_page                           -      -
+    1,00%  [k] gro_normal_one                                   -      -
+    0,99%  [k] build_skb_around                                 -      -
+    0,94%  [k] __page_pool_alloc_pages_slow                     -      -
+    0,92%  [k] ip_rcv_core.isra.0                               -      -
+    0,90%  [k] prep_new_page                                    -      -
+    0,89%  [k] __build_skb_around                               -      -
+    0,81%  [k] free_pcp_prepare                                 -      -
</pre>
</div>
</div>
</div>


<div id="outline-container-Using-alloc_pages_bulk" class="outline-2">
<h2 id="Using-alloc_pages_bulk"><a href="#Using-alloc_pages_bulk">Using alloc_pages_bulk</a></h2>
<div class="outline-text-2" id="text-Using-alloc_pages_bulk">
<p>
Kernel: Linux broadwell 5.11.0-net-next-alloc_pages_bulk+
</p>
<ul class="org-ul">
<li>With this fix <a href="https://patchwork.kernel.org/project/netdevbpf/patch/161402344429.1980160.4798557236979159924.stgit@firesoul/">to mlx5 driver</a></li>
</ul>

<p>
This patch: <a href="https://lore.kernel.org/linux-mm/20210210130705.GC3629@suse.de/">https://lore.kernel.org/linux-mm/20210210130705.GC3629@suse.de/</a>
With this fix: <a href="https://lore.kernel.org/linux-mm/20210211091235.GC3697@techsingularity.net/">https://lore.kernel.org/linux-mm/20210211091235.GC3697@techsingularity.net/</a>
</p>
</div>

<div id="outline-container-test01--veth-redirect--page_pool-bulk-16-" class="outline-3">
<h3 id="test01--veth-redirect--page_pool-bulk-16-"><a href="#test01--veth-redirect--page_pool-bulk-16-">test01: veth redirect (page_pool bulk 16)</a></h3>
<div class="outline-text-3" id="text-test01--veth-redirect--page_pool-bulk-16-">
<p>
Same veth setup as above: 
</p>

<p>
Results below with page_pool using bulk=16 for alloc_pages_bulk().
</p>

<pre class="example" id="org9c0fb25">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       4,066,028   0          
XDP-RX CPU      total   4,066,028  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:5   4,066,023   0          
rx_queue_index    0:sum 4,066,023  
</pre>

<pre class="example" id="orgb9a3ba6">
Samples: 51K of event 'cycles', Event count (approx.): 46934149161
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,94%  [k] __netif_receive_skb_core                         -      -
+    4,02%  [k] kmem_cache_free                                  -      -
+    3,78%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,30%  [k] memset_erms                                      -      -
+    3,26%  [k] __xdp_release_frame                              -      -
+    3,23%  [k] dev_gro_receive                                  -      -
+    3,10%  [k] free_unref_page_commit                           -      -
+    3,06%  [k] veth_xdp_rcv_one                                 -      -
+    2,82%  [k] __list_del_entry_valid                           -      -
+    2,42%  [k] bpf_xdp_redirect_map                             -      -
+    2,40%  [k] __netif_receive_skb_list_core                    -      -
+    2,40%  [k] napi_gro_receive                                 -      -
+    2,21%  [k] __xdp_build_skb_from_frame                       -      -
+    2,16%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    2,07%  [k] dev_map_enqueue                                  -      -
+    2,06%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    2,01%  [k] ip_list_rcv                                      -      -
+    1,94%  [k] netif_receive_skb_list_internal                  -      -
+    1,92%  [k] kmem_cache_alloc_bulk                            -      -
+    1,91%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,90%  [k] veth_xdp_rcv                                     -      -
+    1,74%  [k] __list_add_valid                                 -      -
+    1,73%  [k] xdp_do_redirect                                  -      -
+    1,70%  [k] mlx5e_xdp_handle                                 -      -
+    1,63%  [k] free_unref_page_prepare.part.0                   -      -
+    1,61%  [k] dma_map_page_attrs                               -      -
+    1,53%  [k] __alloc_pages_bulk_nodemask                      -      -
+    1,43%  [k] __slab_free                                      -      -
+    1,42%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,36%  [k] eth_type_trans                                   -      -
+    1,32%  [k] __page_pool_alloc_pages_slow                     -      -
+    1,22%  [k] free_unref_page                                  -      -
+    1,18%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,16%  [k] veth_xdp_xmit                                    -      -
+    1,09%  [k] build_skb_around                                 -      -
+    1,03%  [k] gro_normal_one                                   -      -
+    1,02%  [k] page_pool_release_page                           -      -
+    1,00%  [k] ip_rcv_core.isra.0                               -      -
+    1,00%  [k] dma_unmap_page_attrs                             -      -
+    0,99%  [k] __rmqueue_pcplist                                -      -
+    0,95%  [k] free_pcp_prepare                                 -      -
+    0,91%  [k] __build_skb_around                               -      -
     0,84%  [k] kfree_skb                                        -      -
</pre>
</div>
</div>

<div id="outline-container-test02--veth-redirect--page_pool-bulk-64-" class="outline-3">
<h3 id="test02--veth-redirect--page_pool-bulk-64-"><a href="#test02--veth-redirect--page_pool-bulk-64-">test02: veth redirect (page_pool bulk 64)</a></h3>
<div class="outline-text-3" id="text-test02--veth-redirect--page_pool-bulk-64-">
<p>
Same veth setup as above: 
</p>

<p>
Results below with page_pool using bulk=64 for alloc_pages_bulk().
</p>

<pre class="example" id="org1e732ee">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       4,297,656   0          
XDP-RX CPU      total   4,297,656  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:1   4,297,649   0          
rx_queue_index    0:sum 4,297,649  
</pre>
</div>
</div>
</div>

<div id="outline-container-Test-with-RFC-patchset" class="outline-2">
<h2 id="Test-with-RFC-patchset"><a href="#Test-with-RFC-patchset">Test with RFC patchset</a></h2>
<div class="outline-text-2" id="text-Test-with-RFC-patchset">
<p>
Test with new patchset from Mel
</p>
<ul class="org-ul">
<li>Message-Id: &lt;20210224102603.19524-1-mgorman@techsingularity.net&gt;</li>
<li><a href="https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/">https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/</a></li>
</ul>
</div>

<div id="outline-container-initial-test" class="outline-3">
<h3 id="initial-test"><a href="#initial-test">initial test</a></h3>
<div class="outline-text-3" id="text-initial-test">
<p>
bulk=64
</p>

<pre class="example" id="org121fd83">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,368,926   0          
XDP-RX CPU      total   4,368,926  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,368,917   0          
rx_queue_index    0:sum 4,368,917  
</pre>
</div>
</div>
</div>

<div id="outline-container-patch-notes" class="outline-2">
<h2 id="patch-notes"><a href="#patch-notes">patch notes</a></h2>
<div class="outline-text-2" id="text-patch-notes">
</div>
<div id="outline-container-Follow-up-to-Mel-s-patchset" class="outline-3">
<h3 id="Follow-up-to-Mel-s-patchset"><a href="#Follow-up-to-Mel-s-patchset">Follow up to Mel's patchset</a></h3>
<div class="outline-text-3" id="text-Follow-up-to-Mel-s-patchset">
<p>
Patchset V1:
</p>
<pre class="example" id="org8cbd7ee">
stg mail --version='RFC net-next' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210224102603.19524-1-mgorman@techsingularity.net' \
 03-reorder-add-page_pool_dma_map..mm-make-zone-free_area-order
</pre>
<p>
Message-ID: &lt;161419296941.2718959.12575257358107256094.stgit@firesoul&gt;
</p>

<p>
V2 with minor changes and dropping micro-optimisation:
</p>
<pre class="example" id="org0aae64c">
stg mail --version='RFC V2 net-next' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210224102603.19524-1-mgorman@techsingularity.net' \
 05-03-reorder-add-page_pool_dma_map..06-04-page_pool-use-alloc_pages_bulk
</pre>
<p>
Message-ID: &lt;161460522573.3031322.15721946341157092594.stgit@firesoul&gt;
</p>

<blockquote>
<p>
Use bulk order-0 page allocator API for page_pool
</p>

<p>
This is a followup to Mel Gorman's patchset:
</p>
<ul class="org-ul">
<li>Message-Id: &lt;20210224102603.19524-1-mgorman@techsingularity.net&gt;</li>
<li><a href="https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/">https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/</a></li>
</ul>

<p>
Showing page_pool usage of the API for alloc_pages_bulk().
</p>

<p>
Maybe Mel Gorman will/can carry these patches?
(to keep it together with the alloc_pages_bulk API)
</p>
</blockquote>
</div>
</div>

<div id="outline-container-bench-test-veth" class="outline-3">
<h3 id="bench-test-veth"><a href="#bench-test-veth">bench test veth</a></h3>
<div class="outline-text-3" id="text-bench-test-veth">
<p>
Test again:
</p>
<pre class="example" id="org96a4a9f">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       4,302,291   0          
XDP-RX CPU      total   4,302,291  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   4,302,285   0          
rx_queue_index    0:sum 4,302,285  
</pre>
</div>
</div>

<div id="outline-container-desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path" class="outline-3">
<h3 id="desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path"><a href="#desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path">desc: net: page_pool: use alloc_pages_bulk in refill code path</a></h3>
<div class="outline-text-3" id="text-desc--net--page_pool--use-alloc_pages_bulk-in-refill-code-path">
<blockquote>
<p>
There are cases where the page_pool need to refill with pages from the
page allocator. Some workloads cause the page_pool to release pages
instead of recycling these pages.
</p>

<p>
For these workload it can improve performance to bulk alloc pages from
the page-allocator to refill the alloc cache.
</p>

<p>
For XDP-redirect workload with 100G mlx5 driver (that use page_pool)
redirecting xdp_frame packets into a veth, that does XDP_PASS to create
an SKB from the xdp_frame, which then cannot return the page to the
page_pool. In this case, we saw[1] an improvement of 18.8% from using
the alloc_pages_bulk API (3,677,958 pps -&gt; 4,368,926 pps).
</p>

<p>
[1] <a href="https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org">https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org</a>
</p>
</blockquote>
</div>
</div>
</div>


<div id="outline-container-Test-on-Mel-git-tree--mm-bulk-rebase-v4r2" class="outline-2">
<h2 id="Test-on-Mel-git-tree--mm-bulk-rebase-v4r2"><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v4r2">Test on Mel git-tree: mm-bulk-rebase-v4r2</a></h2>
<div class="outline-text-2" id="text-Test-on-Mel-git-tree--mm-bulk-rebase-v4r2">
<p>
Tests based on Mel Gorman's git tree:
</p>
<ul class="org-ul">
<li>git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git</li>
</ul>

<p>
Branch: mm-bulk-rebase-v4r2
</p>
<ul class="org-ul">
<li>Changed the last patch with page_pool changes</li>
</ul>
</div>

<div id="outline-container-stg-mail" class="outline-3">
<h3 id="stg-mail"><a href="#stg-mail">stg mail</a></h3>
<div class="outline-text-3" id="text-stg-mail">
<p>
Promised to followup in Message-ID: &lt;20210315094038.22d6d79a@carbon&gt;
</p>
<ul class="org-ul">
<li>Below stg <a href="https://lore.kernel.org/netdev/161583677541.3715498.6118778324185171839.stgit@firesoul/">Message-ID</a></li>
</ul>

<pre class="example" id="org7b3e27f">
stg mail --version='mel-git' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com --cc alex \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210315094038.22d6d79a@carbon' \
 net-page_pool-use
</pre>

<blockquote>
<p>
Subj: Followup: Update [PATCH 7/7] in Mel's series
</p>

<p>
This patch is against Mel's git-tree:
 git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git
</p>

<p>
Using branch: mm-bulk-rebase-v4r2 but replacing the last patch related to
the page_pool using __alloc_pages_bulk().
</p>

<p>
<a href="https://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git/log/?h=mm-bulk-rebase-v4r2">https://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git/log/?h=mm-bulk-rebase-v4r2</a>
</p>

<p>
While implementing suggestions by Alexander Duyck, I realised that I could
simplify the code further, and simply take the last page from the
pool-&gt;alloc.cache given this avoids special casing the last page.
</p>

<p>
I re-ran performance tests and the improvement have been reduced to 13% from
18% before, but I don't think the rewrite of the specific patch have
anything to do with this.
</p>

<p>
Notes on tests:
 <a href="https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#test-on-mel-git-tree">https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#test-on-mel-git-tree</a>
</p>
</blockquote>

<p>
Performance summary: +13% faster
</p>
<ul class="org-ul">
<li>(3,810,013 pps -&gt; 4,308,208 pps)</li>
<li>((4308208/3810013)-1)*100 = 13.07%</li>
</ul>

<p>
Previous: 18.8% (3,677,958 pps -&gt; 4,368,926 pps).
</p>
<ul class="org-ul">
<li>Thus, slower than before.</li>
<li>Mostly look like better baseline</li>
</ul>
</div>
</div>

<div id="outline-container-Updated-patch" class="outline-3">
<h3 id="Updated-patch"><a href="#Updated-patch">Updated patch</a></h3>
<div class="outline-text-3" id="text-Updated-patch">
<p>
Alexander Duyck point out there was a cleaner way to implement
changes in function <code>__page_pool_alloc_pages_slow()</code>.
</p>

<pre class="example" id="org43de3c3">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,308,208   0          
XDP-RX CPU      total   4,308,208  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,308,208   0          
rx_queue_index    0:sum 4,308,208  
</pre>
</div>
</div>

<div id="outline-container-Pop-patch-using-__alloc_pages_bulk" class="outline-3">
<h3 id="Pop-patch-using-__alloc_pages_bulk"><a href="#Pop-patch-using-__alloc_pages_bulk">Pop patch using __alloc_pages_bulk</a></h3>
<div class="outline-text-3" id="text-Pop-patch-using-__alloc_pages_bulk">
<pre class="example" id="org7240220">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       3,810,013   0          
XDP-RX CPU      total   3,810,013  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:1   3,810,013   0          
rx_queue_index    0:sum 3,810,013  
</pre>
</div>
</div>
</div>

<div id="outline-container-Test-on-Mel-git-tree--mm-bulk-rebase-v5r9" class="outline-2">
<h2 id="Test-on-Mel-git-tree--mm-bulk-rebase-v5r9"><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v5r9">Test on Mel git-tree: mm-bulk-rebase-v5r9</a></h2>
<div class="outline-text-2" id="text-Test-on-Mel-git-tree--mm-bulk-rebase-v5r9">
<p>
Tests based on Mel Gorman's git tree:
</p>
<ul class="org-ul">
<li>git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git</li>
<li>Branch: mm-bulk-rebase-v5r9</li>
</ul>
</div>

<div id="outline-container-micro-benchmark--page_bench04_bulk" class="outline-3">
<h3 id="micro-benchmark--page_bench04_bulk"><a href="#micro-benchmark--page_bench04_bulk">micro-benchmark: page_bench04_bulk</a></h3>
<div class="outline-text-3" id="text-micro-benchmark--page_bench04_bulk">
<p>
Notice these "per elem" measurements are alloc+free cost for order-0 pages
</p>

<p>
page_bench04_bulk micro-benchmark on branch: mm-bulk-rebase-v5r9
</p>
<ul class="org-ul">
<li><a href="https://lore.kernel.org/netdev/20210322130446.0a505db0@carbon/">https://lore.kernel.org/netdev/20210322130446.0a505db0@carbon/</a></li>
</ul>

<p>
CPU: Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz
</p>

<pre class="example" id="orgdd2133f">
BASELINE
 single_page alloc+put: Per elem: 199 cycles(tsc) 55.472 ns

LIST variant: time_bulk_page_alloc_free_list: step=bulk size

 Per elem: 206 cycles(tsc) 57.478 ns (step:1)
 Per elem: 154 cycles(tsc) 42.861 ns (step:2)
 Per elem: 145 cycles(tsc) 40.536 ns (step:3)
 Per elem: 142 cycles(tsc) 39.477 ns (step:4)
 Per elem: 142 cycles(tsc) 39.610 ns (step:8)
 Per elem: 137 cycles(tsc) 38.155 ns (step:16)
 Per elem: 135 cycles(tsc) 37.739 ns (step:32)
 Per elem: 134 cycles(tsc) 37.282 ns (step:64)
 Per elem: 133 cycles(tsc) 36.993 ns (step:128)

ARRAY variant: time_bulk_page_alloc_free_array: step=bulk size

 Per elem: 202 cycles(tsc) 56.383 ns (step:1)
 Per elem: 144 cycles(tsc) 40.047 ns (step:2)
 Per elem: 134 cycles(tsc) 37.339 ns (step:3)
 Per elem: 128 cycles(tsc) 35.578 ns (step:4)
 Per elem: 120 cycles(tsc) 33.592 ns (step:8)
 Per elem: 116 cycles(tsc) 32.362 ns (step:16)
 Per elem: 113 cycles(tsc) 31.476 ns (step:32)
 Per elem: 110 cycles(tsc) 30.633 ns (step:64)
 Per elem: 110 cycles(tsc) 30.596 ns (step:128)
</pre>
</div>
</div>


<div id="outline-container-Adjust--inline-__rmqueue_pcplist" class="outline-3">
<h3 id="Adjust--inline-__rmqueue_pcplist"><a href="#Adjust--inline-__rmqueue_pcplist">Adjust: inline __rmqueue_pcplist</a></h3>
<div class="outline-text-3" id="text-Adjust--inline-__rmqueue_pcplist">
<p>
When __alloc_pages_bulk() got introduced two callers of
__rmqueue_pcplist exist and the compiler chooses to not inline
this function.
</p>

<div class="org-src-container">
<pre class="src src-sh"> ./scripts/bloat-o-meter vmlinux-before vmlinux-inline__rmqueue_pcplist
add/remove: 0/1 grow/shrink: 2/0 up/down: 164/-125 (39)
Function                                     old     new   delta
rmqueue                                     2197    2296     +99
__alloc_pages_bulk                          1921    1986     +65
__rmqueue_pcplist                            125       -    -125
Total: <span style="font-weight: bold; font-style: italic;">Before</span>=19374127, <span style="font-weight: bold; font-style: italic;">After</span>=19374166, chg +0.00%
</pre>
</div>

<p>
modprobe page_bench04_bulk loops=$((10**7))
</p>

<p>
Type:time_bulk_page_alloc_free_array
</p>
<ul class="org-ul">
<li>Per elem: 106 cycles(tsc) 29.595 ns (step:64)</li>
<li>(measurement period time:0.295955434 sec time_interval:295955434)</li>
<li>(invoke count:10000000 tsc_interval:1065447105)</li>
</ul>

<p>
Before:
</p>
<ul class="org-ul">
<li>Per elem: 110 cycles(tsc) 30.633 ns (step:64)</li>
</ul>

<div class="org-src-container">
<pre class="src src-diff">diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 2cbb8da811ab..f60f51a97a7b 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/mm/page_alloc.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/mm/page_alloc.c</span>
<span style="font-weight: bold;">@@ -3415,7 +3415,8 @@</span><span style="font-weight: bold;"> static inline void zone_statistics(struct zone *preferred_zone, struct zone *z)</span>
 }

 /* Remove page from the per-cpu list, caller must protect the list */
-static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
+static inline
+struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
                        unsigned int alloc_flags,
                        struct per_cpu_pages *pcp,
                        struct list_head *list)
</pre>
</div>

<p>
Below tests include above patch.
</p>
</div>
</div>

<div id="outline-container-page_pool--use-alloc_pages_bulk_list" class="outline-3">
<h3 id="page_pool--use-alloc_pages_bulk_list"><a href="#page_pool--use-alloc_pages_bulk_list">page_pool: use alloc_pages_bulk_list</a></h3>
<div class="outline-text-3" id="text-page_pool--use-alloc_pages_bulk_list">
<pre class="example" id="org9b34a10">
unning XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      4       3,961,569   0          
XDP-RX CPU      total   3,961,569  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:4   3,961,569   0          
rx_queue_index    0:sum 3,961,569  
</pre>
</div>
</div>

<div id="outline-container-page_pool--use-alloc_pages_bulk_array" class="outline-3">
<h3 id="page_pool--use-alloc_pages_bulk_array"><a href="#page_pool--use-alloc_pages_bulk_array">page_pool: use alloc_pages_bulk_array</a></h3>
<div class="outline-text-3" id="text-page_pool--use-alloc_pages_bulk_array">
<pre class="example" id="org39c2c95">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       4,067,120   0          
XDP-RX CPU      total   4,067,120  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:5   4,067,127   0          
rx_queue_index    0:sum 4,067,127  
</pre>

<p>
The results a not super stable, as after a while I get this result:
</p>
<pre class="example" id="orgb230fd3">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,218,956   0          
XDP-RX CPU      total   4,218,956  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,218,960   0          
rx_queue_index    0:sum 4,218,960  
</pre>
</div>
</div>

<div id="outline-container-I-cache-layout-for-__alloc_pages_bulk" class="outline-3">
<h3 id="I-cache-layout-for-__alloc_pages_bulk"><a href="#I-cache-layout-for-__alloc_pages_bulk">I-cache layout for __alloc_pages_bulk</a></h3>
<div class="outline-text-3" id="text-I-cache-layout-for-__alloc_pages_bulk">
<p>
Looking at perf-report and ASM-code for __alloc_pages_bulk() then the code
activated is suboptimal. The compiler guess wrong and place unlikely code in
the beginning. Due to the use of WARN_ON_ONCE() macro the <code>UD2</code> asm
instruction is added to the code, which confuse the I-cache prefetcher in
the CPU.
</p>

<p>
Perf-stat <b>BEFORE</b> during 4,174,649 pps:
</p>
<pre class="example" id="org0e9815e">
$ perf stat -C3 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -e icache.hit -e icache.misses -e icache.ifdata_stall -r 4 sleep 1

 Performance counter stats for 'CPU(s) 3' (4 runs):

     3.967.401.581      cycles                                                        ( +-  0,02% )  (69,23%)
     9.328.404.288      instructions              #    2,35  insn per cycle           ( +-  0,04% )  (76,92%)
        40.081.612      cache-references                                              ( +-  0,06% )  (76,92%)
             1.925      cache-misses              #    0,005 % of all cache refs      ( +- 85,44% )  (76,92%)
     1.772.491.245      branches:k                                                    ( +-  0,03% )  (76,92%)
         3.897.378      branch-misses:k           #    0,22% of all branches          ( +-  0,31% )  (76,92%)
         4.909.219      l2_rqsts.all_code_rd                                          ( +-  0,32% )  (76,92%)
         4.285.616      l2_rqsts.code_rd_hit                                          ( +-  0,30% )  (76,92%)
           620.169      l2_rqsts.code_rd_miss                                         ( +-  0,38% )  (76,92%)
         1.633.584      L1-icache-load-misses                                         ( +-  0,83% )  (76,92%)
       920.823.524      icache.hit                                                    ( +-  0,03% )  (61,55%)
         1.635.497      icache.misses                                                 ( +-  0,92% )  (61,55%)
        15.893.532      icache.ifdata_stall                                           ( +-  1,38% )  (61,55%)
</pre>

<p>
Above cycles 3.97 GHz indicate turbo-mode was engaged.
</p>

<p>
Perf-stat <b>AFTER</b> during 4,284,779 pps:
</p>
<pre class="example" id="org222ad13">
$ perf stat -C4 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -e icache.hit -e icache.misses -e icache.ifdata_stall -r 4 sleep 1

 Performance counter stats for 'CPU(s) 4' (4 runs):

     3.780.344.586      cycles                                                        ( +-  0,00% )  (69,23%)
     9.208.083.065      instructions              #    2,44  insn per cycle           ( +-  0,01% )  (76,92%)
        41.010.130      cache-references                                              ( +-  0,12% )  (76,92%)
             2.063      cache-misses              #    0,005 % of all cache refs      ( +- 68,85% )  (76,92%)
     1.770.974.127      branches:k                                                    ( +-  0,01% )  (76,92%)
         3.378.947      branch-misses:k           #    0,19% of all branches          ( +-  0,10% )  (76,92%)
         4.002.071      l2_rqsts.all_code_rd                                          ( +-  0,39% )  (76,92%)
         3.596.114      l2_rqsts.code_rd_hit                                          ( +-  0,58% )  (76,92%)
           406.726      l2_rqsts.code_rd_miss                                         ( +-  2,86% )  (76,92%)
         1.315.880      L1-icache-load-misses                                         ( +-  0,55% )  (76,92%)
       860.746.134      icache.hit                                                    ( +-  0,03% )  (61,55%)
         1.315.046      icache.misses                                                 ( +-  0,52% )  (61,55%)
         9.666.533      icache.ifdata_stall                                           ( +-  0,72% )  (61,55%)
</pre>

<p>
When comparing these perf stats then it is important to realise that
workload performance was increased +110,130 pps (4174649-4284779). Thus,
take that into account as counts can be higher due to factor.
</p>

<p>
Notice turbo-mode didn't kick in above 3.78GHz. But new measurement below it
did get "turbo-mode" enabled.
</p>

<p>
Perf-stat <b>AFTER</b> during 4,263,396 pps:
</p>
<pre class="example" id="orge75e878">
perf stat -C3 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -e icache.hit -e icache.misses -e icache.ifdata_stall -r 4 sleep 1

Performance counter stats for 'CPU(s) 3' (4 runs):

    3.972.084.312      cycles                                                        ( +-  0,02% )  (69,23%)
    9.377.688.902      instructions              #    2,36  insn per cycle           ( +-  0,03% )  (76,92%)
       41.732.726      cache-references                                              ( +-  0,05% )  (76,92%)
            1.876      cache-misses              #    0,004 % of all cache refs      ( +- 78,93% )  (76,92%)
    1.798.074.138      branches:k                                                    ( +-  0,03% )  (76,92%)
        3.790.004      branch-misses:k           #    0,21% of all branches          ( +-  0,16% )  (76,92%)
        8.131.686      l2_rqsts.all_code_rd                                          ( +-  0,09% )  (76,92%)
        7.689.516      l2_rqsts.code_rd_hit                                          ( +-  0,11% )  (76,92%)
          442.190      l2_rqsts.code_rd_miss                                         ( +-  0,72% )  (76,92%)
        2.063.152      L1-icache-load-misses                                         ( +-  0,34% )  (76,92%)
      949.080.913      icache.hit                                                    ( +-  0,04% )  (61,55%)
        2.062.373      icache.misses                                                 ( +-  0,34% )  (61,55%)
       13.514.870      icache.ifdata_stall                                           ( +-  0,66% )  (61,55%)
</pre>

<p>
Above result is slightly strange: Turbo-mode, but slightly slower PPS
benchmark and it have almost double l2_rqsts.all_code_rd. <b><b>UPDATE</b></b>: There
were a pcp/pmcd service running that seems to disturb the accuracy of these
measurements.
</p>
</div>
</div>
</div>


<div id="outline-container-Test-on-Mel-git-tree--mm-bulk-rebase-v6r5" class="outline-2">
<h2 id="Test-on-Mel-git-tree--mm-bulk-rebase-v6r5"><a href="#Test-on-Mel-git-tree--mm-bulk-rebase-v6r5">Test on Mel git-tree: mm-bulk-rebase-v6r5</a></h2>
<div class="outline-text-2" id="text-Test-on-Mel-git-tree--mm-bulk-rebase-v6r5">
<p>
Mel Gorman's git tree:
</p>
<ul class="org-ul">
<li>git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git</li>
</ul>

<p>
Branch: mm-bulk-rebase-v6r5
</p>
</div>

<div id="outline-container-baseline-kernel" class="outline-3">
<h3 id="baseline-kernel"><a href="#baseline-kernel">baseline kernel</a></h3>
<div class="outline-text-3" id="text-baseline-kernel">
<p>
Kernel: 5.12.0-rc4-mel-mm-bulk-rebase-v6r5-baseline
</p>
<pre class="example" id="org8bb7f0c">
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       3,771,046   0          
XDP-RX CPU      total   3,771,046  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:5   3,771,054   0          
rx_queue_index    0:sum 3,771,054  
</pre>

<pre class="example" id="org061f14a">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       3,690,327   0          
XDP-RX CPU      total   3,690,327  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   3,690,335   0          
rx_queue_index    0:sum 3,690,335  
</pre>

<pre class="example" id="orgee6ec39">
[broadwell ~]
$ perf stat -C0 -e cycles -e  instructions -e cache-references \
 -e cache-misses -e branches:k -e branch-misses:k \
 -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss \
 -e L1-icache-load-misses -e icache.hit -e icache.misses \
 -e icache.ifdata_stall -r 4 sleep 1

 Performance counter stats for 'CPU(s) 0' (4 runs):

     3.781.104.906      cycles                                                        ( +-  0,01% )  (69,23%)
     9.160.272.376      instructions              #    2,42  insn per cycle           ( +-  0,02% )  (76,93%)
        38.754.093      cache-references                                              ( +-  0,14% )  (76,93%)
             3.302      cache-misses              #    0,009 % of all cache refs      ( +- 38,71% )  (76,93%)
     1.702.142.682      branches:k                                                    ( +-  0,02% )  (76,93%)
         3.044.869      branch-misses:k           #    0,18% of all branches          ( +-  0,16% )  (76,93%)
         4.327.779      l2_rqsts.all_code_rd                                          ( +-  1,06% )  (76,93%)
         3.169.107      l2_rqsts.code_rd_hit                                          ( +-  1,81% )  (76,93%)
         1.156.787      l2_rqsts.code_rd_miss                                         ( +-  1,67% )  (76,93%)
         2.031.427      L1-icache-load-misses                                         ( +-  1,00% )  (76,93%)
       862.034.302      icache.hit                                                    ( +-  0,03% )  (61,53%)
         2.031.444      icache.misses                                                 ( +-  1,01% )  (61,53%)
        26.138.294      icache.ifdata_stall                                           ( +-  1,43% )  (61,53%)
</pre>
</div>

<div id="outline-container-BASELINE--Investigate-variation-seen" class="outline-4">
<h4 id="BASELINE--Investigate-variation-seen"><a href="#BASELINE--Investigate-variation-seen">BASELINE: Investigate variation seen</a></h4>
<div class="outline-text-4" id="text-BASELINE--Investigate-variation-seen">
<p>
As below run the test for a long time to establish variations happening in
the system.
</p>

<p>
Result was average: 3,743,497 pps
</p>
<pre class="example" id="orgc8734e9">
n=8756  cur-pps:3871776 avg=3743497 max=3914720 min=3621229
</pre>

<p>
The min to max variation:
</p>
<ul class="org-ul">
<li>3914720 - 3621229 =</li>
<li>(1/3914720-1/3621229)*10^9 = -20.70 nanosec</li>
<li>(1-(3621229/3914720))*100 = 7.50%</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-Use-list-variant" class="outline-3">
<h3 id="Use-list-variant"><a href="#Use-list-variant">Use list variant</a></h3>
<div class="outline-text-3" id="text-Use-list-variant">
<p>
Kernel: 5.12.0-rc4-mel-mm-bulk-rebase-v6r5-jesper05-list+
</p>
<pre class="example" id="orgcafe8ab">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      4       3,940,242   0          
XDP-RX CPU      total   3,940,242  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:4   3,940,243   0          
rx_queue_index    0:sum 3,940,243  
</pre>

<pre class="example" id="org0c82170">
3.780.991.660      cycles                                                        ( +-  0,01% )  (69,23%)
8.983.214.383      instructions              #    2,38  insn per cycle           ( +-  0,03% )  (76,92%)
   40.349.872      cache-references                                              ( +-  0,10% )  (76,92%)
        3.040      cache-misses              #    0,008 % of all cache refs      ( +- 55,39% )  (76,92%)
1.717.544.811      branches:k                                                    ( +-  0,04% )  (76,92%)
    3.718.282      branch-misses:k           #    0,22% of all branches          ( +-  0,06% )  (76,92%)
    6.715.245      l2_rqsts.all_code_rd                                          ( +-  0,70% )  (76,92%)
    5.728.355      l2_rqsts.code_rd_hit                                          ( +-  0,85% )  (76,92%)
      985.961      l2_rqsts.code_rd_miss                                         ( +-  0,41% )  (76,92%)
    2.528.346      L1-icache-load-misses                                         ( +-  0,81% )  (76,92%)
  893.070.210      icache.hit                                                    ( +-  0,05% )  (61,54%)
    2.524.908      icache.misses                                                 ( +-  0,76% )  (61,54%)
   25.131.747      icache.ifdata_stall                                           ( +-  0,78% )  (61,54%)
</pre>
</div>
</div>

<div id="outline-container-Use-array-variant" class="outline-3">
<h3 id="Use-array-variant"><a href="#Use-array-variant">Use array variant</a></h3>
<div class="outline-text-3" id="text-Use-array-variant">
<p>
Kernel: 5.12.0-rc4-mel-mm-bulk-rebase-v6r5-jesper05-array+
</p>
<pre class="example" id="org746e24c">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      4       4,249,224   0          
XDP-RX CPU      total   4,249,224  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:4   4,249,232   0          
rx_queue_index    0:sum 4,249,232  
</pre>

<pre class="example" id="orgd11f442">
$ perf stat -C4 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -e icache.hit -e icache.misses -e icache.ifdata_stall -r 4 sleep 1

 Performance counter stats for 'CPU(s) 4' (4 runs):

     3.780.890.314      cycles                                                        ( +-  0,01% )  (69,22%)
     9.246.317.219      instructions              #    2,45  insn per cycle           ( +-  0,04% )  (76,92%)
        41.257.264      cache-references                                              ( +-  0,11% )  (76,92%)
             2.234      cache-misses              #    0,005 % of all cache refs      ( +- 80,61% )  (76,92%)
     1.756.311.941      branches:k                                                    ( +-  0,04% )  (76,92%)
         3.365.731      branch-misses:k           #    0,19% of all branches          ( +-  1,21% )  (76,92%)
         4.083.650      l2_rqsts.all_code_rd                                          ( +-  0,77% )  (76,92%)
         3.424.494      l2_rqsts.code_rd_hit                                          ( +-  0,95% )  (76,92%)
           659.806      l2_rqsts.code_rd_miss                                         ( +-  0,53% )  (76,92%)
         1.544.119      L1-icache-load-misses                                         ( +-  0,65% )  (76,92%)
       874.066.356      icache.hit                                                    ( +-  0,07% )  (61,54%)
         1.542.576      icache.misses                                                 ( +-  0,63% )  (61,54%)
        17.672.121      icache.ifdata_stall                                           ( +-  0,53% )  (61,54%)
</pre>

<p>
Same kernel performance variations happens, this is lowest result:
</p>
<pre class="example" id="orgc9b61eb">
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      4       4,014,404   0          
XDP-RX CPU      total   4,014,404  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:4   4,014,402   0          
rx_queue_index    0:sum 4,014,402  
</pre>

<pre class="example" id="org9229ace">
3.780.548.548      cycles                                                        ( +-  0,02% )  (69,22%)
8.952.399.813      instructions              #    2,37  insn per cycle           ( +-  0,04% )  (76,92%)
   40.897.182      cache-references                                              ( +-  0,10% )  (76,92%)
        7.464      cache-misses              #    0,018 % of all cache refs      ( +- 52,23% )  (76,92%)
1.695.347.517      branches:k                                                    ( +-  0,04% )  (76,92%)
    3.747.167      branch-misses:k           #    0,22% of all branches          ( +-  0,42% )  (76,92%)
    6.056.595      l2_rqsts.all_code_rd                                          ( +-  0,22% )  (76,92%)
    5.168.875      l2_rqsts.code_rd_hit                                          ( +-  0,17% )  (76,92%)
      886.584      l2_rqsts.code_rd_miss                                         ( +-  0,64% )  (76,92%)
    2.105.558      L1-icache-load-misses                                         ( +-  0,81% )  (76,93%)
  938.533.354      icache.hit                                                    ( +-  0,07% )  (61,54%)
    2.105.882      icache.misses                                                 ( +-  0,74% )  (61,55%)
   24.221.557      icache.ifdata_stall                                           ( +-  0,41% )  (61,54%)
</pre>

<p>
Another test, and perf stat with reduced PMU events (at 4,051,499 pps):
</p>
<pre class="example" id="org7017341">
$ perf stat -C5 -e cycles -e  instructions -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e icache.hit -e icache.misses -e icache.ifdata_stall -r 4 sleep 1

     3.804.088.394      cycles                                                        ( +-  0,02% )
     9.078.500.414      instructions              #    2,39  insn per cycle           ( +-  0,09% )
     1.722.383.884      branches:k                                                    ( +-  0,07% )
         3.740.082      branch-misses:k           #    0,22% of all branches          ( +-  1,54% )
         4.763.321      l2_rqsts.all_code_rd                                          ( +-  0,89% )
         4.174.932      l2_rqsts.code_rd_hit                                          ( +-  1,00% )
           588.384      l2_rqsts.code_rd_miss                                         ( +-  0,44% )
       879.129.268      icache.hit                                                    ( +-  0,04% )
         1.774.102      icache.misses                                                 ( +-  0,54% )
        17.786.226      icache.ifdata_stall                                           ( +-  0,18% )
</pre>
</div>

<div id="outline-container-Investigate-variation-seen" class="outline-4">
<h4 id="Investigate-variation-seen"><a href="#Investigate-variation-seen">Investigate variation seen</a></h4>
<div class="outline-text-4" id="text-Investigate-variation-seen">
<p>
To investigate the variation seen, I ran a longer test, capturing the output
from <code>xdp_redirect_map</code> (samples every 2 sec) for approx 20 hours. (One
caveat is that program does a simple <code>sleep(2)</code> and assumes the cost of
BPF-map calls are zero and also that sleep period was accurate, this can
also screw the results).
</p>

<p>
The output was run through:
</p>
<div class="org-src-container">
<pre class="src src-sh">cat ~/out01-redir-mlx5p1-into-veth1 | <span style="font-style: italic;">\</span>
 awk <span style="font-style: italic;">'BEGIN{max=0; min=0xFFFFFFFF; n=0; c=0}</span>
<span style="font-style: italic;">      /ifindex/ {n++; c+=$3; avg=c/n; max=($3 &gt; max)?$3:max; min=($3 &lt; min)?$3:min; printf("n=%d\tcur-pps:%d\tavg=%d max=%d min=%d\n", n, $3, avg, max, min)}'</span>
</pre>
</div>

<p>
Result was average: 4,222,960 pps
</p>
<pre class="example" id="org0a22a10">
n=37004 cur-pps:4199584 avg=4222960 max=4448640 min=3797280
</pre>

<p>
The min to max variation:
</p>
<ul class="org-ul">
<li>4448640 - 3797280 = 651360 pps</li>
<li>(1/4448640-1/3797280)*10^9 = -38.56 nanosec</li>
<li>(1-(3797280/4448640))*100 = 14.64%</li>
</ul>

<p>
Removing some outliers (35 records), that indicate that for 6 second
periods the performance drops and then is restored afterwards:
</p>
<pre class="example" id="org95f5ddc">
n=36968 cur-pps:4199584 avg=4223221 max=4448640 min=4016608
</pre>

<p>
The min to max variation:
</p>
<ul class="org-ul">
<li>4448640 - 4016608 = 432032 pps</li>
<li>(1/4448640-1/4016608)*10^9 = -24.18 ns</li>
<li>(1-(4016608/4448640))*100 = 9.71%</li>
<li>Avg value only changed 261 (4222960-4223221)</li>
</ul>

<p>
AFTER removing the outliers, the variation comes close to the baseline
variation testing done above (was -20.70 ns and 7.50%).
</p>
</div>
</div>
</div>

<div id="outline-container-Strange-code-layout" class="outline-3">
<h3 id="Strange-code-layout"><a href="#Strange-code-layout">Strange code layout</a></h3>
<div class="outline-text-3" id="text-Strange-code-layout">
<p>
Kernel: 5.12.0-rc4-mel-mm-bulk-rebase-v6r5-jesper05-array+
</p>

<p>
Strange code for alloc_pages_bulk, shows up as <code>__alloc_pages_bulk.part.0</code>.
The compile choose to split-up function in two, which seems very strange.
</p>

<p>
And ASM for __alloc_pages_bulk is very small:
</p>
<div class="org-src-container">
<pre class="src src-asm">       <span style="font-weight: bold;">&#9474;</span>    ffffffff8122d930 &lt;__alloc_pages_bulk&gt;:
       <span style="font-weight: bold;">&#9474;</span>    __alloc_pages_bulk():
       <span style="font-weight: bold;">&#9474;</span>    &#8594; callq __fentry__
<span style="font-weight: bold;">100</span>,00 &#9474;      test  <span style="font-weight: bold; font-style: italic;">%ecx</span>,<span style="font-weight: bold; font-style: italic;">%ecx</span>
       <span style="font-weight: bold;">&#9474;</span>    &#8595; jle   e
       <span style="font-weight: bold;">&#9474;</span>    &#8594; jmpq  ffffffff8122d240 &lt;__alloc_pages_bulk.part.0&gt;
       <span style="font-weight: bold;">&#9474;</span> e:   xor   <span style="font-weight: bold; font-style: italic;">%eax</span>,<span style="font-weight: bold; font-style: italic;">%eax</span>
       <span style="font-weight: bold;">&#9474;</span>    &#8592; retq
</pre>
</div>

<p>
Verified that it is related to below code.  With patch applied the
<code>__alloc_pages_bulk.part.0</code> construct goes away:
</p>
<div class="org-src-container">
<pre class="src src-diff">diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index d900e92884b2..bab456affe6a 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/mm/page_alloc.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/mm/page_alloc.c</span>
<span style="font-weight: bold;">@@ -5002,8 +5002,8 @@</span><span style="font-weight: bold;"> int __alloc_pages_bulk(gfp_t gfp, int preferred_nid,</span>
        unsigned int alloc_flags;
        int nr_populated = 0;

-       if (unlikely(nr_pages &lt;= 0))
-               return 0;
+//     if (unlikely(nr_pages &lt;= 0))
+//             return 0;

        /*
</pre>
</div>
</div>
</div>

<div id="outline-container-Send-patches-to-Mel" class="outline-3">
<h3 id="Send-patches-to-Mel"><a href="#Send-patches-to-Mel">Send patches to Mel</a></h3>
<div class="outline-text-3" id="text-Send-patches-to-Mel">
<div class="org-src-container">
<pre class="src src-sh">stg mail --version=<span style="font-style: italic;">'mel-git'</span> --edit-cover --cc meup <span style="font-style: italic;">\</span>
 --to mel --cc chuck.lever@oracle.com --cc alex <span style="font-style: italic;">\</span>
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml <span style="font-style: italic;">\</span>
 net-page_pool-refactor-dma_map..convert-page_pool_use_array_variant
</pre>
</div>
<p>
<a href="https://lore.kernel.org/netdev/161662166301.940814.9765023867613542235.stgit@firesoul/">https://lore.kernel.org/netdev/161662166301.940814.9765023867613542235.stgit@firesoul/</a>
</p>

<p>
Cover:
</p>
<blockquote>
<p>
Subject: page_pool using alloc_pages_bulk API
</p>

<p>
This patchset is against Mel's tree:
</p>
<ul class="org-ul">
<li>git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git</li>
<li>Branch: mm-bulk-rebase-v6r5</li>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git/log/?h=mm-bulk-rebase-v6r5">https://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git/log/?h=mm-bulk-rebase-v6r5</a></li>
</ul>

<p>
The benchmarks are here:
</p>
<ul class="org-ul">
<li><a href="https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#test-on-mel-git-tree-mm-bulk-rebase-v6r5">https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#test-on-mel-git-tree-mm-bulk-rebase-v6r5</a></li>
</ul>

<p>
The compiler choose a strange code layout see here:
</p>
<ul class="org-ul">
<li><a href="https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#strange-code-layout">https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#strange-code-layout</a></li>
<li>Used: gcc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)</li>
</ul>

<p>
Intent is for Mel to pickup these patches.
</p>
</blockquote>
</div>
</div>
</div>


<div id="outline-container-Test-on-Mel-git-tree--mm-percpu-local_lock-v1r15" class="outline-2">
<h2 id="Test-on-Mel-git-tree--mm-percpu-local_lock-v1r15"><a href="#Test-on-Mel-git-tree--mm-percpu-local_lock-v1r15">Test on Mel git-tree: mm-percpu-local_lock-v1r15</a></h2>
<div class="outline-text-2" id="text-Test-on-Mel-git-tree--mm-percpu-local_lock-v1r15">
<p>
Mel Gorman's git tree:
</p>
<ul class="org-ul">
<li>git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git</li>
</ul>

<p>
Branch: mm-percpu-local_lock-v1r15
</p>
</div>

<div id="outline-container-micro-benchmark--page_bench04_bulk--local_lock-v1r15-" class="outline-3">
<h3 id="micro-benchmark--page_bench04_bulk--local_lock-v1r15-"><a href="#micro-benchmark--page_bench04_bulk--local_lock-v1r15-">micro-benchmark: page_bench04_bulk (local_lock-v1r15)</a></h3>
<div class="outline-text-3" id="text-micro-benchmark--page_bench04_bulk--local_lock-v1r15-">
<p>
Notice these "per elem" measurements are alloc+free cost for order-0 pages
</p>

<p>
uname:
</p>
<ul class="org-ul">
<li>5.12.0-rc4-mel-mm-percpu-local_lock-v1r15+ #52 SMP PREEMPT</li>
</ul>

<pre class="example" id="org62c51fe">
BASELINE
 single_page alloc+put: 194 cycles(tsc) 54.106 ns

LIST variant: time_bulk_page_alloc_free_list: step=bulk size

 Per elem: 200 cycles(tsc) 55.667 ns (step:1)
 Per elem: 143 cycles(tsc) 39.755 ns (step:2)
 Per elem: 132 cycles(tsc) 36.758 ns (step:3)
 Per elem: 128 cycles(tsc) 35.795 ns (step:4)
 Per elem: 123 cycles(tsc) 34.339 ns (step:8)
 Per elem: 120 cycles(tsc) 33.396 ns (step:16)
 Per elem: 118 cycles(tsc) 32.806 ns (step:32)
 Per elem: 115 cycles(tsc) 32.169 ns (step:64)
 Per elem: 116 cycles(tsc) 32.258 ns (step:128)

ARRAY variant: time_bulk_page_alloc_free_array: step=bulk size

 Per elem: 195 cycles(tsc) 54.225 ns (step:1)
 Per elem: 127 cycles(tsc) 35.492 ns (step:2)
 Per elem: 117 cycles(tsc) 32.643 ns (step:3)
 Per elem: 111 cycles(tsc) 30.992 ns (step:4)
 Per elem: 106 cycles(tsc) 29.606 ns (step:8)
 Per elem: 102 cycles(tsc) 28.532 ns (step:16)
 Per elem: 99 cycles(tsc) 27.728 ns (step:32)
 Per elem: 98 cycles(tsc) 27.252 ns (step:64)
 Per elem: 97 cycles(tsc) 27.090 ns (step:128)
</pre>
</div>
</div>
</div>

<div id="outline-container-Micro-optimisations" class="outline-2">
<h2 id="Micro-optimisations"><a href="#Micro-optimisations">Micro optimisations</a></h2>
<div class="outline-text-2" id="text-Micro-optimisations">
<p>
<b>UPDATE</b>: Choosing to drop this patch, it is waste too much memory and
it too fragile as it depends on compiler behaviour.
</p>

<p>
Document steps in micro optimizing page-alloactor code:
</p>
<ul class="org-ul">
<li>make zone-&gt;free_area[order] access faster</li>
</ul>
</div>

<div id="outline-container-Observations" class="outline-3">
<h3 id="Observations"><a href="#Observations">Observations</a></h3>
<div class="outline-text-3" id="text-Observations">
<p>
The code del_page_from_free_list() generate a strange imul operation:
</p>
<pre class="example" id="orgf312543">
imul   $0x58,%rax,%rax
</pre>

<div class="org-src-container">
<pre class="src src-C"><span style="font-weight: bold;">static</span> <span style="font-weight: bold;">inline</span> <span style="font-weight: bold; text-decoration: underline;">void</span> <span style="font-weight: bold;">del_page_from_free_list</span>(<span style="font-weight: bold;">struct</span> <span style="font-weight: bold; text-decoration: underline;">page</span> *<span style="font-weight: bold; font-style: italic;">page</span>, <span style="font-weight: bold;">struct</span> <span style="font-weight: bold; text-decoration: underline;">zone</span> *<span style="font-weight: bold; font-style: italic;">zone</span>,
                                           <span style="font-weight: bold; text-decoration: underline;">unsigned</span> <span style="font-weight: bold; text-decoration: underline;">int</span> <span style="font-weight: bold; font-style: italic;">order</span>)
{
        <span style="font-weight: bold; font-style: italic;">/* </span><span style="font-weight: bold; font-style: italic;">clear reported state and update reported page count </span><span style="font-weight: bold; font-style: italic;">*/</span>
        <span style="font-weight: bold;">if</span> (page_reported(page))
                __ClearPageReported(page);

        list_del(&amp;page-&gt;lru);
        __ClearPageBuddy(page);
        set_page_private(page, 0);
        zone-&gt;free_area[order].nr_free--;
</pre>
</div>

<p>
Tracked this down to:
</p>
<div class="org-src-container">
<pre class="src src-C"><span style="font-weight: bold;">struct</span> <span style="font-weight: bold; text-decoration: underline;">zone</span> {
    [...]
        <span style="font-weight: bold;">struct</span> free_area        free_area[MAX_ORDER];
</pre>
</div>

<p>
This happens when accessing free_area like this:
</p>
<div class="org-src-container">
<pre class="src src-C">zone-&gt;free_area[order].nr_free--;
</pre>
</div>

<p>
Perf show hot-spot in: rmqueue_bulk.constprop.0 / rmqueue_bulk()
</p>
<pre class="example" id="org260665d">
      │         mov    0x8(%rbx),%rax                                                                                            ▒
      │       __list_del():                                                                                                      ▒
      │         mov    %rax,0x8(%rdx)                                                                                            ▒
      │         mov    %rdx,(%rax)                                                                                               ▒
      │       del_page_from_free_list():                                                                                         ▒
44,54 │1  e2:   imul   $0x58,%rbp,%rbp                                                                                           ▒
      │       expand():                                                                                                          ◆
      │         mov    $0x1,%r9d                                                                                                 ▒
      │         mov    %r13d,%ecx                                                                                                ▒
      │       set_page_private():                                                                                                ▒
      │         movq   $0x0,0x20(%rbx)                                                                                           ▒
      │       __ClearPageBuddy():                                                                                                ▒
      │         orl    $0x80,0x28(%rbx)                                                                                          ▒
      │         lea    -0x1(%r13),%r11d                                                                                          ▒
      │       expand():                                                                                                          ▒
      │         shl    %cl,%r9d                                                                                                  ▒
      │       list_del():                                                                                                        ▒
      │         movabs $0xdead000000000100,%rax                                                                                  ▒
      │         mov    %rax,(%rbx)                                                                                               ▒
      │         add    $0x22,%rax                                                                                                ▒
      │       expand():                                                                                                          ▒
      │         movslq %r9d,%r14                                                                                                 ▒
      │       list_del():                                                                                                        ▒
      │         mov    %rax,0x8(%rbx)                                                                                            ▒
      │       del_page_from_free_list():                                                                                         ▒
      │         subq   $0x1,0x110(%r15,%rbp,1)                                                                                   ▒
      │       expand():                                                                                                          ▒
</pre>
</div>
</div>

<div id="outline-container-Why-happening" class="outline-3">
<h3 id="Why-happening"><a href="#Why-happening">Why happening</a></h3>
<div class="outline-text-3" id="text-Why-happening">
<p>
The size of struct free_area is 88 bytes or 0x58 hex.
</p>

<div class="org-src-container">
<pre class="src src-sh">$ pahole -C free_area mm/page_alloc.o
struct free_area {
        struct list_head           free_list[5];         /*     0    80 */
        /* --- cacheline 1 boundary (64 bytes) was 16 bytes ago --- */
        long unsigned int          nr_free;              /*    80     8 */

        /* size: 88, cachelines: 2, members: 2 */
        /* last cacheline: 24 bytes */
};
</pre>
</div>

<p>
The reason for the code is to find the right struct free_area in struct
zone.  The array of 11 comes from define MAX_ORDER.
</p>

<pre class="example" id="orgb1d17e3">
struct zone {
        long unsigned int          _watermark[3];        /*     0    24 */
 [...]
        /* --- cacheline 3 boundary (192 bytes) --- */
        struct zone_padding        _pad1_ __attribute__((__aligned__(64))); /*   192     0 */
        struct free_area           free_area[11];        /*   192   968 */
        /* --- cacheline 18 boundary (1152 bytes) was 8 bytes ago --- */
        long unsigned int          flags;                /*  1160     8 */
        spinlock_t                 lock;                 /*  1168     4 */

        /* XXX 44 bytes hole, try to pack */

        /* --- cacheline 19 boundary (1216 bytes) --- */
        struct zone_padding        _pad2_ __attribute__((__aligned__(64))); /*  1216     0 */

</pre>

<p>
The size 88 bytes or 0x58 hex. The compiler cannot find a shift
operation to use and instead choose to use a imul to find the offset
into the array free_area[].
</p>

<p>
Asm code to lookout for:
(objdump -Sr mm/page_alloc.o-use-imul)
</p>
<div class="org-src-container">
<pre class="src src-asm">   <span style="font-weight: bold;">zone-&gt;free_area</span>[order].nr_free--<span style="font-weight: bold; font-style: italic;">;</span>
<span style="font-weight: bold;">75ee</span>:       <span style="font-weight: bold;">44</span> 89 f0                mov    <span style="font-weight: bold; font-style: italic;">%r14d</span>,<span style="font-weight: bold; font-style: italic;">%eax</span>
<span style="font-weight: bold;">75f1</span>:       <span style="font-weight: bold;">48</span> 6b c0 58             imul   $0x58,<span style="font-weight: bold; font-style: italic;">%rax</span>,<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">75f5</span>:       <span style="font-weight: bold;">48</span> 03 04 24             add    (<span style="font-weight: bold; font-style: italic;">%rsp</span>),<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">75f9</span>:       <span style="font-weight: bold;">49</span> 83 ac 04 10 01 00    subq   $0x1,0x110(<span style="font-weight: bold; font-style: italic;">%r12</span>,<span style="font-weight: bold; font-style: italic;">%rax</span>,1)
</pre>
</div>

<p>
It looks like it happens 45 times in <code>mm/page_alloc.o</code>:
</p>
<div class="org-src-container">
<pre class="src src-C">$ objdump -Sr mm/page_alloc.o | grep imul | grep <span style="font-weight: bold;">'</span>0x58,<span style="font-weight: bold;">'</span> |wc -l
45
</pre>
</div>

<p>
Code notes for hot-path: The del_page_from_free_list() contains the
zone-&gt;free_area[order].nr_free&#x2013; code, the __rmqueue_smallest was the
hotspot that calls this. This is called by __rmqueue, which is called by
rmqueue_bulk.
</p>
</div>
</div>

<div id="outline-container-Explaining-patch-with-fix" class="outline-3">
<h3 id="Explaining-patch-with-fix"><a href="#Explaining-patch-with-fix">Explaining patch with fix</a></h3>
<div class="outline-text-3" id="text-Explaining-patch-with-fix">
<blockquote>
<p>
mm: make zone-&gt;free_area[order] access faster
</p>

<p>
Avoid multiplication (imul) operations when accessing:
 zone-&gt;free_area[order].nr_free
</p>

<p>
This was really tricky to find. I was puzzled why perf reported that
rmqueue_bulk was using 44% of the time in an imul operation:
</p>

<p>
      │     del_page_from_free_list():
44,54 │ e2:   imul   $0x58,%rax,%rax
</p>

<p>
This operation was generated (by compiler) because the struct free_area
have size 88 bytes or 0x58 hex. The compiler cannot find a shift
operation to use and instead choose to use a imul to find the offset
into the array free_area[].
</p>

<p>
The patch align struct free_area to a cache-line, which cause the
compiler avoid the imul operation. The imul operation is very fast on
modern Intel CPUs. To help fast-path that decrement 'nr_free' move the
member 'nr_free' to be first element, which saves one 'add' operation.
</p>

<p>
Looking up instruction latency this exchange a 3-cycle 'imul' with a
1-cycle 'shl', saving 2-cycles. It does trade some space to do this.
</p>

<p>
Used: gcc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)
</p>
</blockquote>

<p>
Notes about moving members around:
</p>

<p>
Before: Move member 'nr_free':
</p>
<div class="org-src-container">
<pre class="src src-asm"><span style="font-weight: bold;">760e</span>:       <span style="font-weight: bold;">44</span> 89 f0                mov    <span style="font-weight: bold; font-style: italic;">%r14d</span>,<span style="font-weight: bold; font-style: italic;">%eax</span>
<span style="font-weight: bold;">7611</span>:       <span style="font-weight: bold;">48</span> 83 c0 02             add    $0x2,<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">7615</span>:       <span style="font-weight: bold;">48</span> c1 e0 07             shl    $0x7,<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">7619</span>:       <span style="font-weight: bold;">48</span> 03 04 24             add    (<span style="font-weight: bold; font-style: italic;">%rsp</span>),<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">761d</span>:       <span style="font-weight: bold;">49</span> 83 6c 04 10 01       subq   $0x1,0x10(<span style="font-weight: bold; font-style: italic;">%r12</span>,<span style="font-weight: bold; font-style: italic;">%rax</span>,1)
</pre>
</div>

<p>
Move member 'nr_free':
</p>
<div class="org-src-container">
<pre class="src src-asm"><span style="font-weight: bold;">75be</span>:       <span style="font-weight: bold;">44</span> 89 f0                mov    <span style="font-weight: bold; font-style: italic;">%r14d</span>,<span style="font-weight: bold; font-style: italic;">%eax</span>
<span style="font-weight: bold;">75c1</span>:       <span style="font-weight: bold;">48</span> c1 e0 07             shl    $0x7,<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">75c5</span>:       <span style="font-weight: bold;">48</span> 03 04 24             add    (<span style="font-weight: bold; font-style: italic;">%rsp</span>),<span style="font-weight: bold; font-style: italic;">%rax</span>
<span style="font-weight: bold;">75c9</span>:       <span style="font-weight: bold;">49</span> 83 ac 04 c0 00 00    subq   $0x1,0xc0(<span style="font-weight: bold; font-style: italic;">%r12</span>,<span style="font-weight: bold; font-style: italic;">%rax</span>,1)
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2021-09-20 Mon 18:33</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
