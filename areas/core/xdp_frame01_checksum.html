<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>XDP checksum evaluate and extend options</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="/styles/bigblow/css/hideshow.css"/>
<script type="text/javascript" src="/styles/bigblow/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/bigblow.js"></script>
<script type="text/javascript" src="/styles/bigblow/js/hideshow.js"></script>
<script type="text/javascript" src="/styles/lib/js/jquery.stickytableheaders.min.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">XDP checksum evaluate and extend options</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Autogenerated--Table-of-contents">Autogenerated: Table of contents&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TOC">TOC</span></span></a></li>
<li><a href="#Background-knowledge--Packet-checksums">Background knowledge: Packet checksums</a>
<ul>
<li><a href="#More-checksums-in-one-packet">More checksums in one packet</a></li>
</ul>
</li>
<li><a href="#Overview-of-issues-and-use-cases">Overview of issues and use-cases</a>
<ul>
<li><a href="#Issue-use-case--xdp_frame-to-SKB">Issue/use-case: xdp_frame to SKB</a></li>
<li><a href="#Issue-use-case--xdp_frame-to-Guest-OS">Issue/use-case: xdp_frame to Guest-OS</a></li>
<li><a href="#Use-case--DDoS-mitigation">Use-case: DDoS mitigation</a></li>
</ul>
</li>
<li><a href="#Measurements--Assess-overhead">Measurements: Assess overhead</a>
<ul>
<li><a href="#UDP-checksum-overhead--UdpNoPorts-drop-case">UDP checksum overhead: UdpNoPorts drop case</a>
<ul>
<li><a href="#perf-diff-results-UDP-checksum">perf diff results UDP checksum</a></li>
<li><a href="#perf-diff-result-against-netstack">perf diff result against netstack</a></li>
</ul>
</li>
<li><a href="#UDP-checksum-overhead--socket-delivery">UDP checksum overhead: socket delivery</a>
<ul>
<li><a href="#Check-code-path-with-perf">Check code path with perf</a></li>
<li><a href="#Measurement--Trafgen-UDP-checksum">Measurement: Trafgen UDP checksum</a></li>
<li><a href="#Measurement--Trafgen-UDP-checksum-zero">Measurement: Trafgen UDP checksum zero</a></li>
<li><a href="#Measurements--Compare-results---conclusion">Measurements: Compare results + conclusion</a></li>
<li><a href="#Measurements--perf-diff">Measurements: perf diff</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<p>
XDP need to be extended to get info on the packet checksum from NIC
hardware. This document evaluate and test different options.
</p>

<div id="outline-container-Autogenerated--Table-of-contents" class="outline-2">
<h2 id="Autogenerated--Table-of-contents"><a href="#Autogenerated--Table-of-contents">Autogenerated: Table of contents&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TOC">TOC</span></span></a></h2>
<div class="outline-text-2" id="text-Autogenerated--Table-of-contents">
<ul class="org-ul">
<li><ul class="org-ul">
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
<li></li>
<li></li>
</ul></li>
<li><ul class="org-ul">
<li></li>
<li></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-Background-knowledge--Packet-checksums" class="outline-2">
<h2 id="Background-knowledge--Packet-checksums"><a href="#Background-knowledge--Packet-checksums">Background knowledge: Packet checksums</a></h2>
<div class="outline-text-2" id="text-Background-knowledge--Packet-checksums">
<p>
Most people can <b>skip this section</b>. This is a written as reminder on what
the packet checksum means and some details to refresh/remember.
</p>

<p>
Most Internet protocols use the same checksum algorithm. Called the 16-bit
one's complement sum. This include IP, ICMP, IGMP, UDP and TCP, although UDP
and TCP include fields from the IP header (a pseudo header, more detail
below). (As counter example SCTP uses a 32-bit checksum).
</p>
</div>

<div id="outline-container-More-checksums-in-one-packet" class="outline-3">
<h3 id="More-checksums-in-one-packet"><a href="#More-checksums-in-one-packet">More checksums in one packet</a></h3>
<div class="outline-text-3" id="text-More-checksums-in-one-packet">
<p>
Remember that packets usually contain several checksums for the different
layers (or encapsulations). For example IP/UDP and IP/TCP packets contains
two checksums.
</p>

<p>
(We ignore, the Ethernet FCS (frame check sequence), which a CRC
32-bit/4Bytes place at the end of the payload, because XDP at driver L2
frames with bad FCS will not be seen).
</p>

<p>
The IP-header contains a header (L3) checksum, that is calculated over the
IP header only.
</p>

<p>
Both UDP and TCP have (16-bit ones complement) checksums that cover their
headers and their data. Remember that this L4 checksum is stored at
different offsets in the L4-header.
</p>

<p>
Pitfall/detail: UDP/TCP length (in bytes) can be an odd number, but 16-bit
checksum imply 2 bytes (16-bit) to calculate over. The trick is to pad/add a
byte with zero.
</p>

<p>
<b>Remember</b>: Both UDP and TCP include a 12-bytes (for IPv4) pseudo header.
</p>

<p>
Quote <a href="https://tools.ietf.org/html/rfc768">RFC-768</a>:
</p>
<blockquote>
<p>
The pseudo  header  conceptually prefixed to the UDP header contains the
source  address,  the destination  address,  the protocol,  and the  UDP
length.   This information gives protection against misrouted datagrams.
This checksum procedure is the same as is used in TCP.
</p>
</blockquote>

<p>
The pseudo header:
</p>
<pre class="example" id="org45416d2">
 0      7 8     15 16    23 24    31
+--------+--------+--------+--------+
|         source address            |
+--------+--------+--------+--------+
|        destination address        |
+--------+--------+--------+--------+
|  zero  |protocol|   UDP length    |
+--------+--------+--------+--------+
</pre>
</div>
</div>
</div>


<div id="outline-container-Overview-of-issues-and-use-cases" class="outline-2">
<h2 id="Overview-of-issues-and-use-cases"><a href="#Overview-of-issues-and-use-cases">Overview of issues and use-cases</a></h2>
<div class="outline-text-2" id="text-Overview-of-issues-and-use-cases">
</div>
<div id="outline-container-Issue-use-case--xdp_frame-to-SKB" class="outline-3">
<h3 id="Issue-use-case--xdp_frame-to-SKB"><a href="#Issue-use-case--xdp_frame-to-SKB">Issue/use-case: xdp_frame to SKB</a></h3>
<div class="outline-text-3" id="text-Issue-use-case--xdp_frame-to-SKB">
<p>
When creating an SKB (<code>struct sk_buff</code>) based on <code>xdp_frame</code>, then some
optional fields (originating from hardware offload-hints) are lacking.
One of these are the <b>checksum</b> "indication".
</p>

<p>
This result in creating an SKB with "CHECKSUM_NONE" (<code>skb-&gt;summed</code>).
</p>

<p>
This issue is (obviously) that network stack have to calculate and check the
packet checksum in software.
</p>

<p>
Creating SKBs this way happens when XDP-redirecting frames into CPUMAP and
<code>veth</code> devices (container use-case).
</p>
</div>
</div>

<div id="outline-container-Issue-use-case--xdp_frame-to-Guest-OS" class="outline-3">
<h3 id="Issue-use-case--xdp_frame-to-Guest-OS"><a href="#Issue-use-case--xdp_frame-to-Guest-OS">Issue/use-case: xdp_frame to Guest-OS</a></h3>
<div class="outline-text-3" id="text-Issue-use-case--xdp_frame-to-Guest-OS">
<p>
When XDP redirecting into a Guest-OS the same issue occurs, as the xdp_frame
don't carry <b>checksum</b> "indication" from HW.
</p>
</div>
</div>

<div id="outline-container-Use-case--DDoS-mitigation" class="outline-3">
<h3 id="Use-case--DDoS-mitigation"><a href="#Use-case--DDoS-mitigation">Use-case: DDoS mitigation</a></h3>
<div class="outline-text-3" id="text-Use-case--DDoS-mitigation">
<p>
When NIC HW detects a L3 or L4 checksum issue, the packet is (usually) still
allowed to continue into other layers of the network stack.
</p>

<p>
If the XDP-prog had access to the HW checksum "indication", then it could
drop packets earlier, as part of the DDoS mitigation step.
</p>

<p>
The <b>challenge here</b> is adding this feature <b>MUST NOT</b> cause a <b>slowdown</b>
for the baseline XDP (DROP) use-cases that don't use/want this feature.
</p>
</div>
</div>
</div>

<div id="outline-container-Measurements--Assess-overhead" class="outline-2">
<h2 id="Measurements--Assess-overhead"><a href="#Measurements--Assess-overhead">Measurements: Assess overhead</a></h2>
<div class="outline-text-2" id="text-Measurements--Assess-overhead">
<p>
Quantify: What is the overhead of this missing HW checksum indication?
</p>

<p>
Answer: Based on tests below for 1500 bytes packets:
</p>
<ul class="org-ul">
<li>UdpNoPorts-test isolate checksum overhead cost to 91.79 ns</li>
<li>Combining csum-with-copy gives overhead cost of 54.28 ns</li>
<li>Possible explanation: UdpNoPorts-test don't read packet payload</li>
</ul>
</div>

<div id="outline-container-UDP-checksum-overhead--UdpNoPorts-drop-case" class="outline-3">
<h3 id="UDP-checksum-overhead--UdpNoPorts-drop-case"><a href="#UDP-checksum-overhead--UdpNoPorts-drop-case">UDP checksum overhead: UdpNoPorts drop case</a></h3>
<div class="outline-text-3" id="text-UDP-checksum-overhead--UdpNoPorts-drop-case">
<p>
UDP packets can opt-out of checksumming by setting the checksum filed to
zero, which the kernel <b>pktgen</b> tool does.
</p>

<p>
To solve that, here is a <code>trafgen</code> based config that generated large UDP
packets with a correct UDP checksum:
</p>
<ul class="org-ul">
<li><a href="https://github.com/netoptimizer/network-testing/blob/master/trafgen/udp_example03_checksum.trafgen">https://github.com/netoptimizer/network-testing/blob/master/trafgen/udp_example03_checksum.trafgen</a></li>
</ul>

<p>
To keep the kernel code path as short as possible, I'm sending to correct
IP+MAC but there is no listen UDP socket, thus packets are getting dropped
with nstat counter UdpNoPorts.
</p>

<p>
<b>UPDATE</b>: The performance slowdown is not only due checksum missing, it is
also caused by the test-case that cause <code>page_pool</code> in mlx5 to run dry, and
do page allocations. Add result with trafgen use zero as UDP checksum
("zero-csum").
</p>

<p>
Performance results:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Description of test</th>
<th scope="col" class="org-left">nstat UdpNoPorts: pps</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Normal netstack handling</td>
<td class="org-left">UdpNoPorts: 3,343,400 pps</td>
</tr>

<tr>
<td class="org-left">XDP-redirect into veth with csum</td>
<td class="org-left">UdpNoPorts: 2,178,586 pps</td>
</tr>

<tr>
<td class="org-left">XDP-redirect into veth <b>zero-csum</b></td>
<td class="org-left">UdpNoPorts: 2,723,200 pps</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
Calculate <b>overhead</b> (1500 Byte packet): 2,178,586 pps vs 2,723,200 pps
</p>
<ul class="org-ul">
<li>+544,614 pps = (2723200 - 2178586)</li>
<li>+91.79 ns slower = ((1/2178586-1/2723200)*10^9)</li>
<li>25% increase in pps if avoid checksum calc = ((2723200/2178586)-1)*100</li>
</ul>

<p>
The <b>test setup</b> with XDP-redirect into veth (that does XDP_PASS) is the
same as described in <a href="../mem/page_pool06_alloc_pages_bulk.html">../mem/page_pool06_alloc_pages_bulk.html</a>.
Side-note: Kernel used was the devel version with some page allocator
optimizations (also described in linked file).
</p>
</div>

<div id="outline-container-perf-diff-results-UDP-checksum" class="outline-4">
<h4 id="perf-diff-results-UDP-checksum"><a href="#perf-diff-results-UDP-checksum">perf diff results UDP checksum</a></h4>
<div class="outline-text-4" id="text-perf-diff-results-UDP-checksum">
<p>
The perf diff result (10 sec measurement) with csum vs. zero-csum:
</p>

<pre class="example" id="orgaf73df2">
# Event 'cycles'
#
# Baseline  Delta Abs  Shared Object                                      Symbol                              &gt;
# ........  .........  .................................................  ....................................&gt;
#
              +14.72%  [kernel.vmlinux]                                   [k] do_csum
     3.46%     -0.98%  [kernel.vmlinux]                                   [k] __udp4_lib_lookup
               +0.97%  [kernel.vmlinux]                                   [k] __skb_checksum_complete
     2.43%     -0.84%  [kernel.vmlinux]                                   [k] __xdp_release_frame
               +0.67%  [kernel.vmlinux]                                   [k] __skb_checksum
     2.87%     -0.65%  [kernel.vmlinux]                                   [k] ip_rcv_core.isra.0
     3.37%     -0.56%  [kernel.vmlinux]                                   [k] __netif_receive_skb_core
     1.94%     -0.50%  [kernel.vmlinux]                                   [k] dev_gro_receive
     2.67%     -0.49%  [kernel.vmlinux]                                   [k] __list_del_entry_valid
     2.09%     -0.47%  [kernel.vmlinux]                                   [k] __list_add_valid
     2.44%     -0.47%  bpf_prog_943df0a1ce7ea5c2_xdp_prognum0             [k] bpf_prog_943df0a1ce7ea5c2_xdp_pr&gt;
     1.72%     -0.43%  [kernel.vmlinux]                                   [k] __icmp_send
     4.01%     -0.43%  [veth]                                             [k] 0x0000000000004c05
     2.85%     -0.40%  [kernel.vmlinux]                                   [k] kmem_cache_free
     1.72%     -0.39%  [kernel.vmlinux]                                   [k] bpf_xdp_redirect_map
     1.37%     -0.37%  [kernel.vmlinux]                                   [k] nf_hook_slow
     0.90%     -0.34%  [kernel.vmlinux]                                   [k] free_pcp_prepare
     2.01%     -0.33%  [kernel.vmlinux]                                   [k] nf_hook_slow_list
     1.17%     -0.33%  [kernel.vmlinux]                                   [k] fib_validate_source
     1.95%     -0.32%  [kernel.vmlinux]                                   [k] free_unref_page_commit
     1.49%     -0.28%  [kernel.vmlinux]                                   [k] ip_route_use_hint
     1.38%     -0.28%  [kernel.vmlinux]                                   [k] netif_receive_skb_list_internal
     0.49%     -0.27%  [kernel.vmlinux]                                   [k] skb_release_head_state
               +0.26%  [kernel.vmlinux]                                   [k] csum_partial
     0.97%     -0.26%  [kernel.vmlinux]                                   [k] ip_list_rcv
     0.69%     -0.24%  [kernel.vmlinux]                                   [k] gro_normal_one
     1.11%     -0.24%  [kernel.vmlinux]                                   [k] ip_protocol_deliver_rcu
     1.47%     -0.24%  [kernel.vmlinux]                                   [k] __slab_free
     0.88%     -0.23%  [kernel.vmlinux]                                   [k] free_unref_page
     1.39%     -0.23%  [mlx5_core]                                        [k] mlx5e_port_ptp_open
     0.47%     -0.22%  [kernel.vmlinux]                                   [k] kfree_skb
     1.22%     -0.22%  [kernel.vmlinux]                                   [k] ip_sublist_rcv
     3.10%     -0.21%  [kernel.vmlinux]                                   [k] __udp4_lib_rcv
     0.64%     -0.21%  [kernel.vmlinux]                                   [k] page_frag_free
     1.39%     -0.21%  [kernel.vmlinux]                                   [k] napi_gro_receive
     1.35%     -0.20%  [kernel.vmlinux]                                   [k] xdp_do_redirect
     1.12%     -0.20%  [kernel.vmlinux]                                   [k] free_unref_page_prepare.part.0
     1.58%     -0.20%  bpf_prog_a55118bafe28d557_xdp_redirect_map_native  [k] bpf_prog_a55118bafe28d557_xdp_re&gt;
</pre>
</div>
</div>

<div id="outline-container-perf-diff-result-against-netstack" class="outline-4">
<h4 id="perf-diff-result-against-netstack"><a href="#perf-diff-result-against-netstack">perf diff result against netstack</a></h4>
<div class="outline-text-4" id="text-perf-diff-result-against-netstack">
<p>
The perf diff result against netstack (10 sec measurement) below:
</p>

<pre class="example" id="org7d1a511">
# Event 'cycles'
#
# Baseline  Delta Abs  Shared Object                                      Symbol                                               
# ........  .........  .................................................  .....................................................
#
              +11.93%  [kernel.vmlinux]                                   [k] do_csum
               +3.40%  [veth]                                             [k] 0x0000000000004c00
     4.16%     -2.54%  [mlx5_core]                                        [k] mlx5e_fec_admin_field
     4.99%     -2.41%  [kernel.vmlinux]                                   [k] kmem_cache_free
               +1.92%  [kernel.vmlinux]                                   [k] memset_erms
               +1.88%  bpf_prog_943df0a1ce7ea5c2_xdp_prognum0             [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0
     3.28%     -1.85%  [kernel.vmlinux]                                   [k] dev_gro_receive
     1.85%     -1.85%  [kernel.vmlinux]                                   [k] kmem_cache_alloc
               +1.80%  [kernel.vmlinux]                                   [k] __xdp_build_skb_from_frame
     4.47%     -1.72%  [kernel.vmlinux]                                   [k] __netif_receive_skb_core
     5.08%     -1.64%  [kernel.vmlinux]                                   [k] __udp4_lib_rcv
               +1.62%  [kernel.vmlinux]                                   [k] free_unref_page_commit
               +1.47%  [kernel.vmlinux]                                   [k] __xdp_release_frame
     4.18%     -1.42%  [kernel.vmlinux]                                   [k] __udp4_lib_lookup
               +1.35%  [kernel.vmlinux]                                   [k] bpf_xdp_redirect_map
               +1.32%  bpf_prog_a55118bafe28d557_xdp_redirect_map_native  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_native
     3.56%     -1.28%  [kernel.vmlinux]                                   [k] ip_rcv_core.isra.0
               +1.20%  [kernel.vmlinux]                                   [k] __alloc_pages_bulk
               +1.16%  [kernel.vmlinux]                                   [k] dev_map_enqueue
     1.74%     -1.15%  [mlx5_core]                                        [k] mlx5e_tx_reporter_dump
               +1.08%  [kernel.vmlinux]                                   [k] kmem_cache_alloc_bulk
               +1.06%  [kernel.vmlinux]                                   [k] free_unref_page_prepare.part.0
     0.07%     +1.03%  [kernel.vmlinux]                                   [k] __slab_free
               +1.02%  [kernel.vmlinux]                                   [k] xdp_do_redirect
               +0.99%  [kernel.vmlinux]                                   [k] dma_map_page_attrs
               +0.97%  [kernel.vmlinux]                                   [k] __skb_checksum_complete
     2.67%     -0.96%  [kernel.vmlinux]                                   [k] nf_hook_slow_list
     2.25%     -0.87%  [kernel.vmlinux]                                   [k] __icmp_send
     1.50%     -0.80%  [mlx5_core]                                        [k] mlx5e_tx_reporter_build_diagnose_output_sq_common
               +0.78%  [kernel.vmlinux]                                   [k] build_skb_around
     0.42%     +0.77%  [kernel.vmlinux]                                   [k] eth_type_trans
     1.27%     -0.75%  [kernel.vmlinux]                                   [k] __build_skb_around
               +0.75%  [kernel.vmlinux]                                   [k] free_unref_page
               +0.74%  [kernel.vmlinux]                                   [k] __rmqueue_pcplist
     2.02%     -0.71%  [kernel.vmlinux]                                   [k] ip_rcv_finish_core.isra.0
     2.04%     -0.70%  [kernel.vmlinux]                                   [k] ip_route_use_hint
     0.62%     +0.68%  [mlx5_core]                                        [k] mlx5e_port_ptp_open
               +0.68%  [kernel.vmlinux]                                   [k] __skb_checksum
               +0.68%  [kernel.vmlinux]                                   [k] __page_pool_alloc_pages_slow
     1.80%     -0.66%  [kernel.vmlinux]                                   [k] netif_receive_skb_list_internal
</pre>

<p>
So, I now have a test that shows the problem. It is very clear that +11.93%
[k] do_csum function is taking too much time.
</p>

<p>
Example call-stack for <code>do_csum</code> :
</p>
<pre class="example" id="orgfaa54eb">
ksoftirqd/2    24 [002] 68022.158164:    1133633   cycles: 
        ffffffff81506047 do_csum+0x77 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8150614d csum_partial+0xd (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8178d5fa __skb_checksum+0x6a (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8178dd91 __skb_checksum_complete+0x31 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81873d24 __udp4_lib_rcv+0xb84 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff818361e5 ip_protocol_deliver_rcu+0xc5 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81836325 ip_local_deliver_finish+0x55 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8183639e ip_local_deliver+0x5e (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8183657c ip_sublist_rcv_finish+0x7c (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81836719 ip_sublist_rcv+0x189 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff818369aa ip_list_rcv+0x12a (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817aabb2 __netif_receive_skb_list_core+0x292 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817aad91 netif_receive_skb_list_internal+0x1c1 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817aaf99 gro_normal_list.part.0+0x19 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817ab901 napi_gro_receive+0x61 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffffa0031798 veth_xdp_rcv_skb+0x558 (/lib/modules/5.12.0-rc2-mel-git-alloc_pages_bulk+/kernel/drivers/net/veth.ko)
</pre>

<p>
Decode <code>__udp4_lib_rcv+0xb84</code>:
</p>
<pre class="example" id="org35e4533">
$ ./scripts/faddr2line vmlinux __udp4_lib_rcv+0xb84
__udp4_lib_rcv+0xb84/0xb90:
__udp_lib_checksum_complete at include/net/udp.h:112
(inlined by) udp_lib_checksum_complete at include/net/udp.h:119
(inlined by) udp_lib_checksum_complete at include/net/udp.h:116
(inlined by) __udp4_lib_rcv at net/ipv4/udp.c:2408
</pre>
</div>
</div>
</div>

<div id="outline-container-UDP-checksum-overhead--socket-delivery" class="outline-3">
<h3 id="UDP-checksum-overhead--socket-delivery"><a href="#UDP-checksum-overhead--socket-delivery">UDP checksum overhead: socket delivery</a></h3>
<div class="outline-text-3" id="text-UDP-checksum-overhead--socket-delivery">
<p>
As DaveM have <a href="https://netdevconf.info/1.1/proceedings/slides/miller-hardware-checksumming.pdf">pointed</a> out before,
when the checksum check is done combined with a packet copy, then the
overhead is much less.
</p>

<p>
We can test this fairly easily with our UDP trafgen test, because UDP allows
us to disable checksumming via providing zero as checksum value. Plus we can
run a UDP sink program in userspace that consumes the packets (which cause a
copy into userspace via UDP socket).
</p>
</div>

<div id="outline-container-Check-code-path-with-perf" class="outline-4">
<h4 id="Check-code-path-with-perf"><a href="#Check-code-path-with-perf">Check code path with perf</a></h4>
<div class="outline-text-4" id="text-Check-code-path-with-perf">
<p>
Step one: Check code path do combine copy and checksum.
</p>

<p>
In perf top we clearly see <code>csum_partial_copy_generic</code> which should confirm
this is happening. (Included CPU column below to show RX happens on CPU-3
and udp_sink program runs on CPU-4).
</p>

<pre class="example" id="org40cfcaf">
# Overhead  CPU  Symbol
# ........  ...  .....................................................
#
    10.96%  004  [k] syscall_exit_to_user_mode                        
     7.76%  004  [k] csum_partial_copy_generic                        
     5.14%  004  [k] entry_SYSCALL_64                                 
     4.35%  004  [k] syscall_return_via_sysret                        
     3.81%  003  [k] memset_erms                                      
     2.62%  004  [k] page_frag_free                                   
     1.85%  003  [k] __list_del_entry_valid                           
     1.57%  004  [k] free_pcppages_bulk                               
     1.41%  003  [k] kmem_cache_alloc_bulk                            
     1.38%  003  [k] __udp_enqueue_schedule_skb                       
     1.27%  003  [k] rmqueue_bulk.constprop.0                         
     1.26%  004  [k] skb_copy_and_csum_datagram_msg                   
     1.22%  003  [k] __netif_receive_skb_core                         
     1.17%  003  [k] __udp4_lib_lookup                                
     1.12%  004  [k] kmem_cache_free                                  
     1.10%  004  [k] csum_and_copy_to_iter                            
     1.09%  003  [k] __udp4_lib_rcv                                   
     1.04%  003  [k] ip_rcv_core.isra.0                               
     1.01%  003  [k] udp4_lib_lookup2                                 
     0.99%  003  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           
     0.97%  003  [k] udp_queue_rcv_one_skb                            
     0.96%  004  [k] __slab_free                                      
     0.91%  003  [k] __xdp_release_frame                              
     0.86%  003  [k] sock_def_readable                                
     0.86%  003  [k] dev_gro_receive                                  
     0.85%  003  [k] __xdp_build_skb_from_frame                       
     0.84%  004  [k] free_unref_page_commit                           
     0.79%  003  [k] __list_add_valid                                 
     0.77%  004  [k] udp_rmem_release                                 
     0.77%  003  [k] __cgroup_bpf_run_filter_skb                      
     0.77%  004  [.] __libc_recv                                      
     0.75%  004  [k] __skb_datagram_iter                              
     0.72%  004  [k] udp_recvmsg                                      
     0.69%  003  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_native
     0.68%  003  [k] nf_hook_slow_list                                
     0.67%  003  [k] build_skb_around                                 
</pre>

<p>
Perf call stack for: csum_partial_copy_generic
</p>
<pre class="example" id="org516cb16">
udp_sink 90008 [004] 161320.394594:    1218198   cycles:
        ffffffff81505e43 csum_partial_copy_generic+0x53 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81506243 csum_and_copy_to_user+0x43 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff814b8f21 csum_and_copy_to_iter+0xb1 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81796287 __skb_datagram_iter+0x2b7 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81796585 skb_copy_and_csum_datagram_msg+0x85 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81871680 udp_recvmsg+0x240 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8187fbe2 inet_recvmsg+0xf2 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817848b6 __sys_recvfrom+0x166 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817848f5 __x64_sys_recvfrom+0x25 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff819a4c83 do_syscall_64+0x33 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81a0007c entry_SYSCALL_64+0x7c (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
            7faf7b340960 __libc_recv+0x20 (/usr/lib64/libc-2.30.so)
</pre>
</div>
</div>

<div id="outline-container-Measurement--Trafgen-UDP-checksum" class="outline-4">
<h4 id="Measurement--Trafgen-UDP-checksum"><a href="#Measurement--Trafgen-UDP-checksum">Measurement: Trafgen UDP checksum</a></h4>
<div class="outline-text-4" id="text-Measurement--Trafgen-UDP-checksum">
<p>
The cmdline for <a href="https://github.com/netoptimizer/network-testing/blob/master/src/udp_sink.c">udp_sink</a>:
</p>
<div class="org-src-container">
<pre class="src src-sh">$ taskset -c 4 ./udp_sink -l6666 --repeat 10000 --recv
</pre>
</div>

<p>
Output:
</p>
<pre class="example" id="org28005fd">
$ sudo taskset -c 4 ./udp_sink -l6666 --repeat 10000 --recv
          	run      count   	ns/pkt	pps		cycles	payload
recv      	run:  0	 1000000	735.43	1359742.86	2647	1472	 demux:1
recv      	run:  1	 1000000	728.99	1371768.87	2624	1472	 demux:1
recv      	run:  2	 1000000	730.43	1369047.89	2629	1472	 demux:1
</pre>
</div>
</div>

<div id="outline-container-Measurement--Trafgen-UDP-checksum-zero" class="outline-4">
<h4 id="Measurement--Trafgen-UDP-checksum-zero"><a href="#Measurement--Trafgen-UDP-checksum-zero">Measurement: Trafgen UDP checksum zero</a></h4>
<div class="outline-text-4" id="text-Measurement--Trafgen-UDP-checksum-zero">
<p>
Change trafgen conf to use UDP checksum zero:
</p>
<pre class="example" id="orgae1c7d0">
recv      	run: 339	 1000000	679.13	1472481.98	2444	1472	 demux:1
recv      	run: 340	 1000000	676.17	1478920.63	2434	1472	 demux:1
recv      	run: 341	 1000000	676.15	1478969.08	2434	1472	 demux:1
</pre>
</div>
</div>

<div id="outline-container-Measurements--Compare-results---conclusion" class="outline-4">
<h4 id="Measurements--Compare-results---conclusion"><a href="#Measurements--Compare-results---conclusion">Measurements: Compare results + conclusion</a></h4>
<div class="outline-text-4" id="text-Measurements--Compare-results---conclusion">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">UDP socket delivery</th>
<th scope="col" class="org-left">ns/pkt</th>
<th scope="col" class="org-right">pps</th>
<th scope="col" class="org-left">pps readable</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">checksum</td>
<td class="org-left">730.43 ns</td>
<td class="org-right">1369047.89</td>
<td class="org-left">1,369,048 pps</td>
</tr>

<tr>
<td class="org-left">no-checksum</td>
<td class="org-left">676.15 ns</td>
<td class="org-right">1478969.08</td>
<td class="org-left">1,478,969 pps</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">diff</td>
<td class="org-left">54.28 ns</td>
<td class="org-right">-109921.19</td>
<td class="org-left">-109,921 pps</td>
</tr>
</tbody>
</table>

<p>
Performance increase with 8% if we can avoid the checksumming step
(((1478969.08/1369047.89)-1)*100).
</p>

<p>
The UdpNoPorts had +91.79 ns slower compared to 54.28 ns.  Thus, we can
conclude that it is there are less overhead when combining copy and
checksumming, but the overhead is still significant.
</p>
</div>
</div>

<div id="outline-container-Measurements--perf-diff" class="outline-4">
<h4 id="Measurements--perf-diff"><a href="#Measurements--perf-diff">Measurements: perf diff</a></h4>
<div class="outline-text-4" id="text-Measurements--perf-diff">
<pre class="example" id="org86f9510">
# Event 'cycles'
#
# Baseline  Delta Abs    Symbol
# ........  .........    .....................................................
#
               +7.73%    [k] csum_partial_copy_generic
     5.69%     -5.68%    [k] copy_user_enhanced_fast_string
     2.00%     -1.27%    [k] udp_recvmsg
               +1.13%    [k] csum_and_copy_to_iter
               +1.10%    [k] skb_copy_and_csum_datagram_msg
    12.00%     -0.98%    [k] syscall_exit_to_user_mode
     0.97%     -0.97%    [k] _copy_to_iter
     1.53%     -0.79%    [k] skb_release_data
               +0.73%    [k] __skb_datagram_iter
               +0.59%    [k] do_csum
     2.41%     +0.56%    [k] page_frag_free
     5.54%     -0.46%    [k] entry_SYSCALL_64
     4.68%     -0.37%    [k] syscall_return_via_sysret
               +0.35%    [k] csum_and_copy_to_user
     4.26%     -0.35%    [k] memset_erms
     1.35%     -0.19%    [k] rmqueue_bulk.constprop.0
     1.29%     -0.19%    [k] __udp4_lib_lookup
               +0.16%    [k] csum_partial
     1.74%     -0.16%    [k] free_pcppages_bulk
     1.69%     -0.15%    [k] kmem_cache_alloc_bulk
     1.03%     +0.10%    [k] __udp4_lib_rcv
     1.20%     -0.10%    [k] __slab_free
     0.13%     +0.09%    [k] __x86_retpoline_rax
     0.71%     -0.09%    [k] eth_type_trans
     0.90%     +0.08%    [k] udp_queue_rcv_one_skb
     1.02%     -0.08%    [k] ip_rcv_core.isra.0
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2021-03-17 Wed 17:27</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
