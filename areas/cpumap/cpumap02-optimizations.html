<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>CPUMAP optimizations patchset-V1</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" type="text/css" href="/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/styles/readtheorg/css/readtheorg.css"/>
<script type="text/javascript" src="/styles/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="/styles/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="/styles/readtheorg/js/readtheorg.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">CPUMAP optimizations patchset-V1</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#WARNING%3A%20F27%20issue">WARNING: F27 issue</a></li>
<li><a href="#Testlab%20machine">Testlab machine</a></li>
<li><a href="#patch%20descriptions%20%2F%20notes">patch descriptions / notes</a>
<ul>
<li><a href="#Cover%20letter">Cover letter</a>
<ul>
<li><a href="#stg%20mail%20V1">stg mail V1</a></li>
</ul>
</li>
<li><a href="#baseline">baseline</a>
<ul>
<li><a href="#NIC%3A%20i40e1%20-%20Kernel%3A%205.1.0-rc2-bpf-next-cpumap-baseline%2B">NIC: i40e1 - Kernel: 5.1.0-rc2-bpf-next-cpumap-baseline+</a></li>
<li><a href="#NIC%3A%20i40e1%20-%20UDP%20socket%20baseline">NIC: i40e1 - UDP socket baseline</a></li>
<li><a href="#NIC%3A%20i40e1%20-%20baseline%20cpumap%20redirect">NIC: i40e1 - baseline cpumap redirect</a></li>
</ul>
</li>
<li><a href="#Patch%3A%20bpf%3A%20cpumap%20use%20ptr_ring_consume_batched">Patch: bpf: cpumap use ptr_ring_consume_batched</a>
<ul>
<li><a href="#benchmarks%20on%20this%20patch">benchmarks on this patch</a></li>
</ul>
</li>
<li><a href="#Patch%3A%20bpf%3A%20cpumap%20send%20a%20SKB-list%20towards%20network%20stack.">Patch: bpf: cpumap send a SKB-list towards network stack.</a>
<ul>
<li><a href="#benchmark01%20on%20this%20patch">benchmark01 on this patch</a></li>
<li><a href="#benchmark02%20on%20this%20patch">benchmark02 on this patch</a></li>
<li><a href="#benchmark03%20more%20reorg">benchmark03 more reorg</a></li>
</ul>
</li>
<li><a href="#Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around">Patch: net: core: introduce build_skb_around</a>
<ul>
<li><a href="#Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around--benchmarks%20on%20this%20patch">benchmarks on this patch</a></li>
</ul>
</li>
<li><a href="#Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs">Patch: bpf: cpumap do bulk allocation of SKBs</a>
<ul>
<li><a href="#Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs--benchmarks%20on%20this%20patch">benchmarks on this patch</a></li>
</ul>
</li>
<li><a href="#Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page">Patch: bpf: cpumap memory prefetchw optimizations for struct page</a>
<ul>
<li><a href="#Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page--benchmarks%20on%20this%20patch">benchmarks on this patch</a></li>
</ul>
</li>
<li><a href="#test%20reorg">test reorg</a>
<ul>
<li><a href="#benchmarks%20on%20experimental%20patch">benchmarks on experimental patch</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#notes">notes</a></li>
<li><a href="#Evaluating%20effect%20of%20page-prefetchw">Evaluating effect of page-prefetchw</a>
<ul>
<li><a href="#page-prefetchw%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop">page-prefetchw + i40e + batch-16 + iptables-raw-drop</a></li>
<li><a href="#page-prefetch%20%28non-W%29%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop">page-prefetch (non-W) + i40e + batch-16 + iptables-raw-drop</a></li>
</ul>
</li>
<li><a href="#Eval%20prefetch%20of%20xdp_frame%20area">Eval prefetch of xdp_frame area</a>
<ul>
<li><a href="#prefetchw%20xdp_frame">prefetchw xdp_frame</a></li>
<li><a href="#remove%20any%20prefetch%20of%20xdp_frame">remove any prefetch of xdp_frame</a></li>
<li><a href="#test%20reduce%20CPUMAP_BATCH%20to%208">test reduce CPUMAP_BATCH to 8</a></li>
<li><a href="#Test%3A%20prefetchw%20single%20%2B%20i%2B1">Test: prefetchw single + i+1</a></li>
<li><a href="#Test%3A%20Remove%20all%20prefetches">Test: Remove all prefetches</a></li>
</ul>
</li>
<li><a href="#Hack%20use%20Felix%20kfree_skb_list%20bulk">Hack use Felix kfree_skb_list bulk</a>
<ul>
<li><a href="#test%3A%20remove%20kmem_cache_free_bulk">test: remove kmem_cache_free_bulk</a></li>
</ul>
</li>
<li><a href="#--notes">notes</a>
<ul>
<li><a href="#Experiments">Experiments</a></li>
<li><a href="#Experiment%3A%20cut-out%20netfilter-code-path">Experiment: cut-out netfilter-code-path</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
Next benchmark iteration in <a href="cpumap03-optimizations.html">cpumap03-optimizations.html</a>.
</p>

<div id="outline-container-WARNING%3A%20F27%20issue" class="outline-2">
<h2 id="WARNING%3A%20F27%20issue"><a href="#WARNING%3A%20F27%20issue">WARNING: F27 issue</a></h2>
<div class="outline-text-2" id="text-WARNING%3A%20F27%20issue">
<p>
A lot of the test-results have to be redone, because Fedora release 27 (F27)
have been loading iptables-filter behind my back&#x2026;
</p>

<p>
When testing I've been unloading all the iptables/netfilter modules, as they
have a fairly high base-cost, even when no rules are present.
</p>

<p>
I found that F27 periodically or on some event (I have yet to figure out)
will reload the modules ip6table_filter and iptable_filter (and
ip_tables+x_tables) which activate the iptables filter hooks. (Strangely
there is no rules inserted, the tables are empty).
</p>

<p>
This unfortunately invalidates a lot of the test results, as I don't know if
the result recored with or without iptables filter loaded.
</p>
</div>
</div>

<div id="outline-container-Testlab%20machine" class="outline-2">
<h2 id="Testlab%20machine"><a href="#Testlab%20machine">Testlab machine</a></h2>
<div class="outline-text-2" id="text-Testlab%20machine">
<p>
The testlab machine:
</p>
<ul class="org-ul">
<li>Intel CPU E5-1650 v4 @ 3.60GHz</li>
<li>Disabled HT (HyperThreading)</li>
<li>Fedora 27</li>
</ul>
</div>
</div>

<div id="outline-container-patch%20descriptions%20%2F%20notes" class="outline-2">
<h2 id="patch%20descriptions%20%2F%20notes"><a href="#patch%20descriptions%20%2F%20notes">patch descriptions / notes</a></h2>
<div class="outline-text-2" id="text-patch%20descriptions%20%2F%20notes">
</div>
<div id="outline-container-Cover%20letter" class="outline-3">
<h3 id="Cover%20letter"><a href="#Cover%20letter">Cover letter</a></h3>
<div class="outline-text-3" id="text-Cover%20letter">
<p>
Bulk optimization for XDP cpumap redirect
</p>

<p>
This patchset utilize a number of different kernel bulk APIs for optimizing
the performance for the XDP cpumap redirect feature.
</p>

<p>
Patch-1: ptr_ring batch consume
Patch-2: Send SKB-lists to network stack
Patch-3: Introduce SKB helper to alloc SKB outside net-core
Patch-4: kmem_cache bulk alloc of SKBs
Patch-5: Prefetch struct page to solve CPU stall
</p>
</div>

<div id="outline-container-stg%20mail%20V1" class="outline-4">
<h4 id="stg%20mail%20V1"><a href="#stg%20mail%20V1">stg mail V1</a></h4>
<div class="outline-text-4" id="text-stg%20mail%20V1">
<pre class="example">
stg mail --version=bpf-next --edit-cover --cc meup \
 --to netdev --cc bpf@vger.kernel.org \
 --to daniel --to alexei --to davem \
 --cc toke --cc ilias \
 use--ptr_ring_consume_batched..optimizations
</pre>

<p>
<a href="https://patchwork.ozlabs.org/project/netdev/list/?series=101964&amp;state=%2a">https://patchwork.ozlabs.org/project/netdev/list/?series=101964&amp;state=%2a</a>
</p>
</div>
</div>
</div>

<div id="outline-container-baseline" class="outline-3">
<h3 id="baseline"><a href="#baseline">baseline</a></h3>
<div class="outline-text-3" id="text-baseline">
</div>
<div id="outline-container-NIC%3A%20i40e1%20-%20Kernel%3A%205.1.0-rc2-bpf-next-cpumap-baseline%2B" class="outline-4">
<h4 id="NIC%3A%20i40e1%20-%20Kernel%3A%205.1.0-rc2-bpf-next-cpumap-baseline%2B"><a href="#NIC%3A%20i40e1%20-%20Kernel%3A%205.1.0-rc2-bpf-next-cpumap-baseline%2B">NIC: i40e1 - Kernel: 5.1.0-rc2-bpf-next-cpumap-baseline+</a></h4>
<div class="outline-text-4" id="text-NIC%3A%20i40e1%20-%20Kernel%3A%205.1.0-rc2-bpf-next-cpumap-baseline%2B">
<p>
Below tests done on top of bpf-next on base commit dd399ac9e343c.
</p>

<p>
The processing and (deliberate) packet drops happens on same CPU as packet
was RX-ed on, which have many cache advantages.
</p>

<pre class="example">
$ uname -a
Linux broadwell 5.1.0-rc2-bpf-next-cpumap-baseline+ #113 SMP PREEMPT Wed Apr 10 16:24:18 CEST 2019 x86_64 x86_64 x86_64 GNU/Linux
</pre>

<p>
NIC: i40e1
</p>
<pre class="example">
$ ethtool -i i40e1
driver: i40e
version: 2.8.10-k
firmware-version: 5.05 0x80002924 1.1313.0
expansion-rom-version: 
bus-info: 0000:04:00.0
supports-statistics: yes
supports-test: yes
supports-eeprom-access: yes
supports-register-dump: yes
supports-priv-flags: yes
</pre>
</div>

<ul class="org-ul">
<li><a id="baseline%3A%20UdpNoPorts%3A%203%2C326%2C983%20pps"></a><a href="#baseline%3A%20UdpNoPorts%3A%203%2C326%2C983%20pps">baseline: UdpNoPorts: 3,326,983 pps</a><br />
<div class="outline-text-5" id="text-baseline%3A%20UdpNoPorts%3A%203%2C326%2C983%20pps">
<p>
Unloaded all netfilter/iptables modules.
</p>

<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    3326983            0.0
IpInDelivers                    3326984            0.0
IpOutRequests                   1                  0.0
IcmpOutMsgs                     1                  0.0
IcmpOutDestUnreachs             1                  0.0
IcmpMsgOutType3                 1                  0.0
UdpNoPorts                      3326982            0.0
IpExtInOctets                   153042460          0.0
IpExtOutOctets                  74                 0.0
IpExtInNoECTPkts                3327010            0.0
</pre>
</div>
</li>

<li><a id="baseline%3A%20iptables-raw%20drop%3A%205%2C705%2C245%20pps%20%28GRO-enabled%29"></a><a href="#baseline%3A%20iptables-raw%20drop%3A%205%2C705%2C245%20pps%20%28GRO-enabled%29">baseline: iptables-raw drop: 5,705,245 pps (GRO-enabled)</a><br />
<div class="outline-text-5" id="text-baseline%3A%20iptables-raw%20drop%3A%205%2C705%2C245%20pps%20%28GRO-enabled%29">
<p>
Command used to drop packets:
</p>
<ul class="org-ul">
<li>iptables -t raw -I PREROUTING -p udp &#x2013;dport 9 -j DROP</li>
</ul>

<p>
With (default) GRO enabled:
</p>
<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    5705245            0.0
IpExtInOctets                   262445778          0.0
IpExtInNoECTPkts                5705343            0.0
</pre>
</div>
</li>

<li><a id="baseline%3A%20iptables-raw%20drop%3A%206%2C701%2C692%20pps%20%28GRO-disabled%29"></a><a href="#baseline%3A%20iptables-raw%20drop%3A%206%2C701%2C692%20pps%20%28GRO-disabled%29">baseline: iptables-raw drop: 6,701,692 pps (GRO-disabled)</a><br />
<div class="outline-text-5" id="text-baseline%3A%20iptables-raw%20drop%3A%206%2C701%2C692%20pps%20%28GRO-disabled%29">
<p>
Command to disable GRO:
</p>
<ul class="org-ul">
<li>ethtool -K i40e1 gro off tso off</li>
</ul>

<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    6701692            0.0
IpExtInOctets                   308276958          0.0
IpExtInNoECTPkts                6701673            0.0
</pre>
</div>
</li>
</ul>
</div>

<div id="outline-container-NIC%3A%20i40e1%20-%20UDP%20socket%20baseline" class="outline-4">
<h4 id="NIC%3A%20i40e1%20-%20UDP%20socket%20baseline"><a href="#NIC%3A%20i40e1%20-%20UDP%20socket%20baseline">NIC: i40e1 - UDP socket baseline</a></h4>
<div class="outline-text-4" id="text-NIC%3A%20i40e1%20-%20UDP%20socket%20baseline">
<p>
Testing with UDP socket sink program:
</p>
<ul class="org-ul">
<li><a href="https://github.com/netoptimizer/network-testing/blob/master/src/udp_sink.c">https://github.com/netoptimizer/network-testing/blob/master/src/udp_sink.c</a></li>
</ul>

<p>
In these situations where the system is overloaded with packets, it is NOT
an advantage to run the UDP socket program on the same CPU as the NAPI
RX-CPU. The reason is that softirq takes up too many resources. (We/kernel
community have solved this to give 50% CPU to each, but still softirq-side
spend all its time dropping packet on the full-socket-queue, that is it
stealing CPU-time-slices from).
</p>
</div>

<ul class="org-ul">
<li><a id="udp_sink%3A%20same%20CPU%20as%20RX%20%3D%20718%2C135%20pps"></a><a href="#udp_sink%3A%20same%20CPU%20as%20RX%20%3D%20718%2C135%20pps">udp_sink: same CPU as RX = 718,135 pps</a><br />
<div class="outline-text-5" id="text-udp_sink%3A%20same%20CPU%20as%20RX%20%3D%20718%2C135%20pps">
<pre class="example">
[jbrouer@broadwell src]$ sudo taskset -c 1 ./udp_sink --port 9 --recvmsg --repeat 1000 --reuse
          	run      count   	ns/pkt	pps		cycles	payload
recvmsg   	run:  0	 1000000	1435.21	696761.84	5166	18	 demux:1
recvmsg   	run:  1	 1000000	1392.46	718153.22	5012	18	 demux:1
recvmsg   	run:  2	 1000000	1392.50	718135.42	5013	18	 demux:1
recvmsg   	run:  3	 1000000	1395.57	716553.74	5024	18	 demux:1
recvmsg   	run:  4	 1000000	1390.34	719249.54	5005	18	 demux:1
recvmsg   	run:  5	 1000000	1400.70	713930.03	5042	18	 demux:1
recvmsg   	run:  6	 1000000	1387.11	720924.95	4993	18	 demux:1
recvmsg   	run:  7	 1000000	1398.32	715144.81	5033	18	 demux:1
recvmsg   	run:  8	 1000000	1392.27	718250.94	5012	18	 demux:1
</pre>
</div>
</li>

<li><a id="udp_sink%3A%20another%20CPU%20than%20RX%20%3D%202%2C311%2C585%20pps"></a><a href="#udp_sink%3A%20another%20CPU%20than%20RX%20%3D%202%2C311%2C585%20pps">udp_sink: another CPU than RX = 2,311,585 pps</a><br />
<div class="outline-text-5" id="text-udp_sink%3A%20another%20CPU%20than%20RX%20%3D%202%2C311%2C585%20pps">
<pre class="example">
[jbrouer@broadwell src]$ sudo taskset -c 3 ./udp_sink --port 9 --recvmsg --repeat 1000 --reuse
          	run      count   	ns/pkt	pps		cycles	payload
recvmsg   	run:  0	 1000000	441.01	2267502.40	1587	18	 demux:1
recvmsg   	run:  1	 1000000	432.89	2310074.13	1558	18	 demux:1
recvmsg   	run:  2	 1000000	432.60	2311585.12	1557	18	 demux:1
recvmsg   	run:  3	 1000000	432.48	2312230.99	1556	18	 demux:1
recvmsg   	run:  4	 1000000	433.49	2306867.38	1560	18	 demux:1
recvmsg   	run:  5	 1000000	432.44	2312474.25	1556	18	 demux:1
recvmsg   	run:  6	 1000000	432.46	2312345.45	1556	18	 demux:1
recvmsg   	run:  7	 1000000	432.39	2312702.92	1556	18	 demux:1
recvmsg   	run:  8	 1000000	432.54	2311903.29	1557	18	 demux:1
recvmsg   	run:  9	 1000000	432.62	2311479.47	1557	18	 demux:1
</pre>
</div>
</li>
</ul>
</div>

<div id="outline-container-NIC%3A%20i40e1%20-%20baseline%20cpumap%20redirect" class="outline-4">
<h4 id="NIC%3A%20i40e1%20-%20baseline%20cpumap%20redirect"><a href="#NIC%3A%20i40e1%20-%20baseline%20cpumap%20redirect">NIC: i40e1 - baseline cpumap redirect</a></h4>
<div class="outline-text-4" id="text-NIC%3A%20i40e1%20-%20baseline%20cpumap%20redirect">
<p>
What is the baseline CPUMAP redirect performance.
</p>
</div>

<ul class="org-ul">
<li><a id="baseline-redirect%3A%20UdpNoPorts%3A%202%2C727%2C840%20pps"></a><a href="#baseline-redirect%3A%20UdpNoPorts%3A%202%2C727%2C840%20pps">baseline-redirect: UdpNoPorts: 2,727,840 pps</a><br />
<div class="outline-text-5" id="text-baseline-redirect%3A%20UdpNoPorts%3A%202%2C727%2C840%20pps">
<pre class="example">
sudo ./xdp_redirect_cpu --dev i40e1 --qsize 128 --cpu 4 --prog xdp_cpu_map0
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          1       15,198,085     0           0          
XDP-RX          total   15,198,085     0          
cpumap-enqueue    1:4   15,198,122     12,470,287  8.00       bulk-average
cpumap-enqueue  sum:4   15,198,122     12,470,287  8.00       bulk-average
cpumap_kthread  4       2,727,840      0           0          
cpumap_kthread  total   2,727,840      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    2701784            0.0
IpInDelivers                    2701783            0.0
IpOutRequests                   1                  0.0
IcmpOutMsgs                     1                  0.0
IcmpOutDestUnreachs             1                  0.0
IcmpMsgOutType3                 1                  0.0
UdpNoPorts                      2701775            0.0
IpExtInOctets                   124283720          0.0
IpExtOutOctets                  74                 0.0
IpExtInNoECTPkts                2701820            0.0
</pre>
</div>
</li>

<li><a id="baseline-redirect%3A%20iptables-raw%20drop%3A%206%2C166%2C709%20pps"></a><a href="#baseline-redirect%3A%20iptables-raw%20drop%3A%206%2C166%2C709%20pps">baseline-redirect: iptables-raw drop: 6,166,709 pps</a><br />
<div class="outline-text-5" id="text-baseline-redirect%3A%20iptables-raw%20drop%3A%206%2C166%2C709%20pps">
<pre class="example">
sudo ./xdp_redirect_cpu --dev i40e1 --qsize 128 --cpu 4 --prog xdp_cpu_map0
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          1       18,850,942     0           0          
XDP-RX          total   18,850,942     0          
cpumap-enqueue    1:4   18,850,947     12,684,239  8.00       bulk-average
cpumap-enqueue  sum:4   18,850,947     12,684,239  8.00       bulk-average
cpumap_kthread  4       6,166,709      0           0          
cpumap_kthread  total   6,166,709      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    6167205            0.0
IpExtInOctets                   283689544          0.0
IpExtInNoECTPkts                6167164            0.0
</pre>
</div>
</li>
</ul>
</div>
</div>

<div id="outline-container-Patch%3A%20bpf%3A%20cpumap%20use%20ptr_ring_consume_batched" class="outline-3">
<h3 id="Patch%3A%20bpf%3A%20cpumap%20use%20ptr_ring_consume_batched"><a href="#Patch%3A%20bpf%3A%20cpumap%20use%20ptr_ring_consume_batched">Patch: bpf: cpumap use ptr_ring_consume_batched</a></h3>
<div class="outline-text-3" id="text-Patch%3A%20bpf%3A%20cpumap%20use%20ptr_ring_consume_batched">
<p>
Move ptr_ring dequeue outside loop, that allocate SKBs and calls network
stack, as these operations that can take some time. The ptr_ring is a
communication channel between CPUs, where we want to reduce/limit any
cacheline bouncing.
</p>

<p>
Do a concentrated bulk dequeue via ptr_ring_consume_batched, to shorten the
period and times the remote cacheline in ptr_ring is read
</p>

<p>
Batch size 8 is both to (1) limit BH-disable period, and (2) consume one
cacheline on 64-bit archs. After reducing the BH-disable section further
then we can consider changing this, while still thinking about L1 cacheline
size being active.
</p>
</div>

<div id="outline-container-benchmarks%20on%20this%20patch" class="outline-4">
<h4 id="benchmarks%20on%20this%20patch"><a href="#benchmarks%20on%20this%20patch">benchmarks on this patch</a></h4>
<div class="outline-text-4" id="text-benchmarks%20on%20this%20patch">
</div>
<ul class="org-ul">
<li><a id="redirect%3A%20UdpNoPorts%3A%202%2C817%2C054"></a><a href="#redirect%3A%20UdpNoPorts%3A%202%2C817%2C054">redirect: UdpNoPorts: 2,817,054</a><br />
<div class="outline-text-5" id="text-redirect%3A%20UdpNoPorts%3A%202%2C817%2C054">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          5       13,967,785     0           0          
XDP-RX          total   13,967,785     0          
cpumap-enqueue    5:4   13,967,766     11,150,711  8.00       bulk-average
cpumap-enqueue  sum:4   13,967,766     11,150,711  8.00       bulk-average
cpumap_kthread  4       2,817,054      0           0          
cpumap_kthread  total   2,817,054      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    2829056            0.0
IpInDelivers                    2829057            0.0
IpOutRequests                   1                  0.0
IcmpOutMsgs                     1                  0.0
IcmpOutDestUnreachs             1                  0.0
IcmpMsgOutType3                 1                  0.0
UdpNoPorts                      2829061            0.0
IpExtInOctets                   130137312          0.0
IpExtOutOctets                  74                 0.0
IpExtInNoECTPkts                2829076            0.0
</pre>
</div>
</li>

<li><a id="redirect%3A%20iptables-raw%20drop%3A%206%2C328%2C978"></a><a href="#redirect%3A%20iptables-raw%20drop%3A%206%2C328%2C978">redirect: iptables-raw drop: 6,328,978</a><br />
<div class="outline-text-5" id="text-redirect%3A%20iptables-raw%20drop%3A%206%2C328%2C978">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          5       18,458,183     0           0          
XDP-RX          total   18,458,183     0          
cpumap-enqueue    5:4   18,458,184     12,129,207  8.00       bulk-average
cpumap-enqueue  sum:4   18,458,184     12,129,207  8.00       bulk-average
cpumap_kthread  4       6,328,978      0           0          
cpumap_kthread  total   6,328,978      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    6358270            0.0
IpInDelivers                    1                  0.0
IpOutRequests                   1                  0.0
TcpInSegs                       1                  0.0
TcpOutSegs                      1                  0.0
TcpExtTCPHPAcks                 1                  0.0
TcpExtTCPOrigDataSent           1                  0.0
TcpExtTCPDelivered              1                  0.0
IpExtInOctets                   292478632          0.0
IpExtOutOctets                  680                0.0
IpExtInNoECTPkts                6358232            0.0
</pre>
</div>
</li>
</ul>
</div>
</div>


<div id="outline-container-Patch%3A%20bpf%3A%20cpumap%20send%20a%20SKB-list%20towards%20network%20stack." class="outline-3">
<h3 id="Patch%3A%20bpf%3A%20cpumap%20send%20a%20SKB-list%20towards%20network%20stack."><a href="#Patch%3A%20bpf%3A%20cpumap%20send%20a%20SKB-list%20towards%20network%20stack.">Patch: bpf: cpumap send a SKB-list towards network stack.</a></h3>
<div class="outline-text-3" id="text-Patch%3A%20bpf%3A%20cpumap%20send%20a%20SKB-list%20towards%20network%20stack.">
<p>
Reduce BH-disable period further by moving cpu_map_build_skb()
outside/before invoking the network stack. And build up a skb_list that is
used for netif_receive_skb_list. This is also an I-cache optimization.
</p>

<p>
When injecting packets into the network stack, cpumap used a special
function named netif_receive_skb_core(), in-order to skip generic-XDP.
For this reason create an equivalent list version named
netif_receive_skb_list_core().
</p>
</div>

<div id="outline-container-benchmark01%20on%20this%20patch" class="outline-4">
<h4 id="benchmark01%20on%20this%20patch"><a href="#benchmark01%20on%20this%20patch">benchmark01 on this patch</a></h4>
<div class="outline-text-4" id="text-benchmark01%20on%20this%20patch">
</div>
<ul class="org-ul">
<li><a id="redirect%3A%20UdpNoPorts%3A%202%2C846%2C583"></a><a href="#redirect%3A%20UdpNoPorts%3A%202%2C846%2C583">redirect: UdpNoPorts: 2,846,583</a><br />
<div class="outline-text-5" id="text-redirect%3A%20UdpNoPorts%3A%202%2C846%2C583">
<pre class="example">
sudo ./xdp_redirect_cpu --dev i40e1 --qsize 128 --cpu 4 --prog xdp_cpu_map0 --sec 3
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          0       14,810,855     0           0          
XDP-RX          total   14,810,855     0          
cpumap-enqueue    0:4   14,810,875     11,964,289  8.00       bulk-average
cpumap-enqueue  sum:4   14,810,875     11,964,289  8.00       bulk-average
cpumap_kthread  4       2,846,583      0           0          
cpumap_kthread  total   2,846,583      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>

<li><a id="redirect%3A%20iptables-raw%20drop%3A%205%2C535%2C958"></a><a href="#redirect%3A%20iptables-raw%20drop%3A%205%2C535%2C958">redirect: iptables-raw drop: 5,535,958</a><br />
<div class="outline-text-5" id="text-redirect%3A%20iptables-raw%20drop%3A%205%2C535%2C958">
<p>
Strange performance drop.
</p>

<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          0       18,252,989     0           0          
XDP-RX          total   18,252,989     0          
cpumap-enqueue    0:4   18,252,986     12,717,028  8.00       bulk-average
cpumap-enqueue  sum:4   18,252,986     12,717,028  8.00       bulk-average
cpumap_kthread  4       5,535,958      0           0          
cpumap_kthread  total   5,535,958      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>

<li><a id="iptables-raw%20drop%3A%205%2C378%2C828%20pps%20%28GRO-enabled%29"></a><a href="#iptables-raw%20drop%3A%205%2C378%2C828%20pps%20%28GRO-enabled%29">iptables-raw drop: 5,378,828 pps (GRO-enabled)</a><br />
<div class="outline-text-5" id="text-iptables-raw%20drop%3A%205%2C378%2C828%20pps%20%28GRO-enabled%29">
<p>
Command used to drop packets:
</p>
<ul class="org-ul">
<li>iptables -t raw -I PREROUTING -p udp &#x2013;dport 9 -j DROP</li>
</ul>

<p>
Using standard Linux kernel and NAPI-RX iptables-raw drop. It doesn't make
sense that performance is reduced. As the patch only change/add
netif_receive_skb_list_core to net/core/dev.c.
</p>

<p>
With (default) GRO enabled:
</p>
<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    5378828            0.0
IpExtInOctets                   247426732          0.0
IpExtInNoECTPkts                5378842            0.0
</pre>

<p>
GRO-disable:
</p>
<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    6269627            0.0
IpExtInOctets                   288405556          0.0
IpExtInNoECTPkts                6269686            0.0
</pre>
</div>
</li>
</ul>
</div>

<div id="outline-container-benchmark02%20on%20this%20patch" class="outline-4">
<h4 id="benchmark02%20on%20this%20patch"><a href="#benchmark02%20on%20this%20patch">benchmark02 on this patch</a></h4>
<div class="outline-text-4" id="text-benchmark02%20on%20this%20patch">
<p>
Re-organize code in net/core/dev.c.
</p>
</div>

<ul class="org-ul">
<li><a id="redirect%3A%20UdpNoPorts%3A%202%2C829%2C666"></a><a href="#redirect%3A%20UdpNoPorts%3A%202%2C829%2C666">redirect: UdpNoPorts: 2,829,666</a><br />
<div class="outline-text-5" id="text-redirect%3A%20UdpNoPorts%3A%202%2C829%2C666">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          4       14,996,383     0           0          
XDP-RX          total   14,996,383     0          
cpumap-enqueue    4:5   14,996,387     12,166,725  8.00       bulk-average
cpumap-enqueue  sum:5   14,996,387     12,166,725  8.00       bulk-average
cpumap_kthread  5       2,829,666      0           0          
cpumap_kthread  total   2,829,666      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>

<li><a id="redirect%3A%20iptables-raw%20drop%3A%205%2C529%2C818"></a><a href="#redirect%3A%20iptables-raw%20drop%3A%205%2C529%2C818">redirect: iptables-raw drop: 5,529,818</a><br />
<div class="outline-text-5" id="text-redirect%3A%20iptables-raw%20drop%3A%205%2C529%2C818">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          4       18,256,809     0           0          
XDP-RX          total   18,256,809     0          
cpumap-enqueue    4:5   18,256,806     12,726,988  8.00       bulk-average
cpumap-enqueue  sum:5   18,256,806     12,726,988  8.00       bulk-average
cpumap_kthread  5       5,529,818      0           0          
cpumap_kthread  total   5,529,818      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>

<li><a id="iptables-raw%20drop%3A%205%2C420%2C909"></a><a href="#iptables-raw%20drop%3A%205%2C420%2C909">iptables-raw drop: 5,420,909</a><br />
<div class="outline-text-5" id="text-iptables-raw%20drop%3A%205%2C420%2C909">
<p>
Using standard Linux kernel and NAPI-RX iptables-raw drop.
</p>
<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    5420909            0.0
IpExtInOctets                   249361032          0.0
IpExtInNoECTPkts                5420892            0.0
</pre>
</div>
</li>
</ul>
</div>

<div id="outline-container-benchmark03%20more%20reorg" class="outline-4">
<h4 id="benchmark03%20more%20reorg"><a href="#benchmark03%20more%20reorg">benchmark03 more reorg</a></h4>
<div class="outline-text-4" id="text-benchmark03%20more%20reorg">
<p>
Re-organize code in net/core/dev.c.
</p>

<p>
redirect: UdpNoPorts: 2,866,070
redirect: iptables-raw drop: 5,516,606
</p>
</div>
</div>
</div>


<div id="outline-container-Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around" class="outline-3">
<h3 id="Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around"><a href="#Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around">Patch: net: core: introduce build_skb_around</a></h3>
<div class="outline-text-3" id="text-Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around">
<p>
The function build_skb() also have the responsibility to allocate and clear
the SKB structure. Introduce a new function build_skb_around(), that moves
the responsibility of allocation and clearing to the caller. This allows
caller to use kmem_cache (slab/slub) bulk allocation API.
</p>

<p>
Next patch use this function combined with kmem_cache_alloc_bulk.
</p>
</div>

<div id="outline-container-Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around--benchmarks%20on%20this%20patch" class="outline-4">
<h4 id="Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around--benchmarks%20on%20this%20patch"><a href="#Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around--benchmarks%20on%20this%20patch">benchmarks on this patch</a></h4>
<div class="outline-text-4" id="text-Patch%3A%20net%3A%20core%3A%20introduce%20build_skb_around--benchmarks%20on%20this%20patch">
</div>
<ul class="org-ul">
<li><a id="redirect%3A%20UdpNoPorts%3A%202%2C832%2C411"></a><a href="#redirect%3A%20UdpNoPorts%3A%202%2C832%2C411">redirect: UdpNoPorts: 2,832,411</a><br />
<div class="outline-text-5" id="text-redirect%3A%20UdpNoPorts%3A%202%2C832%2C411">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          5       14,951,827     0           0          
XDP-RX          total   14,951,827     0          
cpumap-enqueue    5:4   14,951,808     12,119,396  8.00       bulk-average
cpumap-enqueue  sum:4   14,951,808     12,119,396  8.00       bulk-average
cpumap_kthread  4       2,832,411      0           0          
cpumap_kthread  total   2,832,411      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>

<li><a id="redirect%3A%20iptables-raw%20drop%3A%205%2C522%2C555"></a><a href="#redirect%3A%20iptables-raw%20drop%3A%205%2C522%2C555">redirect: iptables-raw drop: 5,522,555</a><br />
<div class="outline-text-5" id="text-redirect%3A%20iptables-raw%20drop%3A%205%2C522%2C555">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          5       18,495,707     0           0          
XDP-RX          total   18,495,707     0          
cpumap-enqueue    5:4   18,495,706     12,973,151  8.00       bulk-average
cpumap-enqueue  sum:4   18,495,706     12,973,151  8.00       bulk-average
cpumap_kthread  4       5,522,555      0           0          
cpumap_kthread  total   5,522,555      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>

<li><a id="iptables-raw%20drop%3A%205%2C396%2C717"></a><a href="#iptables-raw%20drop%3A%205%2C396%2C717">iptables-raw drop: 5,396,717</a><br />
<div class="outline-text-5" id="text-iptables-raw%20drop%3A%205%2C396%2C717">
<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    5396717            0.0
IpExtInOctets                   248249120          0.0
IpExtInNoECTPkts                5396720            0.0
</pre>
</div>
</li>
</ul>
</div>
</div>

<div id="outline-container-Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs" class="outline-3">
<h3 id="Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs"><a href="#Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs">Patch: bpf: cpumap do bulk allocation of SKBs</a></h3>
<div class="outline-text-3" id="text-Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs">
<p>
As cpumap now batch consume xdp_frame's from the ptr_ring, it knows how many
SKBs it need to allocate. Thus, lets bulk allocate these SKBs via
kmem_cache_alloc_bulk() API, and use the previously introduced function
build_skb_around().
</p>

<p>
Notice that the flag __GFP_ZERO asks the slab/slub allocator to clear the
memory for us. This does clear a larger area than needed, but my micro
benchmarks on Intel CPUs show that this is slightly faster due to being a
cacheline aligned area is cleared for the SKBs. (For SLUB allocator, there
is a future optimization potential, because SKBs will with high probability
originate from same page. If we can find/identify continuous memory areas
then the Intel CPU memset rep stos will have a real performance gain.)
</p>
</div>

<div id="outline-container-Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs--benchmarks%20on%20this%20patch" class="outline-4">
<h4 id="Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs--benchmarks%20on%20this%20patch"><a href="#Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs--benchmarks%20on%20this%20patch">benchmarks on this patch</a></h4>
<div class="outline-text-4" id="text-Patch%3A%20bpf%3A%20cpumap%20do%20bulk%20allocation%20of%20SKBs--benchmarks%20on%20this%20patch">
</div>
<ul class="org-ul">
<li><a id="redirect%3A%20UdpNoPorts%3A%202%2C943%2C928"></a><a href="#redirect%3A%20UdpNoPorts%3A%202%2C943%2C928">redirect: UdpNoPorts: 2,943,928</a><br />
<div class="outline-text-5" id="text-redirect%3A%20UdpNoPorts%3A%202%2C943%2C928">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          4       13,650,238     0           0          
XDP-RX          total   13,650,238     0          
cpumap-enqueue    4:5   13,650,246     10,706,320  8.00       bulk-average
cpumap-enqueue  sum:5   13,650,246     10,706,320  8.00       bulk-average
cpumap_kthread  5       2,943,928      0           0          
cpumap_kthread  total   2,943,928      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>

<li><a id="redirect%3A%20iptables-raw%20drop%3A%205%2C908%2C032"></a><a href="#redirect%3A%20iptables-raw%20drop%3A%205%2C908%2C032">redirect: iptables-raw drop: 5,908,032</a><br />
<div class="outline-text-5" id="text-redirect%3A%20iptables-raw%20drop%3A%205%2C908%2C032">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          4       18,256,284     0           0          
XDP-RX          total   18,256,284     0          
cpumap-enqueue    4:5   18,256,282     12,348,249  8.00       bulk-average
cpumap-enqueue  sum:5   18,256,282     12,348,249  8.00       bulk-average
cpumap_kthread  5       5,908,032      0           0          
cpumap_kthread  total   5,908,032      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<pre class="example">
$ perf stat -C5 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -r 4 sleep 1

 Performance counter stats for 'CPU(s) 5' (4 runs):

     3.803.541.867      cycles                                                        ( +-  0,00% )
     7.181.656.680      instructions              #    1,89  insn per cycle           ( +-  0,03% )
        38.215.645      cache-references                                              ( +-  0,13% )
               956      cache-misses              #    0,003 % of all cache refs      ( +- 68,12% )
     1.359.526.208      branches:k                                                    ( +-  0,03% )
         2.127.934      branch-misses:k           #    0,16% of all branches          ( +-  0,83% )
            94.326      l2_rqsts.all_code_rd                                          ( +-  1,60% )
            74.614      l2_rqsts.code_rd_hit                                          ( +-  1,67% )
            19.709      l2_rqsts.code_rd_miss                                         ( +-  2,45% )
            36.783      L1-icache-load-misses                                         ( +-  1,31% )
</pre>

<pre class="example">
$ perf stat -C5 -e cycles -e  instructions -e l1d.replacement -e l1d_pend_miss.fb_full -e l1d_pend_miss.pending -e l1d_pend_miss.pending_cycles  -r3 sleep 1

 Performance counter stats for 'CPU(s) 5' (3 runs):

     3.795.165.763      cycles                                                        ( +-  0,00% )  (33,27%)
     7.164.568.267      instructions              #    1,89  insn per cycle           ( +-  0,04% )  (49,95%)
        53.336.896      l1d.replacement                                               ( +-  0,68% )  (66,63%)
               549      l1d_pend_miss.fb_full                                         ( +- 96,09% )  (83,32%)
     1.345.207.553      l1d_pend_miss.pending                                         ( +-  0,25% )  (83,38%)
       806.293.783      l1d_pend_miss.pending_cycles                                     ( +-  0,29% )  (16,62%)
</pre>
</div>
</li>
</ul>
</div>
</div>

<div id="outline-container-Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page" class="outline-3">
<h3 id="Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page"><a href="#Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page">Patch: bpf: cpumap memory prefetchw optimizations for struct page</a></h3>
<div class="outline-text-3" id="text-Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page">
<p>
A lot of the performance gain comes from this patch.
</p>

<p>
While analysing performance overhead it was found that the largest CPU
stalls were caused when touching the struct page area. It is first read with
a READ_ONCE from build_skb_around via page_is_pfmemalloc(), and when freed
written by page_frag_free() call.
</p>

<p>
Measurements show that the prefetchw (W) variant operation is needed to
achieve the performance gain. We believe this optimization it two fold,
first the W-variant saves one step in the cache-coherency protocol, and
second it helps us to avoid the non-temporal prefetch HW optimizations and
bring this into all cache-levels. It might be worth investigating if
prefetch into L2 will have the same benefit.
</p>
</div>

<div id="outline-container-Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page--benchmarks%20on%20this%20patch" class="outline-4">
<h4 id="Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page--benchmarks%20on%20this%20patch"><a href="#Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page--benchmarks%20on%20this%20patch">benchmarks on this patch</a></h4>
<div class="outline-text-4" id="text-Patch%3A%20bpf%3A%20cpumap%20memory%20prefetchw%20optimizations%20for%20struct%20page--benchmarks%20on%20this%20patch">
</div>
<ul class="org-ul">
<li><a id="redirect%3A%20UdpNoPorts%3A%203%2C270%2C640"></a><a href="#redirect%3A%20UdpNoPorts%3A%203%2C270%2C640">redirect: UdpNoPorts: 3,270,640</a><br />
<div class="outline-text-5" id="text-redirect%3A%20UdpNoPorts%3A%203%2C270%2C640">
<pre class="example">
unning XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          1       14,773,250     0           0          
XDP-RX          total   14,773,250     0          
cpumap-enqueue    1:5   14,773,260     11,502,619  8.00       bulk-average
cpumap-enqueue  sum:5   14,773,260     11,502,619  8.00       bulk-average
cpumap_kthread  5       3,270,640      0           0          
cpumap_kthread  total   3,270,640      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>

<li><a id="redirect%3A%20iptables-raw%20drop%3A%206%2C882%2C973"></a><a href="#redirect%3A%20iptables-raw%20drop%3A%206%2C882%2C973">redirect: iptables-raw drop: 6,882,973</a><br />
<div class="outline-text-5" id="text-redirect%3A%20iptables-raw%20drop%3A%206%2C882%2C973">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          1       19,235,746     0           0          
XDP-RX          total   19,235,746     0          
cpumap-enqueue    1:5   19,235,747     12,352,773  8.00       bulk-average
cpumap-enqueue  sum:5   19,235,747     12,352,773  8.00       bulk-average
cpumap_kthread  5       6,882,973      0           0          
cpumap_kthread  total   6,882,973      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>
</ul>
</div>
</div>

<div id="outline-container-test%20reorg" class="outline-3">
<h3 id="test%20reorg"><a href="#test%20reorg">test reorg</a></h3>
<div class="outline-text-3" id="text-test%20reorg">
</div>
<div id="outline-container-benchmarks%20on%20experimental%20patch" class="outline-4">
<h4 id="benchmarks%20on%20experimental%20patch"><a href="#benchmarks%20on%20experimental%20patch">benchmarks on experimental patch</a></h4>
<div class="outline-text-4" id="text-benchmarks%20on%20experimental%20patch">
<p>
Re-organize code in net/core/dev.c. Results look like the performance
problem was solved.  UPDATE: This could be cause by F27 reloading iptables
filter chains and kernel modules.  For the iptrables-raw it shouldn't be as
effected by iptables-filter being loaded or not.
</p>
</div>

<ul class="org-ul">
<li><a id="redirect%3A%20UdpNoPorts%3A%203%2C060%2C774"></a><a href="#redirect%3A%20UdpNoPorts%3A%203%2C060%2C774">redirect: UdpNoPorts: 3,060,774</a><br />
<div class="outline-text-5" id="text-redirect%3A%20UdpNoPorts%3A%203%2C060%2C774">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          0       14,265,023     0           0          
XDP-RX          total   14,265,023     0          
cpumap-enqueue    0:5   14,265,033     11,204,255  8.00       bulk-average
cpumap-enqueue  sum:5   14,265,033     11,204,255  8.00       bulk-average
cpumap_kthread  5       3,060,774      0           0          
cpumap_kthread  total   3,060,774      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</li>

<li><a id="redirect%3A%20iptables-raw%20drop%3A%207%2C035%2C517"></a><a href="#redirect%3A%20iptables-raw%20drop%3A%207%2C035%2C517">redirect: iptables-raw drop: 7,035,517</a><br />
<div class="outline-text-5" id="text-redirect%3A%20iptables-raw%20drop%3A%207%2C035%2C517">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          0       18,710,012     0           0          
XDP-RX          total   18,710,012     0          
cpumap-enqueue    0:5   18,710,010     11,674,495  8.00       bulk-average
cpumap-enqueue  sum:5   18,710,010     11,674,495  8.00       bulk-average
cpumap_kthread  5       7,035,517      0           0          
cpumap_kthread  total   7,035,517      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<p>
Perf stats results:
</p>
<pre class="example">
$ perf stat -C5 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -r 4 sleep 1

 Performance counter stats for 'CPU(s) 5' (4 runs):

     3.803.441.397      cycles                                                        ( +-  0,00% )
     8.631.964.172      instructions              #    2,27  insn per cycle           ( +-  0,09% )
        38.712.388      cache-references                                              ( +-  0,24% )
               828      cache-misses              #    0,002 % of all cache refs      ( +- 27,03% )
     1.628.030.913      branches:k                                                    ( +-  0,09% )
         2.471.318      branch-misses:k           #    0,15% of all branches          ( +-  0,40% )
            64.688      l2_rqsts.all_code_rd                                          ( +-  1,19% )
            56.469      l2_rqsts.code_rd_hit                                          ( +-  1,23% )
             8.179      l2_rqsts.code_rd_miss                                         ( +-  1,49% )
            17.866      L1-icache-load-misses                                         ( +-  0,90% )
</pre>

<pre class="example">
$ perf stat -C5 -e cycles -e  instructions -e l1d.replacement -e l1d_pend_miss.fb_full -e l1d_pend_miss.pending -e l1d_pend_miss.pending_cycles  -r3 sleep 1

 Performance counter stats for 'CPU(s) 5' (3 runs):

     3.795.335.615      cycles                                                        ( +-  0,00% )  (33,27%)
     8.599.169.329      instructions              #    2,27  insn per cycle           ( +-  0,16% )  (49,95%)
        58.903.910      l1d.replacement                                               ( +-  0,71% )  (66,63%)
            93.303      l1d_pend_miss.fb_full                                         ( +-  4,39% )  (83,32%)
       804.495.333      l1d_pend_miss.pending                                         ( +-  0,32% )  (83,35%)
       639.584.616      l1d_pend_miss.pending_cycles                                     ( +-  0,57% )  (16,65%)

        1,00107125 +- 0,00000745 seconds time elapsed  ( +-  0,00% )
</pre>
</div>
</li>

<li><a id="iptables-raw%20drop%3A%205%2C412%2C097%20%28GRO-enabled%29"></a><a href="#iptables-raw%20drop%3A%205%2C412%2C097%20%28GRO-enabled%29">iptables-raw drop: 5,412,097 (GRO-enabled)</a><br />
<div class="outline-text-5" id="text-iptables-raw%20drop%3A%205%2C412%2C097%20%28GRO-enabled%29">
<p>
Command used to drop packets:
</p>
<ul class="org-ul">
<li>iptables -t raw -I PREROUTING -p udp &#x2013;dport 9 -j DROP</li>
</ul>

<p>
Using standard Linux kernel and NAPI-RX iptables-raw drop.
</p>
<pre class="example">
nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    5412097            0.0
IpExtInOctets                   248955956          0.0
IpExtInNoECTPkts                5412085            0.0
</pre>

<p>
*
</p>
</div>
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-notes" class="outline-2">
<h2 id="notes"><a href="#notes">notes</a></h2>
<div class="outline-text-2" id="text-notes">
<p>
-e l2_lines_in.all -e l2_lines_in.e -e l2_lines_in.i -e l2_lines_in.s
</p>

<p>
-e l1d.replacement -e l1d_pend_miss.fb_full -e l1d_pend_miss.pending -e l1d_pend_miss.pending_cycles -e l1d_pend_miss.pending_cycles_any
</p>
</div>
</div>

<div id="outline-container-Evaluating%20effect%20of%20page-prefetchw" class="outline-2">
<h2 id="Evaluating%20effect%20of%20page-prefetchw"><a href="#Evaluating%20effect%20of%20page-prefetchw">Evaluating effect of page-prefetchw</a></h2>
<div class="outline-text-2" id="text-Evaluating%20effect%20of%20page-prefetchw">
<p>
(Below tests done on top of base commit dd399ac9e343c)
</p>

<p>
Conclusion: based on below, the prefetchw on struct-page is important.
</p>
</div>

<div id="outline-container-page-prefetchw%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop" class="outline-3">
<h3 id="page-prefetchw%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop"><a href="#page-prefetchw%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop">page-prefetchw + i40e + batch-16 + iptables-raw-drop</a></h3>
<div class="outline-text-3" id="text-page-prefetchw%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop">
<pre class="example">
$ sudo ./xdp_redirect_cpu --prog 0 --dev i40e1 --qsize 128 --cpu 5
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          0       18,028,028     0           0          
XDP-RX          total   18,028,028     0          
cpumap-enqueue    0:5   18,028,030     10,724,216  8.00       bulk-average
cpumap-enqueue  sum:5   18,028,030     10,724,216  8.00       bulk-average
cpumap_kthread  5       7,303,802      0           0          
cpumap_kthread  total   7,303,802      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</div>

<div id="outline-container-page-prefetch%20%28non-W%29%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop" class="outline-3">
<h3 id="page-prefetch%20%28non-W%29%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop"><a href="#page-prefetch%20%28non-W%29%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop">page-prefetch (non-W) + i40e + batch-16 + iptables-raw-drop</a></h3>
<div class="outline-text-3" id="text-page-prefetch%20%28non-W%29%20%2B%20i40e%20%2B%20batch-16%20%2B%20iptables-raw-drop">
<pre class="example">
$ sudo ./xdp_redirect_cpu --prog 0 --dev i40e1 --qsize 128 --cpu 5
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          3       19,137,856     0           0          
XDP-RX          total   19,137,856     0          
cpumap-enqueue    3:5   19,137,856     12,784,500  8.00       bulk-average
cpumap-enqueue  sum:5   19,137,856     12,784,500  8.00       bulk-average
cpumap_kthread  5       6,353,356      0           0          
cpumap_kthread  total   6,353,356      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<p>
Code change:
</p>
<div class="org-src-container">
<pre class="src src-diff">diff --git a/kernel/bpf/cpumap.c b/kernel/bpf/cpumap.c
index bdbb3c1131b5..74d4bc16dd67 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/kernel/bpf/cpumap.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/kernel/bpf/cpumap.c</span>
<span style="font-weight: bold;">@@ -288,7 +288,7 @@</span><span style="font-weight: bold;"> static int cpu_map_kthread_run(void *data)</span>
                for (i = 0; i &lt; n; i++) {
                        void *f = frames[i];
                        struct page *page = virt_to_page(f);
-                       prefetchw(page);
+                       prefetch(page);
                }

                m = kmem_cache_alloc_bulk(skbuff_head_cache, gfp, n, skbs);
</pre>
</div>

<p>
Not using CPUMAP redirect iptable-raw-drop performance is: 5,264,940 pps
</p>
<pre class="example">
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    5264940            0.0
IpExtInOctets                   242187562          0.0
IpExtInNoECTPkts                5264948            0.0
</pre>
</div>
</div>
</div>

<div id="outline-container-Eval%20prefetch%20of%20xdp_frame%20area" class="outline-2">
<h2 id="Eval%20prefetch%20of%20xdp_frame%20area"><a href="#Eval%20prefetch%20of%20xdp_frame%20area">Eval prefetch of xdp_frame area</a></h2>
<div class="outline-text-2" id="text-Eval%20prefetch%20of%20xdp_frame%20area">
<p>
Normal prefetch of xdp_frame area didn't improve performance (batch 16).
One theory is eviction from L1-cache.
</p>

<p>
Using prefetchw helped a little, but it can be caused by prefetchw is a
non-temporal prefetch, meaning it will stay in L2, if we have L1-eviction.
</p>

<p>
The problem with xdp_frame area is that it is placed at the same offset in
the page, which can leads to cache-eviction (N-way caches). We would rather
do a L2-cache prefetch.
</p>
</div>

<div id="outline-container-prefetchw%20xdp_frame" class="outline-3">
<h3 id="prefetchw%20xdp_frame"><a href="#prefetchw%20xdp_frame">prefetchw xdp_frame</a></h3>
<div class="outline-text-3" id="text-prefetchw%20xdp_frame">
<p>
Using prefetchw helped:
</p>
<pre class="example">
$ sudo ./xdp_redirect_cpu --prog 0 --dev i40e1 --qsize 64 --cpu 4
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          1       19,307,072     0           0          
XDP-RX          total   19,307,072     0          
cpumap-enqueue    1:4   19,307,073     11,794,092  8.00       bulk-average
cpumap-enqueue  sum:4   19,307,073     11,794,092  8.00       bulk-average
cpumap_kthread  4       7,512,970      0           0          
cpumap_kthread  total   7,512,970      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<pre class="example">
$ perf stat -C4 -e cycles -e  instructions -e l1d.replacement -e l1d_pend_miss.fb_full -e l1d_pend_miss.pending -e l1d_pend_miss.pending_cycles -e l1d_pend_miss.pending_cycles_any  -r 4 sleep 1

 Performance counter stats for 'CPU(s) 4' (4 runs):

     3.794.861.380  cycles                                               ( +-  0,00% )  (28,57%)
     8.950.874.892  instructions              #    2,36  insn per cycle  ( +-  0,07% )  (42,86%)
        92.133.094  l1d.replacement                                      ( +-  0,46% )  (57,14%)
        89.670.480  l1d_pend_miss.fb_full                                ( +-  0,99% )  (71,43%)
       695.281.894  l1d_pend_miss.pending                                ( +-  0,47% )  (71,43%)
       616.443.707  l1d_pend_miss.pending_cycles                         ( +-  0,40% )  (14,29%)
       615.381.726  l1d_pend_miss.pending_cycles_any                     ( +-  0,36% )  (14,29%)
</pre>
</div>
</div>

<div id="outline-container-remove%20any%20prefetch%20of%20xdp_frame" class="outline-3">
<h3 id="remove%20any%20prefetch%20of%20xdp_frame"><a href="#remove%20any%20prefetch%20of%20xdp_frame">remove any prefetch of xdp_frame</a></h3>
<div class="outline-text-3" id="text-remove%20any%20prefetch%20of%20xdp_frame">
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          0       18,349,802     0           0          
XDP-RX          total   18,349,802     0          
cpumap-enqueue    0:4   18,349,802     10,799,899  8.00       bulk-average
cpumap-enqueue  sum:4   18,349,802     10,799,899  8.00       bulk-average
cpumap_kthread  4       7,549,897      0           1          sched
cpumap_kthread  total   7,549,897      0           1          sched-sum
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<pre class="example">
$ perf stat -C4 -e cycles -e  instructions -e l1d.replacement -e l1d_pend_miss.fb_full -e l1d_pend_miss.pending -e l1d_pend_miss.pending_cycles -e l1d_pend_miss.pending_cycles_any  -r 4 sleep 1
 Performance counter stats for 'CPU(s) 4' (4 runs):

     3.794.603.721  cycles                                               ( +-  0,00% )  (28,57%)
     9.001.741.962  instructions              #    2,37  insn per cycle  ( +-  0,05% )  (42,86%)
        82.657.850  l1d.replacement                                      ( +-  0,34% )  (57,14%)
        20.614.863  l1d_pend_miss.fb_full                                ( +-  1,13% )  (71,43%)
       682.789.984  l1d_pend_miss.pending                                ( +-  0,30% )  (71,43%)
       646.913.349  l1d_pend_miss.pending_cycles                         ( +-  0,29% )  (14,29%)
       646.047.378  l1d_pend_miss.pending_cycles_any                     ( +-  0,29% )  (14,29%)
</pre>

<p>
Info on perf events:
</p>
<pre class="example">
l1d.replacement                                   
     [L1D data line replacements]
l1d_pend_miss.fb_full                             
     [Cycles a demand request was blocked due to Fill Buffers inavailability]
l1d_pend_miss.pending                             
     [L1D miss oustandings duration in cycles]
l1d_pend_miss.pending_cycles                      
     [Cycles with L1D load Misses outstanding]
l1d_pend_miss.pending_cycles_any                  
     [Cycles with L1D load Misses outstanding from any thread on physical core]
</pre>

<p>
Notice how: l1d_pend_miss.fb_full was reduced from 89.670.480 to 20.614.863.
</p>
</div>
</div>

<div id="outline-container-test%20reduce%20CPUMAP_BATCH%20to%208" class="outline-3">
<h3 id="test%20reduce%20CPUMAP_BATCH%20to%208"><a href="#test%20reduce%20CPUMAP_BATCH%20to%208">test reduce CPUMAP_BATCH to 8</a></h3>
<div class="outline-text-3" id="text-test%20reduce%20CPUMAP_BATCH%20to%208">
<p>
This hurt performance:
</p>
<pre class="example">
sudo ./xdp_redirect_cpu --prog 0 --dev i40e1 --qsize 64 --cpu 5
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          4       18,396,301     0           0          
XDP-RX          total   18,396,301     0          
cpumap-enqueue    4:5   18,396,296     11,656,127  8.00       bulk-average
cpumap-enqueue  sum:5   18,396,296     11,656,127  8.00       bulk-average
cpumap_kthread  5       6,740,176      0           0          
cpumap_kthread  total   6,740,176      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<p>
Using &#x2013;qsize 128 is slightly better:
</p>
<pre class="example">
sudo ./xdp_redirect_cpu --prog 0 --dev i40e1 --qsize 128 --cpu 5
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          4       17,713,328     0           0          
XDP-RX          total   17,713,328     0          
cpumap-enqueue    4:5   17,713,334     10,725,345  8.00       bulk-average
cpumap-enqueue  sum:5   17,713,334     10,725,345  8.00       bulk-average
cpumap_kthread  5       6,987,990      0           0          
cpumap_kthread  total   6,987,990      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<pre class="example">
$ perf stat -C5 -e cycles -e  instructions -e l1d.replacement -e l1d_pend_miss.fb_full -e l1d_pend_miss.pending -e l1d_pend_miss.pending_cycles -e l1d_pend_miss.pending_cycles_any  -r 10 sleep 1

 Performance counter stats for 'CPU(s) 5' (10 runs):

   3.794.963.218  cycles                                             ( +-  0,00% )  (28,57%)
   8.589.996.063  instructions              #  2,26  insn per cycle  ( +-  0,08% )  (42,86%)
      56.201.273  l1d.replacement                                    ( +-  0,56% )  (57,14%)
          68.600  l1d_pend_miss.fb_full                              ( +-  3,05% )  (71,43%)
     775.802.766  l1d_pend_miss.pending                              ( +-  0,37% )  (71,43%)
     624.584.133  l1d_pend_miss.pending_cycles                       ( +-  0,43% )  (14,29%)
     623.719.946  l1d_pend_miss.pending_cycles_any                   ( +-  0,41% )  (14,29%)
</pre>

<p>
The perf stat show that our Fill Buffers inavailability (is significantly
reduced).
</p>
</div>
</div>

<div id="outline-container-Test%3A%20prefetchw%20single%20%2B%20i%2B1" class="outline-3">
<h3 id="Test%3A%20prefetchw%20single%20%2B%20i%2B1"><a href="#Test%3A%20prefetchw%20single%20%2B%20i%2B1">Test: prefetchw single + i+1</a></h3>
<div class="outline-text-3" id="text-Test%3A%20prefetchw%20single%20%2B%20i%2B1">
<p>
Test if prefetch xdp_frame i+1 before cpu_map_build_skb() works.
</p>

<div class="org-src-container">
<pre class="src src-C"><span style="font-weight: bold;">for</span> (i = 0; i &lt; n; i++) {
        <span style="font-weight: bold;">struct</span> <span style="font-weight: bold; text-decoration: underline;">xdp_frame</span> *<span style="font-weight: bold; font-style: italic;">xdpf</span> = frames[i];
        <span style="font-weight: bold;">struct</span> <span style="font-weight: bold; text-decoration: underline;">sk_buff</span> *<span style="font-weight: bold; font-style: italic;">skb</span> = skbs[i];

        <span style="font-weight: bold; font-style: italic;">/* </span><span style="font-weight: bold; font-style: italic;">Bring in xdp_frame area </span><span style="font-weight: bold; font-style: italic;">*/</span>
        prefetchw(frames[i+1]);

        skb = cpu_map_build_skb(rcpu, xdpf, skb);
        <span style="font-weight: bold;">if</span> (!skb) {
                xdp_return_frame(xdpf);
                <span style="font-weight: bold;">continue</span>;
        }
        list_add_tail(&amp;skb-&gt;list, &amp;skb_list);
}
</pre>
</div>

<div class="org-src-container">
<pre class="src src-diff"><span style="font-weight: bold;">@@ -311,6 +311,9 @@</span><span style="font-weight: bold;"> static int cpu_map_kthread_run(void *data)</span>
                        struct xdp_frame *xdpf = frames[i];
                        struct sk_buff *skb = skbs[i];

+                       /* Bring in xdp_frame area */
+                       prefetchw(frames[i+1]);
+
                        skb = cpu_map_build_skb(rcpu, xdpf, skb);
                        if (!skb) {
                                xdp_return_frame(xdpf);
</pre>
</div>

<p>
This helped a bit:
</p>
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          0       18,615,647     0           0          
XDP-RX          total   18,615,647     0          
cpumap-enqueue    0:5   18,615,645     11,492,025  8.00       bulk-average
cpumap-enqueue  sum:5   18,615,645     11,492,025  8.00       bulk-average
cpumap_kthread  5       7,123,614      0           0          
cpumap_kthread  total   7,123,614      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<p>
And Fill Buffer is not stalled:
</p>
<pre class="example">
$ perf stat -C5 -e cycles -e  instructions -e l1d.replacement -e l1d_pend_miss.fb_full -e l1d_pend_miss.pending_cycles  -r 10 sleep 1
 Performance counter stats for 'CPU(s) 5' (10 runs):
     3.803.323.203   cycles                                               ( +-  0,00% )
     8.789.579.607   instructions              #    2,31  insn per cycle  ( +-  0,02% )
        55.889.908   l1d.replacement                                      ( +-  0,65% )
           160.042   l1d_pend_miss.fb_full                                ( +-  3,40% )
       524.989.740   l1d_pend_miss.pending_cycles                         ( +-  0,25% )
</pre>
</div>
</div>

<div id="outline-container-Test%3A%20Remove%20all%20prefetches" class="outline-3">
<h3 id="Test%3A%20Remove%20all%20prefetches"><a href="#Test%3A%20Remove%20all%20prefetches">Test: Remove all prefetches</a></h3>
<div class="outline-text-3" id="text-Test%3A%20Remove%20all%20prefetches">
<p>
Very significant performance drop:
</p>
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          0       17,295,937     0           0          
XDP-RX          total   17,295,937     0          
cpumap-enqueue    0:5   17,295,935     11,471,150  8.00       bulk-average
cpumap-enqueue  sum:5   17,295,935     11,471,150  8.00       bulk-average
cpumap_kthread  5       5,824,778      0           0          
cpumap_kthread  total   5,824,778      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<p>
Want to see if 'l1d.replacement' number change, which is doesn't.  That is
good, as it shows that our prefetch are not causing this.
</p>

<pre class="example">
$ perf stat -C5 -e cycles -e  instructions -e l1d.replacement -e l1d_pend_miss.fb_full -e l1d_pend_miss.pending_cycles  -r 10 sleep 1
 Performance counter stats for 'CPU(s) 5' (10 runs):

  3.803.344.664   cycles                                                ( +-  0,00% )
  6.949.904.074   instructions              #    1,83  insn per cycle   ( +-  0,01% )
     53.345.100   l1d.replacement                                       ( +-  0,13% )
              8   l1d_pend_miss.fb_full                                 ( +- 12,85% )
    840.232.862   l1d_pend_miss.pending_cycles                          ( +-  0,07% )
</pre>
</div>
</div>
</div>



<div id="outline-container-Hack%20use%20Felix%20kfree_skb_list%20bulk" class="outline-2">
<h2 id="Hack%20use%20Felix%20kfree_skb_list%20bulk"><a href="#Hack%20use%20Felix%20kfree_skb_list%20bulk">Hack use Felix kfree_skb_list bulk</a></h2>
<div class="outline-text-2" id="text-Hack%20use%20Felix%20kfree_skb_list%20bulk">
<p>
Replace netif_receive_skb_list_core() with bulk free variant of Felix'es
kfree_skb_list.
</p>

<p>
One baseline is iptables-raw drop in RX-CPU: 5,469,705 pps (GRO-enabled).
</p>
<pre class="example">
iptables -t raw -I PREROUTING -p udp --dport 9 -j DROP
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    5469705            0.0
IpExtInOctets                   251604498          0.0
IpExtInNoECTPkts                5469662            0.0
</pre>

<p>
Disable GRO baseline is iptables-raw drop in RX-CPU: 6378415 pps
(GRO-disabled).
</p>
<pre class="example">
ethtool -K i40e1 gro off tso off
$ nstat -n &amp;&amp; sleep 1 &amp;&amp; nstat
#kernel
IpInReceives                    6378415            0.0
IpExtInOctets                   293407596          0.0
IpExtInNoECTPkts                6378426            0.0
</pre>

<p>
Overhead of GRO:
</p>
<ul class="org-ul">
<li>(1/5469705-1/6378415)*10^9 = 26 ns</li>
</ul>

<p>
Another baseline is from above: 6,987,990 pps before this patch, with cpumap
and iptables-raw drop.
</p>

<div class="org-src-container">
<pre class="src src-diff">diff --git a/kernel/bpf/cpumap.c b/kernel/bpf/cpumap.c
index 37269728a526..7f2e1eecd95a 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/kernel/bpf/cpumap.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/kernel/bpf/cpumap.c</span>
<span style="font-weight: bold;">@@ -259,6 +259,7 @@</span><span style="font-weight: bold;"> static int cpu_map_kthread_run(void *data)</span>
                void *frames[CPUMAP_BATCH];
                void *skbs[CPUMAP_BATCH];
                struct list_head skb_list;
+               struct sk_buff *first_skb;
                gfp_t gfp = __GFP_ZERO | GFP_ATOMIC;
                int i, n, m;

<span style="font-weight: bold;">@@ -321,7 +322,11 @@</span><span style="font-weight: bold;"> static int cpu_map_kthread_run(void *data)</span>
                local_bh_disable();

                /* Inject into network stack */
-               netif_receive_skb_list_core(&amp;skb_list);
+//             netif_receive_skb_list_core(&amp;skb_list);
+               // hack: what is *MAX* achivable perf with bulk drop now
+               (skb_list.prev)-&gt;next = NULL;
+               first_skb = list_first_entry(&amp;skb_list, struct sk_buff, list);
+               kfree_skb_list(first_skb);

</pre>
</div>

<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          4       18,561,003     0           0          
XDP-RX          total   18,561,003     0          
cpumap-enqueue    4:5   18,561,003     4,492,703   8.00       bulk-average
cpumap-enqueue  sum:5   18,561,003     4,492,703   8.00       bulk-average
cpumap_kthread  5       14,068,307     0           0          
cpumap_kthread  total   14,068,307     0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<p>
The speedup is ashonishing:
</p>
<ul class="org-ul">
<li>iptables -t raw -j DROP:  6,987,990 pps</li>
<li>This patch             : 14,068,307 pps</li>
<li>(1/6987990-1/14068307)*10^9 = 72 ns</li>
</ul>

<p>
And the batch size is rather small = 8:  #define CPUMAP_BATCH 8
</p>

<pre class="example">
$ perf stat -C5 -e cycles -e  instructions -e l1d.replacement -e l1d_pend_miss.fb_full -e l1d_pend_miss.pending -e l1d_pend_miss.pending_cycles  -r3 sleep 1

 Performance counter stats for 'CPU(s) 5' (3 runs):

     3.794.909.591      cycles                                              ( +-  0,00% )  (33,27%)
     5.647.624.119      instructions              #  1,49  insn per cycle   ( +-  0,45% )  (49,95%)
        92.070.295      l1d.replacement                                     ( +-  0,52% )  (66,63%)
         2.030.914      l1d_pend_miss.fb_full                               ( +-  0,78% )  (83,32%)
     1.581.098.313      l1d_pend_miss.pending                               ( +-  0,29% )  (83,35%)
     1.300.932.415      l1d_pend_miss.pending_cycles                        ( +-  0,38% )  (16,65%)
</pre>

<p>
The insn per cycle is actually note very good.
</p>

<p>
Detailed perf analysis shows these "l1d_pend_miss.pending" is caused when
reading xdp_frame first time, and when reading packet payload
(xdp_frame-&gt;data).
</p>

<pre class="example">
$ perf stat -C5 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 5' (4 runs):

     3.803.907.079      cycles                                                  ( +-  0,00% )
     5.680.449.445      instructions              # 1,49  insn per cycle        ( +-  0,26% )
        77.631.914      cache-references                                        ( +-  0,29% )
             1.148      cache-misses              # 0,001 % of all cache refs   ( +- 44,44% )
     1.114.192.930      branches:k                                              ( +-  0,26% )
         4.041.461      branch-misses:k           # 0,36% of all branches       ( +-  0,24% )
            54.077      l2_rqsts.all_code_rd                                    ( +-  2,57% )
            45.202      l2_rqsts.code_rd_hit                                    ( +-  1,91% )
             8.838      l2_rqsts.code_rd_miss                                   ( +-  6,30% )
</pre>

<p>
Perf report on CPU 5:
</p>
<pre class="example">
Samples: 120K of event 'cycles:ppp', Event count (approx.): 113416388646
  Overhead  CPU  Command          Shared Object     Symbol
+   28,68%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] cpu_map_kthread_run
+   17,95%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] build_skb_around
+    9,86%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] memset_erms
+    6,29%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] skb_release_data
+    5,54%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] eth_type_trans
+    5,43%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] kmem_cache_alloc_bulk
+    4,57%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] page_frag_free
+    4,14%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] kmem_cache_free_bulk
+    2,99%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] kfree_skb_list
+    2,08%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] skb_release_head_state
+    1,70%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] skb_release_all
+    1,47%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] bpf_prog_e7b6a25b0d20485e
+    1,42%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] skb_free_head
+    1,30%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] perf_trace_xdp_cpumap_kthread
+    1,28%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] memset
+    1,28%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] trace_call_bpf
+    0,97%  005  cpumap/5/map:46  [kernel.vmlinux]  [k] __list_add_valid
</pre>

<p>
Deducting per packet nanosec cost from: 14,068,307 pps = 71 ns
</p>
<ul class="org-ul">
<li>(1/14068307)*10^9 = 71 ns</li>
</ul>

<p>
Cost of skb alloc+free reduced to: 6.8 ns
</p>
<ul class="org-ul">
<li>5,43%  kmem_cache_alloc_bulk (71/100*5.43 = 3.8553 ns)</li>
<li>4,14%  kmem_cache_free_bulk  (71/100*4.14 = 2.9394 ns)</li>
<li>9.57%  = 6.7947 ns</li>
</ul>

<p>
There is a L1-miss (from L3) in two top functions:
</p>
<ul class="org-ul">
<li>28,68%  cpu_map_kthread_run 71/100*28.68 = 20.3628 ns</li>
<li>17,95%  build_skb_around    71/100*17.95 = 12.7445 ns</li>
<li>46.63% = 33.1 ns</li>
</ul>

<p>
The memset is in two functions
</p>
<ul class="org-ul">
<li>9,86%   memset_erms (71/100*9.86 = 7.0006 ns)</li>
<li>1,28%   memset      (71/100*1.28 = 0.9088 ns)</li>
<li>11.14% = 7.9094 ns</li>
</ul>
</div>

<div id="outline-container-test%3A%20remove%20kmem_cache_free_bulk" class="outline-3">
<h3 id="test%3A%20remove%20kmem_cache_free_bulk"><a href="#test%3A%20remove%20kmem_cache_free_bulk">test: remove kmem_cache_free_bulk</a></h3>
<div class="outline-text-3" id="text-test%3A%20remove%20kmem_cache_free_bulk">
<p>
Isolate the effect of using <code>kmem_cache_free_bulk()</code>. The change the bulk
variant of <code>kfree_skb_list</code>, to revert back to use <code>kfree_skb()</code>, which
makes it not use bulking. Notice, that <code>kfree_skb_list</code> still get the
effect/improvement for the I-cache optimization.
</p>

<p>
Code change:
</p>
<div class="org-src-container">
<pre class="src src-diff">diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index f1391379177f..1851c9c622af 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/net/core/skbuff.c</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/net/core/skbuff.c</span>
<span style="font-weight: bold;">@@ -707,6 +707,10 @@</span><span style="font-weight: bold;"> void kfree_skb_list(struct sk_buff *segs)</span>
                        continue;
                }

+               kfree_skb(segs);
+               continue;
+
+#if 0
                if (!skb_unref(segs))
                        continue;

<span style="font-weight: bold;">@@ -722,6 +726,7 @@</span><span style="font-weight: bold;"> void kfree_skb_list(struct sk_buff *segs)</span>

                kmem_cache_free_bulk(skbuff_head_cache, n_skbs, skbs);
                n_skbs = 0;
+#endif
        }
</pre>
</div>

<p>
Performance change:
</p>
<ul class="org-ul">
<li>before: 14,068,307 pps</li>
<li>after:  13,362,498 pps</li>
<li>diff-pps: -705,809 pps</li>
<li>diff-ns:  (1/13362498-1/14068307)*10^9 = 3.754548 ns</li>
</ul>

<pre class="example">
sudo ./xdp_redirect_cpu --dev i40e1 --qsize 128 --cpu 4 --prog xdp_cpu_map0
[...]
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          3       19,370,256     0           0          
XDP-RX          total   19,370,256     0          
cpumap-enqueue    3:4   19,370,259     6,007,762   8.00       bulk-average
cpumap-enqueue  sum:4   19,370,259     6,007,762   8.00       bulk-average
cpumap_kthread  4       13,362,498     0           0          
cpumap_kthread  total   13,362,498     0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<p>
Below is it clear that the cost of <code>kmem_cache_free</code> increased. (We know the
call <code>kmem_cache_free</code> is hitting the fast-path of the SLUB allocator, due
to this limited micro-benchmark, which makes the improvement impressive. The
<code>kmem_cache_free_bulk</code> for SLUB will have a larger performance advantage
over <code>kmem_cache_free</code> once we move out-of this fast-path area).
</p>

<pre class="example">
Samples: 120K of event 'cycles:ppp', Event count (approx.): 113422085196
  Overhead  CPU  Command         Shared Object     Symbol
+   27,50%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] cpu_map_kthread_run
+   17,03%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] build_skb_around
+    9,95%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] memset_erms
+    7,08%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] kmem_cache_free
+    5,26%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] kmem_cache_alloc_bulk
+    5,24%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] eth_type_trans
+    3,45%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] skb_release_data
+    3,15%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] kfree_skb
+    3,09%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] skb_release_head_state
+    2,57%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] page_frag_free
+    2,23%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] kfree_skb_list
+    1,66%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] skb_release_all
+    1,45%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] bpf_prog_e7b6a25b0d20485e
+    1,27%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] trace_call_bpf
+    1,25%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] perf_trace_xdp_cpumap_kthread
+    1,24%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] memset
+    1,00%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] kfree_skbmem
+    0,95%  004  cpumap/4/map:1  [kernel.vmlinux]  [k] __list_add_valid
</pre>
</div>
</div>
</div>


<div id="outline-container---notes" class="outline-2">
<h2 id="--notes"><a href="#--notes">notes</a></h2>
<div class="outline-text-2" id="text---notes">
</div>
<div id="outline-container-Experiments" class="outline-3">
<h3 id="Experiments"><a href="#Experiments">Experiments</a></h3>
<div class="outline-text-3" id="text-Experiments">
<pre class="example">
955.571564128                MUX:                                                14.29 +-     0.00 %       
956.520987559 BE             Backend_Bound:                                      36.61 +-     0.00 % Slots 
956.520987559 BE/Mem         Backend_Bound.Memory_Bound:                         15.48 +-     0.00 % Slots 
956.520987559 BE/Core        Backend_Bound.Core_Bound:                           21.13 +-     0.00 % Slots 
956.520987559 BE/Mem         Backend_Bound.Memory_Bound.L1_Bound:                13.63 +-     0.00 % Stalls
956.520987559 BE/Mem         Backend_Bound.Memory_Bound.L3_Bound:                 8.42 +-     0.00 % Stalls
956.520987559 BE/Core        Backend_Bound.Core_Bound.Ports_Utilization:         33.17 +-     0.00 % Clocks &lt;==
956.520987559                MUX:                                                14.29 +-     0.00 %       
Sampling:
perf record -g -e cycles:pp,cpu/event=0xd1,umask=0x4,name=L3_Bound_MEM_LOAD_UOPS_RETIRED_L3_HIT,period=50021/pp,cpu/event=0xd1,umask=0x1,name=L1_Bound_MEM_LOAD_UOPS_RETIRED_L1_HIT,period=2000003/pp,cpu/event=0xd1,umask=0x40,name=L1_Bound_MEM_LOAD_UOPS_RETIRED_HIT_LFB,period=100003/pp -o perf.data --cpu 4 -a
[jbrouer@broadwell pmu-tools]$ perf record -g -e cycles:pp,cpu/event=0xd1,umask=0x4,name=L3_Bound_MEM_LOAD_UOPS_RETIRED_L3_HIT,period=50021/pp,cpu/event=0xd1,umask=0x1,name=L1_Bound_MEM_LOAD_UOPS_RETIRED_L1_HIT,period=2000003/pp,cpu/event=0xd1,umask=0x40,name=L1_Bound_MEM_LOAD_UOPS_RETIRED_HIT_LFB,period=100003/pp -o perf.data --cpu 4 -a
</pre>
</div>
</div>

<div id="outline-container-Experiment%3A%20cut-out%20netfilter-code-path" class="outline-3">
<h3 id="Experiment%3A%20cut-out%20netfilter-code-path"><a href="#Experiment%3A%20cut-out%20netfilter-code-path">Experiment: cut-out netfilter-code-path</a></h3>
<div class="outline-text-3" id="text-Experiment%3A%20cut-out%20netfilter-code-path">
<p>
Ugly hack cut-out nf_nook invocation, and drop all SKBs directly in NF_HOOK_LIST.
</p>

<div class="org-src-container">
<pre class="src src-diff">diff --git a/include/linux/netfilter.h b/include/linux/netfilter.h
index 72cb19c3db6a..edcd49c11ba3 100644
<span style="font-weight: bold;">--- </span><span style="font-weight: bold;">a/include/linux/netfilter.h</span>
<span style="font-weight: bold;">+++ </span><span style="font-weight: bold;">b/include/linux/netfilter.h</span>
<span style="font-weight: bold;">@@ -301,8 +301,9 @@</span><span style="font-weight: bold;"> NF_HOOK_LIST(uint8_t pf, unsigned int hook, struct net *net, struct sock *sk,</span>
        INIT_LIST_HEAD(&amp;sublist);
        list_for_each_entry_safe(skb, next, head, list) {
                list_del(&amp;skb-&gt;list);
-               if (nf_hook(pf, hook, net, sk, skb, in, out, okfn) == 1)
-                       list_add_tail(&amp;skb-&gt;list, &amp;sublist);
+               kfree_skb(skb); // XXX hack partition code-path test
+               //if (nf_hook(pf, hook, net, sk, skb, in, out, okfn) == 1)
+               //      list_add_tail(&amp;skb-&gt;list, &amp;sublist);
        }
        /* Put passed packets back on main list */
        list_splice(&amp;sublist, head);
</pre>
</div>

<p>
On-top of: "Patch: bpf: cpumap use netif_receive_skb_list" and batch=16
</p>
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          1       19,193,940     0           0          
XDP-RX          total   19,193,940     0          
cpumap-enqueue    1:4   19,193,949     12,275,618  8.00       bulk-average
cpumap-enqueue  sum:4   19,193,949     12,275,618  8.00       bulk-average
cpumap_kthread  4       6,918,329      0           0          
cpumap_kthread  total   6,918,329      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<p>
Batch=64 (On-top of: "Patch: bpf: cpumap use netif_receive_skb_list")
</p>
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          4       18,643,859     0           0          
XDP-RX          total   18,643,859     0          
cpumap-enqueue    4:5   18,643,866     11,638,701  8.00       bulk-average
cpumap-enqueue  sum:5   18,643,866     11,638,701  8.00       bulk-average
cpumap_kthread  5       7,005,145      0           0          
cpumap_kthread  total   7,005,145      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>

<p>
Batch=8 (On-top of: "bpf: cpumap memory prefetchw optimizations for struct page")
</p>
<pre class="example">
Running XDP/eBPF prog_num:0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          1       19,182,669     0           0          
XDP-RX          total   19,182,669     0          
cpumap-enqueue    1:5   19,182,679     10,166,330  8.00       bulk-average
cpumap-enqueue  sum:5   19,182,679     10,166,330  8.00       bulk-average
cpumap_kthread  5       9,016,347      0           0          
cpumap_kthread  total   9,016,347      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2019-04-12 Fri 11:17</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
